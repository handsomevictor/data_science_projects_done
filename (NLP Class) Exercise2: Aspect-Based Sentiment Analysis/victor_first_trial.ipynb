{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"victor_first_trial.ipynb","provenance":[],"collapsed_sections":["-KBF8AcZxcsM","_3ZFI3x8yZOg","LG1xfyXX9eoQ","V8zzmH62Zao4","Z45_m9JB3wXB","MWmNT7mL5O7t"],"machine_shape":"hm","authorship_tag":"ABX9TyNBux7qHjsLV9DgMzZQIEUw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8SnArzM9Ywn6","executionInfo":{"status":"ok","timestamp":1646780185612,"user_tz":-60,"elapsed":1726,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"beb08c67-4f9e-4022-eaaf-5678ba51f3b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/DSBA M2/2 NLP/exercise_2/src')"],"metadata":{"id":"Uk3J0AdmYtUl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import classifier\n","import tester\n","import pandas as pd\n","import numpy as np\n","import csv\n","import sklearn\n","from tqdm import tqdm\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"KRC9dFvVYtX2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prepare the data"],"metadata":{"id":"-KBF8AcZxcsM"}},{"cell_type":"code","source":["# prepare devdata\n","devdata_list = []\n","with open('/content/drive/MyDrive/DSBA M2/2 NLP/exercise_2/data/devdata.csv')as f:\n","    f_csv = csv.reader(f)\n","    for row in f_csv:\n","        devdata_list.append(','.join(row))\n","\n","devdata_df = pd.DataFrame(list(map(lambda x: x.split('\\t'), devdata_list)), columns = ['polarity', 'aspect_category', 'target_term', 'character_offsets', 'sentence'])\n","devdata_df.head(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"CDdzO7D_vqLu","executionInfo":{"status":"ok","timestamp":1646780186474,"user_tz":-60,"elapsed":5,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"e334b599-e2fc-4ebe-b479-e48edfe84a8c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-7d2bc0fd-178f-4f0b-8b5c-803d5a35f36a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>polarity</th>\n","      <th>aspect_category</th>\n","      <th>target_term</th>\n","      <th>character_offsets</th>\n","      <th>sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>positive</td>\n","      <td>LOCATION#GENERAL</td>\n","      <td>neighborhood</td>\n","      <td>54:66</td>\n","      <td>great food, great wine list, great service in ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>negative</td>\n","      <td>RESTAURANT#GENERAL</td>\n","      <td>place</td>\n","      <td>15:20</td>\n","      <td>I thought this place was totally overrated.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7d2bc0fd-178f-4f0b-8b5c-803d5a35f36a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7d2bc0fd-178f-4f0b-8b5c-803d5a35f36a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7d2bc0fd-178f-4f0b-8b5c-803d5a35f36a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   polarity     aspect_category   target_term character_offsets  \\\n","0  positive    LOCATION#GENERAL  neighborhood             54:66   \n","1  negative  RESTAURANT#GENERAL         place             15:20   \n","\n","                                            sentence  \n","0  great food, great wine list, great service in ...  \n","1        I thought this place was totally overrated.  "]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# prepare traindata\n","traindata_list = []\n","with open('/content/drive/MyDrive/DSBA M2/2 NLP/exercise_2/data/traindata.csv')as f:\n","    f_csv = csv.reader(f)\n","    for row in f_csv:\n","        traindata_list.append(','.join(row))\n","\n","traindata_df = pd.DataFrame(list(map(lambda x: x.split('\\t'), traindata_list)), columns = ['polarity', 'aspect_category', 'target_term', 'character_offsets', 'sentence'])\n","traindata_df.head(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"8ATMhJ2_xhgi","executionInfo":{"status":"ok","timestamp":1646780189594,"user_tz":-60,"elapsed":303,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"14e04f49-d185-4647-8bee-4d3eaafc0ca0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-5400d572-e337-45dd-b568-35c4e066f5f2\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>polarity</th>\n","      <th>aspect_category</th>\n","      <th>target_term</th>\n","      <th>character_offsets</th>\n","      <th>sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>positive</td>\n","      <td>AMBIENCE#GENERAL</td>\n","      <td>seating</td>\n","      <td>18:25</td>\n","      <td>short and sweet – seating is great:it's romant...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>positive</td>\n","      <td>AMBIENCE#GENERAL</td>\n","      <td>trattoria</td>\n","      <td>25:34</td>\n","      <td>This quaint and romantic trattoria is at the t...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5400d572-e337-45dd-b568-35c4e066f5f2')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-5400d572-e337-45dd-b568-35c4e066f5f2 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5400d572-e337-45dd-b568-35c4e066f5f2');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   polarity   aspect_category target_term character_offsets  \\\n","0  positive  AMBIENCE#GENERAL     seating             18:25   \n","1  positive  AMBIENCE#GENERAL   trattoria             25:34   \n","\n","                                            sentence  \n","0  short and sweet – seating is great:it's romant...  \n","1  This quaint and romantic trattoria is at the t...  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":[""],"metadata":{"id":"Gg1otG-bxg9X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Some useful variables"],"metadata":{"id":"_3ZFI3x8yZOg"}},{"cell_type":"code","source":["aspect_categories = list(set(traindata_df.aspect_category.to_list()))"],"metadata":{"id":"H2xaYExnxg5w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(aspect_categories)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yXH09I62xg3Q","executionInfo":{"status":"ok","timestamp":1646767639507,"user_tz":-60,"elapsed":2,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"4d1c5681-d1f4-4b5d-ccaf-c9bafe22489d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["devdata_df.polarity.value_counts(), traindata_df.polarity.value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bppuGeogwIeU","executionInfo":{"status":"ok","timestamp":1646767640377,"user_tz":-60,"elapsed":3,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"541c0a6a-8ad6-48f8-cf52-e9889ff905ef"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(positive    264\n"," negative     98\n"," neutral      14\n"," Name: polarity, dtype: int64, positive    1055\n"," negative     390\n"," neutral       58\n"," Name: polarity, dtype: int64)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":[""],"metadata":{"id":"xziIeNOb8KVj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Solve Package Version Problem"],"metadata":{"id":"ekehc7IK8KyF"}},{"cell_type":"code","source":["import sys\n","sys.version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"upbkcF3Yyos_","executionInfo":{"status":"ok","timestamp":1646780192793,"user_tz":-60,"elapsed":238,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"6a37577d-c076-46cd-9ddc-1b6e0fd9de5b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'3.7.12 (default, Jan 15 2022, 18:48:18) \\n[GCC 7.5.0]'"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["!pip install transformers --quiet\n","!pip install stanza --quiet"],"metadata":{"id":"3pW-VQUS1jYA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# change your version according to the .__version__ result\n","!pip uninstall nltk -y --quiet\n","!pip uninstall gensim -y --quiet\n","!pip install nltk==3.6.0 --quiet\n","!pip install gensim==4.1.2 --quiet"],"metadata":{"id":"A3bUvoOE7-L2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk       # >= 3.6.0\n","import sklearn    # >=0.24.0\n","import pandas as pd     # >= 1.3.0\n","import gensim     # >= 4.1.2\n","import stanza     # == 1.3.0\n","import torch      # >= 1.10.0\n","import transformers # version 4.16.x.\n","\n","'nltk version:' + nltk.__version__, \\\n","'sklearn version:' + sklearn.__version__, \\\n","'pandas version:' + pandas.__version__, \\\n","'gensim version:' + gensim.__version__, \\\n","'stanza version:' + stanza.__version__,\\\n","'torch version:' + torch.__version__,\\\n","'transformers version:' + transformers.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9t9R4mms1v_Y","executionInfo":{"status":"ok","timestamp":1646780216264,"user_tz":-60,"elapsed":1946,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"d8f7c6a2-22c9-4c42-9105-8a93961649e2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('nltk version:3.6',\n"," 'sklearn version:1.0.2',\n"," 'pandas version:1.3.5',\n"," 'gensim version:4.1.2',\n"," 'stanza version:1.3.0',\n"," 'torch version:1.10.0+cu111',\n"," 'transformers version:4.17.0')"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":[""],"metadata":{"id":"hHk8VG6O3q2i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Complete Classifier class"],"metadata":{"id":"LG1xfyXX9eoQ"}},{"cell_type":"code","source":["import transformers\n","from transformers import RobertaTokenizer, TFRobertaModel\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder"],"metadata":{"id":"sLafG_M8ESgR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Classifier:\n","    \"\"\"Copied from some random RoBerta tutorials and I don't understand much about how the sentiments were generated\"\"\"\n","\n","    def __init__(self):\n","    # initialising the class and loading the BERT model from HuggingFace and giving max embeddings to get for each columns.\n","        # load BERT models\n","        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-large-mnli')\n","        self.bert_model = TFRobertaModel.from_pretrained('roberta-large-mnli')\n","\n","        # parameters for getting embedding\n","        self.max_token_dict = {'asp_cat_emb':16, 'asp_term_emb':24, 'review_emb':50}\n","        self.src_column_dict = {'asp_cat_emb':'aspect_category', 'asp_term_emb':'aspect_term', 'review_emb':'review'}\n","        \n","        # loading variable encoder\n","        self.encoder = LabelEncoder()\n","\n","        # model\n","        self.model = self.create_model()\n","\n","\n","    def train(self, trainfile, devfile=None):\n","        # loading\n","        print('loading the train file...')\n","        df = self.loadfile(trainfile)\n","        X = self.get_embeddings(df) # 自变量\n","        Y = df['sentiment'].values\n","        Y = self.encoder.fit_transform(np.array(Y).reshape(-1,1))\n","        \n","        # optimizer and scheduler\n","        print('optimizing and scheduling...')\n","        optim = tf.keras.optimizers.Adam(learning_rate=0.001)\n","        # rlrp = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\n","        \n","        # loss\n","        print('calculating loss...')\n","        self.model.compile(optimizer=optim, loss= 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n","        \n","        # training\n","        print('training & fit...')\n","        temp_start = time.time()\n","        # self.model.fit(X, Y, epochs =50, callbacks=[rlrp], verbose = 0)\n","        self.model.fit(X, Y, epochs =50, verbose = 0)\n","\n","        temp_end = time.time()\n","        print('It takes ', str(round((temp_end - temp_start), 3)), ' seconds to train')\n","\n","\n","    def predict(self, datafile):\n","        \"\"\"Predicts class labels for the input instances in file 'datafile'\n","        Returns the list of predicted labels\n","        \"\"\"\n","        # loading the test file\n","        df = self.loadfile(datafile)\n","        X = self.get_embeddings(df)\n","        \n","        # Predictions\n","        pred = self.model.predict(X)\n","        \n","        # Encoding to original class labels\n","        pred = np.argmax(pred, axis= 1)\n","        pred = self.encoder.inverse_transform(pred)\n","        \n","        return list(pred)\n","\n","\n","    def loadfile(self, csv_file):\n","        '''\n","        Load the files as pandas dataframe object\n","        '''\n","        columns = ['sentiment', 'aspect_category', 'aspect_term', 'slice', 'review']\n","        df = pd.read_csv(csv_file, sep='\\t', names = columns, header = None)\n","        return df\n","\n","\n","    def get_embeddings(self, df, to_print=False):\n","        '''\n","        Load embeddings  from the RoBERTa Model\n","        '''\n","        emb_list = []\n","        for col, MAX_LENGTH in self.max_token_dict.items():\n","            str_inp = df[self.src_column_dict[col]].values\n","            inputs = self.tokenizer([str(i) for i in str_inp],\n","                                    max_length = MAX_LENGTH,\n","                                    pad_to_max_length = True,\n","                                    return_tensors=\"pt\",\n","                                    truncation=True)\n","        \n","            inputs = [np.array(v) for _, v in inputs.items()]\n","            if to_print:\n","                print(inputs)\n","\n","            out = self.bert_model.predict(inputs)\n","            emb_list.append(out[0])\n","\n","        return np.concatenate(emb_list, axis =1)\n","\n","\n","    def create_model(self):\n","        '''\n","        Final Classifier NN model. \n","        Takes embeddings as input and predicts the class encoded label.\n","        '''\n","        model= tf.keras.Sequential()\n","        model.add(tf.keras.layers.Dense(2048))\n","        model.add(tf.keras.layers.Dense(512))\n","        model.add(tf.keras.layers.Dense(256))\n","        model.add(tf.keras.layers.Dense(128))\n","\n","        model.add(tf.keras.layers.Flatten())\n","\n","        model.add(tf.keras.layers.Dense(4000))\n","        model.add(tf.keras.layers.Dropout(0.2))\n","        \n","        model.add(tf.keras.layers.Dense(1250))\n","        model.add(tf.keras.layers.Dropout(0.2))\n","        \n","        model.add(tf.keras.layers.Dense(512))\n","        model.add(tf.keras.layers.Dropout(0.2))\n","        \n","        model.add(tf.keras.layers.Dense(256,activation= tf.nn.leaky_relu))\n","        model.add(tf.keras.layers.Dropout(0.2))\n","        \n","        model.add(tf.keras.layers.Dense(64)) #tf.nn.leaky_relu\n","        model.add(tf.keras.layers.Dropout(0.2))\n","        \n","        model.add(tf.keras.layers.Dense(16,activation= tf.nn.leaky_relu))\n","        model.add(tf.keras.layers.Dropout(0.1))\n","        \n","        model.add(tf.keras.layers.Dense(units=3, activation='softmax'))\n","        \n","        return model"],"metadata":{"id":"H9Sy5usw1v67"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","sys.argv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AZ_T-QjnxnHu","executionInfo":{"status":"ok","timestamp":1646767688258,"user_tz":-60,"elapsed":13,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"d9e58e58-05be-4738-ad3e-59e65d9b6256"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py',\n"," '-f',\n"," '/root/.local/share/jupyter/runtime/kernel-c7a1d6ac-2cd5-4365-a390-a4f07f15b2e8.json']"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# test.py\n","import random as rn\n","import os\n","import time, sys\n","import numpy as np\n","\n","def set_reproducible():\n","    # The below is necessary to have reproducible behavior.\n","    import random as rn\n","    import os\n","    os.environ['PYTHONHASHSEED'] = '0'\n","    # The below is necessary for starting Numpy generated random numbers\n","    # in a well-defined initial state.\n","    np.random.seed(17)\n","    # The below is necessary for starting core Python generated random numbers\n","    # in a well-defined state.\n","    rn.seed(12345)\n","\n","def load_label_output(filename):\n","    with open(filename, 'r', encoding='UTF-8') as f:\n","        return [line.strip().split(\"\\t\")[0] for line in f if line.strip()]\n","\n","def eval_list(glabels, slabels):\n","    if (len(glabels) != len(slabels)):\n","        print(\"\\nWARNING: label count in system output (%d) is different from gold label count (%d)\\n\" % (\n","        len(slabels), len(glabels)))\n","    n = min(len(slabels), len(glabels))\n","    incorrect_count = 0\n","    for i in range(n):\n","        if slabels[i] != glabels[i]: incorrect_count += 1\n","    acc = (n - incorrect_count) / n\n","    return acc*100\n","\n","def train_and_eval(classifier, trainfile, devfile, testfile, run_id):\n","    print(f\"\\nRUN: {run_id}\")\n","    print(\"  %s.1. Training the classifier...\" % str(run_id))\n","    # classifier.train(trainfile, devfile)\n","    classifier.train(trainfile, devfile)\n","    print()\n","    print(\"  %s.2. Eval on the dev set...\" % str(run_id), end=\"\")\n","    slabels = classifier.predict(devfile)\n","    glabels = load_label_output(devfile)\n","    devacc = eval_list(glabels, slabels)\n","    print(\" Acc.: %.2f\" % devacc)\n","    testacc = -1\n","    if testfile is not None:\n","        # Evaluation on the test data\n","        print(\"  %s.3. Eval on the test set...\" % str(run_id), end=\"\")\n","        slabels = classifier.predict(testfile)\n","        glabels = load_label_output(testfile)\n","        testacc = eval_list(glabels, slabels)\n","        print(\" Acc.: %.2f\" % testacc)\n","    print()\n","    return (devacc, testacc)"],"metadata":{"id":"UHEn9GJnyRGF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["set_reproducible()\n","n_runs = 5\n","datadir = \"../data/\"\n","trainfile =  '/content/drive/MyDrive/DSBA M2/2 NLP/exercise_2/data/traindata.csv'\n","devfile =  '/content/drive/MyDrive/DSBA M2/2 NLP/exercise_2/data/devdata.csv'\n","testfile = None\n","\n","# Runs\n","start_time = time.perf_counter()\n","devaccs = []\n","testaccs = []\n","for i in range(1, n_runs+1):\n","    classifier =  Classifier()\n","    devacc, testacc = train_and_eval(classifier, trainfile, devfile, testfile, i)\n","    devaccs.append(np.round(devacc, 2))\n","    testaccs.append(np.round(testacc, 2))\n","print('\\nCompleted %d runs.' % n_runs)\n","total_exec_time = (time.perf_counter() - start_time)\n","print(\"Dev accs:\", devaccs)\n","print(\"Test accs:\", testaccs)\n","print()\n","print(\"Mean Dev Acc.: %.2f (%.2f)\" % (np.mean(devaccs), np.std(devaccs)))\n","print(\"Mean Test Acc.: %.2f (%.2f)\" % (np.mean(testaccs), np.std(testaccs)))\n","print(\"\\nExec time: %.2f s. ( %d per run )\" % (total_exec_time, total_exec_time / n_runs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D2D22V5GxTMQ","executionInfo":{"status":"ok","timestamp":1646772741533,"user_tz":-60,"elapsed":498819,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"11c37a80-5b27-4219-f304-b4366e5b2b3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at roberta-large-mnli were not used when initializing TFRobertaModel: ['classifier']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-large-mnli.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["\n","RUN: 1\n","  1.1. Training the classifier...\n","loading the train file...\n","optimizing and scheduling...\n","calculating loss...\n","training & fit...\n","It takes  43.988  seconds to train\n","\n","  1.2. Eval on the dev set... Acc.: 77.66\n","\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at roberta-large-mnli were not used when initializing TFRobertaModel: ['classifier']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-large-mnli.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["\n","RUN: 2\n","  2.1. Training the classifier...\n","loading the train file...\n","optimizing and scheduling...\n","calculating loss...\n","training & fit...\n","It takes  44.222  seconds to train\n","\n","  2.2. Eval on the dev set... Acc.: 85.37\n","\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at roberta-large-mnli were not used when initializing TFRobertaModel: ['classifier']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-large-mnli.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["\n","RUN: 3\n","  3.1. Training the classifier...\n","loading the train file...\n","optimizing and scheduling...\n","calculating loss...\n","training & fit...\n","It takes  43.872  seconds to train\n","\n","  3.2. Eval on the dev set... Acc.: 85.11\n","\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at roberta-large-mnli were not used when initializing TFRobertaModel: ['classifier']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-large-mnli.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["\n","RUN: 4\n","  4.1. Training the classifier...\n","loading the train file...\n","optimizing and scheduling...\n","calculating loss...\n","training & fit...\n","It takes  43.934  seconds to train\n","\n","  4.2. Eval on the dev set... Acc.: 69.15\n","\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at roberta-large-mnli were not used when initializing TFRobertaModel: ['classifier']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-large-mnli.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["\n","RUN: 5\n","  5.1. Training the classifier...\n","loading the train file...\n","optimizing and scheduling...\n","calculating loss...\n","training & fit...\n","It takes  43.962  seconds to train\n","\n","  5.2. Eval on the dev set... Acc.: 84.84\n","\n","\n","Completed 5 runs.\n","Dev accs: [77.66, 85.37, 85.11, 69.15, 84.84]\n","Test accs: [-1, -1, -1, -1, -1]\n","\n","Mean Dev Acc.: 80.43 (6.34)\n","Mean Test Acc.: -1.00 (0.00)\n","\n","Exec time: 498.59 s. ( 99 per run )\n"]}]},{"cell_type":"code","source":["devaccs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TFswQ4f4w6f5","executionInfo":{"status":"ok","timestamp":1646768278489,"user_tz":-60,"elapsed":33,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"1ba308cd-992d-48b5-f8a0-bab2eb003723"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[86.97, 87.23, 88.03, 86.44, 88.03]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["testaccs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_TNjB2c4w9TE","executionInfo":{"status":"ok","timestamp":1646768278490,"user_tz":-60,"elapsed":14,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"79d52500-0639-4ac1-dc3a-65b003308690"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[-1, -1, -1, -1, -1]"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# # 每一句的sentiment存起来\n","# for i in devaccs:\n","#     with open('/content/drive/MyDrive/DSBA M2/2 NLP/exercise_2/saved_lists/devaccs.txt', 'a') as f:\n","#         f.write(i)\n","#         f.write('\\n')\n","\n","# for i in testaccs:\n","#     with open('/content/drive/MyDrive/DSBA M2/2 NLP/exercise_2/saved_lists/testaccs.txt', 'a') as f:\n","#         f.write(i)\n","#         f.write('\\n')\n","\n","# f.close()"],"metadata":{"id":"gJReg9Ua49W7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # 重新reload lists\n","# devaccs = []\n","# testaccs = []\n","\n","# with open('test1.txt', 'r') as f1:\n","#     list1 = f1.readlines()\n","#     print(list1)\n","\n","# for i in range(0, len(list1)):\n","#     list1[i] = list1[i].rstrip('\\n')"],"metadata":{"id":"lqlrEwNbYEVD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"BxOXWiQxYEKY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Followings are trash"],"metadata":{"id":"V8zzmH62Zao4"}},{"cell_type":"code","source":[""],"metadata":{"id":"W5z_yg9lxlqC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 单独抽出来看一句话的情感"],"metadata":{"id":"Z45_m9JB3wXB"}},{"cell_type":"code","source":["set_reproducible()\n","n_runs = 5\n","datadir = \"../data/\"\n","trainfile =  '/content/drive/MyDrive/DSBA M2/2 NLP/exercise_2/data/traindata.csv'\n","devfile =  '/content/drive/MyDrive/DSBA M2/2 NLP/exercise_2/data/devdata.csv'\n","testfile = None\n","\n","slabels = classifier.predict(devfile)"],"metadata":{"id":"0tgRnNrp3wLp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(slabels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n68u6ldoL898","executionInfo":{"status":"ok","timestamp":1646770773757,"user_tz":-60,"elapsed":236,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"8f7a58c1-a526-49ed-ce77-b6f10c5d49c6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["376"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["slabels[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vizBOnZz4K3G","executionInfo":{"status":"ok","timestamp":1646769906325,"user_tz":-60,"elapsed":19,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"431caaa6-9c5b-417e-aa60-7a59a6700e11"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['positive',\n"," 'negative',\n"," 'positive',\n"," 'negative',\n"," 'positive',\n"," 'negative',\n"," 'negative',\n"," 'positive',\n"," 'positive',\n"," 'negative']"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["glabels = load_label_output(devfile)\n","glabels[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r-wGNBal4NRX","executionInfo":{"status":"ok","timestamp":1646769906326,"user_tz":-60,"elapsed":16,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"b436053e-c833-4a61-de77-6b741c074c5d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['positive',\n"," 'negative',\n"," 'positive',\n"," 'negative',\n"," 'neutral',\n"," 'negative',\n"," 'negative',\n"," 'positive',\n"," 'positive',\n"," 'negative']"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["classifier.loadfile(devfile)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"MOQPBgnZEbGy","executionInfo":{"status":"ok","timestamp":1646769906327,"user_tz":-60,"elapsed":13,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"15073734-18d7-424c-a0ea-68046bcf822b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-fc886558-81a3-4ebd-9b51-1053300f02e6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment</th>\n","      <th>aspect_category</th>\n","      <th>aspect_term</th>\n","      <th>slice</th>\n","      <th>review</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>positive</td>\n","      <td>LOCATION#GENERAL</td>\n","      <td>neighborhood</td>\n","      <td>54:66</td>\n","      <td>great food, great wine list, great service in ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>negative</td>\n","      <td>RESTAURANT#GENERAL</td>\n","      <td>place</td>\n","      <td>15:20</td>\n","      <td>I thought this place was totally overrated.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>positive</td>\n","      <td>FOOD#QUALITY</td>\n","      <td>Fish</td>\n","      <td>0:4</td>\n","      <td>Fish is so very fresh.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>negative</td>\n","      <td>SERVICE#GENERAL</td>\n","      <td>manager</td>\n","      <td>19:26</td>\n","      <td>I showed it to the manager, and he smilingly a...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>neutral</td>\n","      <td>DRINKS#QUALITY</td>\n","      <td>margaritas</td>\n","      <td>63:73</td>\n","      <td>The food we ordered was excellent, although I ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>371</th>\n","      <td>positive</td>\n","      <td>RESTAURANT#GENERAL</td>\n","      <td>PLACE</td>\n","      <td>58:63</td>\n","      <td>I CAN EAT HERE EVERY DAY OF THE WEEK REALLY LO...</td>\n","    </tr>\n","    <tr>\n","      <th>372</th>\n","      <td>neutral</td>\n","      <td>RESTAURANT#MISCELLANEOUS</td>\n","      <td>Bark</td>\n","      <td>52:56</td>\n","      <td>Though it's been crowded most times I've gone ...</td>\n","    </tr>\n","    <tr>\n","      <th>373</th>\n","      <td>positive</td>\n","      <td>FOOD#QUALITY</td>\n","      <td>food</td>\n","      <td>4:8</td>\n","      <td>The food is excellent!</td>\n","    </tr>\n","    <tr>\n","      <th>374</th>\n","      <td>negative</td>\n","      <td>FOOD#QUALITY</td>\n","      <td>chow fun and chow see</td>\n","      <td>3:24</td>\n","      <td>My chow fun and chow see was really bland and ...</td>\n","    </tr>\n","    <tr>\n","      <th>375</th>\n","      <td>positive</td>\n","      <td>FOOD#QUALITY</td>\n","      <td>pasta mains</td>\n","      <td>75:86</td>\n","      <td>The anti-pasta was excellent, especially the c...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>376 rows × 5 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc886558-81a3-4ebd-9b51-1053300f02e6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-fc886558-81a3-4ebd-9b51-1053300f02e6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-fc886558-81a3-4ebd-9b51-1053300f02e6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["    sentiment           aspect_category            aspect_term  slice  \\\n","0    positive          LOCATION#GENERAL           neighborhood  54:66   \n","1    negative        RESTAURANT#GENERAL                  place  15:20   \n","2    positive              FOOD#QUALITY                   Fish    0:4   \n","3    negative           SERVICE#GENERAL                manager  19:26   \n","4     neutral            DRINKS#QUALITY             margaritas  63:73   \n","..        ...                       ...                    ...    ...   \n","371  positive        RESTAURANT#GENERAL                  PLACE  58:63   \n","372   neutral  RESTAURANT#MISCELLANEOUS                   Bark  52:56   \n","373  positive              FOOD#QUALITY                   food    4:8   \n","374  negative              FOOD#QUALITY  chow fun and chow see   3:24   \n","375  positive              FOOD#QUALITY            pasta mains  75:86   \n","\n","                                                review  \n","0    great food, great wine list, great service in ...  \n","1          I thought this place was totally overrated.  \n","2                               Fish is so very fresh.  \n","3    I showed it to the manager, and he smilingly a...  \n","4    The food we ordered was excellent, although I ...  \n","..                                                 ...  \n","371  I CAN EAT HERE EVERY DAY OF THE WEEK REALLY LO...  \n","372  Though it's been crowded most times I've gone ...  \n","373                            The food is excellent!   \n","374  My chow fun and chow see was really bland and ...  \n","375  The anti-pasta was excellent, especially the c...  \n","\n","[376 rows x 5 columns]"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["classifier.get_embeddings(classifier.loadfile(devfile))[:2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u_cfJa8DEbMK","executionInfo":{"status":"ok","timestamp":1646769925100,"user_tz":-60,"elapsed":6391,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"d97b6b5f-72b6-463f-acb6-e1e25c95d26e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[-0.057926  ,  0.34858805, -0.5094472 , ...,  0.49975532,\n","         -0.38258582,  0.77565813],\n","        [-0.027873  ,  0.48880327, -0.7928846 , ...,  0.2693749 ,\n","         -0.47329918,  0.6508599 ],\n","        [-0.07967386,  0.5496016 , -0.8315101 , ...,  0.32427025,\n","         -0.5050349 ,  0.65480244],\n","        ...,\n","        [-0.8333128 ,  0.38822085, -0.52785254, ...,  0.31770483,\n","         -0.292842  ,  1.8338207 ],\n","        [-0.8333128 ,  0.38822085, -0.52785254, ...,  0.31770483,\n","         -0.292842  ,  1.8338207 ],\n","        [-0.8333128 ,  0.38822085, -0.52785254, ...,  0.31770483,\n","         -0.292842  ,  1.8338207 ]],\n","\n","       [[-0.09319579,  0.39213178, -0.5307334 , ...,  0.4637822 ,\n","         -0.40383434,  0.82813793],\n","        [-0.01807237,  0.57009214, -1.0802944 , ...,  0.1709255 ,\n","         -0.5365272 ,  0.73843217],\n","        [-0.08067247,  0.54335475, -1.1716623 , ...,  0.02692217,\n","         -0.6566367 ,  0.82870704],\n","        ...,\n","        [-0.97984093,  0.52103263, -0.6923128 , ...,  0.10592414,\n","         -0.06055476,  2.3178513 ],\n","        [-0.97984093,  0.52103263, -0.6923128 , ...,  0.10592414,\n","         -0.06055476,  2.3178513 ],\n","        [-0.97984093,  0.52103263, -0.6923128 , ...,  0.10592414,\n","         -0.06055476,  2.3178513 ]]], dtype=float32)"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["classifier.get_embeddings(classifier.loadfile(devfile)).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"79_8kY5UEbQU","executionInfo":{"status":"ok","timestamp":1646769918733,"user_tz":-60,"elapsed":6399,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"38fad465-f0de-4ed1-fa69-dcfcb6cb96b3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(376, 90, 1024)"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["print(sklearn.metrics.classification_report(glabels, slabels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_VseSqD7EbYT","executionInfo":{"status":"ok","timestamp":1646772760099,"user_tz":-60,"elapsed":217,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"5125e8f8-7e68-4adb-810c-0ee24297fed6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","    negative       0.81      0.63      0.71        98\n","     neutral       0.00      0.00      0.00        14\n","    positive       0.86      0.97      0.91       264\n","\n","    accuracy                           0.85       376\n","   macro avg       0.55      0.54      0.54       376\n","weighted avg       0.81      0.85      0.83       376\n","\n"]}]},{"cell_type":"code","source":["classifier.get_embeddings(classifier.loadfile(devfile), to_print=True)"],"metadata":{"id":"IqAemMeDExXr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Yt1UwjuoExbS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Qbx3u0LfExel"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"_ssWsa6NExh2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # 一个句子一个句子来看\n","# def predict(datafile):\n","#     \"\"\"Predicts class labels for the input instances in file 'datafile'\n","#     Returns the list of predicted labels\n","#     \"\"\"\n","    \n","#     # loading the test file\n","#     columns = ['sentiment','aspect_category','aspect_term','slice','review']\n","#     df2 = pd.read_csv(datafile,sep='\\t',names = columns, header = None)\n","    \n","#     tokenizer = RobertaTokenizer.from_pretrained('roberta-large-mnli')\n","#     bert_model = TFRobertaModel.from_pretrained('roberta-large-mnli')\n","\n","#     # parameters for getting embedding\n","#     max_token_dict = {'asp_cat_emb':16,'asp_term_emb':24,'review_emb':50}\n","#     src_column_dict = {'asp_cat_emb':'aspect_category','asp_term_emb':'aspect_term','review_emb':'review'}\n","\n","#     encoder = LabelEncoder()\n","    \n","#     # get embeddings\n","#     emb_list = []\n","#     for col, MAX_LENGTH in max_token_dict.items():\n","#         str_inp = df2[src_column_dict[col]].values\n","#         inputs = tokenizer([str(i) for i in str_inp],\n","#                       max_length = MAX_LENGTH,\n","#                       pad_to_max_length = True, return_tensors=\"pt\",truncation=True)\n","    \n","#         inputs = [np.array(v) for _,v in inputs.items()]\n","#         out = bert_model.predict(inputs)\n","#         emb_list.append(out[0])\n","  \n","#     X2 = np.concatenate(emb_list,axis =1)\n","\n","#     # Predictions\n","#     model= tf.keras.Sequential()\n","#     model.add(tf.keras.layers.Dense(2048))\n","#     model.add(tf.keras.layers.Dense(512))\n","#     model.add(tf.keras.layers.Dense(256))\n","#     model.add(tf.keras.layers.Dense(128))\n","#     model.add(tf.keras.layers.Flatten())\n","#     model.add(tf.keras.layers.Dense(4000))\n","#     model.add(tf.keras.layers.Dropout(0.2))\n","#     model.add(tf.keras.layers.Dense(1250))\n","#     model.add(tf.keras.layers.Dropout(0.2))\n","#     model.add(tf.keras.layers.Dense(512))\n","#     model.add(tf.keras.layers.Dropout(0.2))\n","#     model.add(tf.keras.layers.Dense(256,activation= tf.nn.leaky_relu))\n","#     model.add(tf.keras.layers.Dropout(0.2))\n","#     model.add(tf.keras.layers.Dense(64)) #tf.nn.leaky_relu\n","#     model.add(tf.keras.layers.Dropout(0.2))\n","#     model.add(tf.keras.layers.Dense(16,activation= tf.nn.leaky_relu))\n","#     model.add(tf.keras.layers.Dropout(0.1))\n","#     model.add(tf.keras.layers.Dense(units=3, activation='softmax'))\n","\n","#     pred = model.predict(X2)\n","    \n","#     # Encoding to original class labels\n","#     pred = np.argmax(pred, axis= 1)\n","#     y2_pred = encoder.inverse_transform(pred)\n","    \n","#     return list(y2_pred)"],"metadata":{"id":"NXLiDPMJ4c0_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict(devfile)"],"metadata":{"id":"J8_3WNqF4c4m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"hYmoqT2_6HtP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test file"],"metadata":{"id":"gzyTii-z6nAS"}},{"cell_type":"code","source":["import time, sys\n","import numpy as np\n","\n","\n","def set_reproducible():\n","    # The below is necessary to have reproducible behavior.\n","    import random as rn\n","    import os\n","    os.environ['PYTHONHASHSEED'] = '0'\n","    # The below is necessary for starting Numpy generated random numbers\n","    # in a well-defined initial state.\n","    np.random.seed(17)\n","    # The below is necessary for starting core Python generated random numbers\n","    # in a well-defined state.\n","    rn.seed(12345)\n","\n","\n","\n","def load_label_output(filename):\n","    with open(filename, 'r', encoding='UTF-8') as f:\n","        return [line.strip().split(\"\\t\")[0] for line in f if line.strip()]\n","\n","\n","\n","def eval_list(glabels, slabels):\n","    if (len(glabels) != len(slabels)):\n","        print(\"\\nWARNING: label count in system output (%d) is different from gold label count (%d)\\n\" % (\n","        len(slabels), len(glabels)))\n","    n = min(len(slabels), len(glabels))\n","    incorrect_count = 0\n","    for i in range(n):\n","        if slabels[i] != glabels[i]: incorrect_count += 1\n","    acc = (n - incorrect_count) / n\n","    return acc*100\n","\n","\n","\n","def train_and_eval(classifier, trainfile, devfile, testfile, run_id):\n","    print(f\"\\nRUN: {run_id}\")\n","    print(\"  %s.1. Training the classifier...\" % str(run_id))\n","    # classifier.train(trainfile, devfile)\n","    classifier.train(trainfile, devfile)\n","    print()\n","    print(\"  %s.2. Eval on the dev set...\" % str(run_id), end=\"\")\n","    slabels = classifier.predict(devfile)\n","    glabels = load_label_output(devfile)\n","    devacc = eval_list(glabels, slabels)\n","    print(\" Acc.: %.2f\" % devacc)\n","    testacc = -1\n","    if testfile is not None:\n","        # Evaluation on the test data\n","        print(\"  %s.3. Eval on the test set...\" % str(run_id), end=\"\")\n","        slabels = classifier.predict(testfile)\n","        glabels = load_label_output(testfile)\n","        testacc = eval_list(glabels, slabels)\n","        print(\" Acc.: %.2f\" % testacc)\n","    print()\n","    return (devacc, testacc)\n","\n","\n","# if __name__ == \"__main__\":\n","#     set_reproducible()\n","#     # n_runs = 5\n","#     n_runs = 2\n","#     # if len(sys.argv) > 1:\n","#     #     print(sys.argv)\n","#     #     n_runs = int(sys.argv[1]) # 这里注释掉了，我没看懂什么意思\n","#     datadir = \"../data/\"\n","#     # trainfile =  datadir + \"traindata.csv\"\n","#     # devfile =  datadir + \"devdata.csv\"\n","#     trainfile =  '/content/drive/MyDrive/DSBA M2/2 NLP/exercise_2/data/traindata.csv'\n","#     devfile =  '/content/drive/MyDrive/DSBA M2/2 NLP/exercise_2/data/devdata.csv'\n","#     testfile = None\n","#     # testfile = datadir + \"testdata.csv\"\n","\n","#     # Runs\n","#     start_time = time.perf_counter()\n","#     devaccs = []\n","#     testaccs = []\n","#     for i in range(1, n_runs+1):\n","#         classifier =  Classifier()\n","#         devacc, testacc = train_and_eval(classifier, trainfile, devfile, testfile, i)\n","#         devaccs.append(np.round(devacc,2))\n","#         testaccs.append(np.round(testacc,2))\n","#     print('\\nCompleted %d runs.' % n_runs)\n","#     total_exec_time = (time.perf_counter() - start_time)\n","#     print(\"Dev accs:\", devaccs)\n","#     print(\"Test accs:\", testaccs)\n","#     print()\n","#     print(\"Mean Dev Acc.: %.2f (%.2f)\" % (np.mean(devaccs), np.std(devaccs)))\n","#     print(\"Mean Test Acc.: %.2f (%.2f)\" % (np.mean(testaccs), np.std(testaccs)))\n","#     print(\"\\nExec time: %.2f s. ( %d per run )\" % (total_exec_time, total_exec_time / n_runs))"],"metadata":{"id":"YELrHQTK1v4X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sys.argv"],"metadata":{"id":"PgI_o2X4ULZ9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646534478463,"user_tz":-60,"elapsed":6,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"05726b01-c6db-4058-b1fa-c2b87b3ed4c7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py',\n"," '-f',\n"," '/root/.local/share/jupyter/runtime/kernel-a6f219ac-c91f-46e8-b0f1-fe08335fb596.json']"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":[""],"metadata":{"id":"gpLzzzA4ULdR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"tvi0ze5rM3sv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PyTorch Version"],"metadata":{"id":"FALuyRsYM43d"}},{"cell_type":"code","source":["torch.device(\"cuda\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w64lHosfwEoQ","executionInfo":{"status":"ok","timestamp":1646780239732,"user_tz":-60,"elapsed":213,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"8580e375-80c2-4418-b86a-889c916b6c77"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import nltk       # >= 3.6.0\n","import sklearn    # >=0.24.0\n","import pandas as pd     # >= 1.3.0\n","import gensim     # >= 4.1.2\n","import stanza     # == 1.3.0\n","import torch      # >= 1.10.0\n","import transformers # version 4.16.x.\n","\n","import pandas as pd\n","import numpy as np\n","import csv\n","import sklearn\n","from tqdm import tqdm\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import sys\n","sys.path.append('..')\n","\n","class Classifier:\n","    \"\"\"The Classifier\"\"\"\n","\n","    def __init__(self):\n","    # initialising the class and loading the BERT model from HuggingFace and giving max embeddings to get for each columns.\n","    # load models\n","        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-large-mnli')\n","        self.bert_model = TFRobertaModel.from_pretrained('roberta-large-mnli')\n","\n","        # parameters for getting embedding\n","        self.max_token_dict = {'asp_cat_emb':16,'asp_term_emb':24,'review_emb':50}\n","        self.src_column_dict = {'asp_cat_emb':'aspect_category','asp_term_emb':'aspect_term','review_emb':'review'}\n","        \n","        # loading variable encoder\n","        self.encoder = LabelEncoder()\n","\n","        # model\n","        self.model = self.create_model()\n","        self.device = torch.device(\"cuda\")\n","        self.model.to(self.device)\n","\n","        # for storing training losses\n","        self.each_train_loss = []\n","\n","    def train(self, trainfile, devfile=None):\n","        # WARNING: DO NOT USE THE DEV DATA AS TRAINING EXAMPLES, YOU CAN USE \n","        # THEM ONLY FOR THE OPTIMIZATION OF MODEL HYPERPARAMETERS Trains the classifier model on the training set stored in file trainfile\n","\n","        # loading the train file\n","        print('loading the train file...')\n","        df = self.loadfile(trainfile)\n","\n","        print('getting embeddings...')\n","        X = self.get_embeddings(df)\n","        Y = df['sentiment'].values\n","\n","        print('encoder fit transform...')\n","        Y = self.encoder.fit_transform(np.array(Y).reshape(-1,1))\n","        X = torch.tensor(X)\n","        Y = torch.tensor(Y)\n","\n","        # device = torch.device(\"cuda\")\n","        X = X.to(self.device)\n","        Y = Y.to(self.device)\n","\n","        # training mode\n","        self.model.train()\n","        self.model.to(self.device)\n","\n","        # optimizer and scheduler\n","        print('optimizing and scheduling...')\n","\n","        optimizer = torch.optim.Adam(self.model.parameters(), 0.0001)\n","        # optimizer = torch.optim.SGD(self.model.parameters(), 0.0001)\n","        # optim = tf.keras.optimizers.Adam(learning_rate=0.001)\n","        \n","        # loss\n","        print('calculating loss...')\n","        criterion = nn.CrossEntropyLoss()\n","\n","        each_train_loss = []\n","        epochs = 100 # 先用5试一下\n","\n","        print('epochs starts')\n","        temp_start = time.time()\n","        for i in tqdm(range(epochs)):\n","\n","            # make everything to 0\n","            optimizer.zero_grad()\n","\n","            # make prediction\n","            self.model.to(self.device)\n","            outputs = self.model(X)\n","\n","            # calculate loss\n","            loss = criterion(outputs, Y)\n","\n","            # backwards\n","            loss.backward()\n","            optimizer.step()\n","\n","            self.each_train_loss.append(loss.item())\n","        \n","        # training\n","        print('training & fit...')\n","        # self.model.fit(X, Y, epochs =50, verbose = 0)\n","        \n","        temp_end = time.time()\n","        print('It takes ', str(round((temp_end - temp_start), 3)), ' seconds to train')\n","\n","\n","    def predict(self, datafile):\n","        \"\"\"Predicts class labels for the input instances in file 'datafile'\n","        Returns the list of predicted labels\n","        \"\"\"\n","        \n","        # loading the test file\n","        df2 = self.loadfile(datafile)\n","        X2 = self.get_embeddings(df2)\n","        X2 = torch.tensor(X2)\n","        X2 = X2.to(self.device)\n","        \n","        # Predictions\n","        # pred = self.model.predict(X2)\n","        # device = torch.device(\"cuda\")\n","        self.model.to(self.device)\n","        pred = self.model(X2)\n","        # print(pred)\n","        pred = pred.cpu().detach().numpy()\n","        \n","        # Encoding to original class labels\n","        pred = np.argmax(pred, axis= 1)\n","        y2_pred = self.encoder.inverse_transform(pred)\n","        \n","        return list(y2_pred)\n","\n","\n","    def loadfile(self, data_csv):\n","        '''\n","        Load the files as pandas dataframe object\n","        '''\n","        columns = ['sentiment','aspect_category','aspect_term','slice','review']\n","        df = pd.read_csv(data_csv,sep='\\t',names = columns, header = None)\n","        return df\n","\n","\n","    def get_embeddings(self, df):\n","        '''\n","        Load embeddings  from the RoBERTa Model\n","        '''\n","        emb_list = []\n","        for col, MAX_LENGTH in self.max_token_dict.items():\n","            str_inp = df[self.src_column_dict[col]].values\n","            inputs = self.tokenizer([str(i) for i in str_inp],\n","                          max_length = MAX_LENGTH,\n","                          pad_to_max_length = True,return_tensors=\"pt\",truncation=True)\n","        \n","            inputs = [np.array(v) for _,v in inputs.items()]\n","            out = self.bert_model.predict(inputs)\n","            emb_list.append(out[0])\n","      \n","        X = np.concatenate(emb_list,axis =1)\n","        return X\n","\n","\n","    def create_model(self):\n","        '''\n","        Final Classifier NN model. \n","        Takes embeddings as input and predicts the class encoded label.\n","        '''\n","\n","        model = nn.Sequential(\n","                      nn.Linear(1024, 2048),\n","                      nn.Linear(2048, 512),\n","                      nn.Linear(512, 256),\n","                      nn.Linear(256, 128),\n","                      nn.Flatten(),\n","\n","                      # nn.Linear(128, 4000),\n","                      nn.Linear(11520, 4000),\n","                      nn.Dropout(p=0.2),\n","\n","                      nn.Linear(4000, 1250),\n","                      nn.Dropout(p=0.2),\n","                      \n","                      nn.Linear(1250, 512),\n","                      nn.Dropout(p=0.2),\n","\n","                      nn.Linear(512, 256),\n","                      nn.LeakyReLU(0.2),\n","                      nn.Dropout(p=0.2),\n","\n","                      nn.Linear(256, 64),\n","                      nn.Dropout(p=0.2),\n","\n","                      nn.Linear(64, 16),\n","                      nn.LeakyReLU(0.2),\n","                      nn.Dropout(p=0.1),\n","\n","                      nn.Linear(16, 3)\n","                      # nn.Softmax()\n","                      )\n","                          \n","        return model"],"metadata":{"id":"KTpsc_kfM3qr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["set_reproducible()\n","n_runs = 5\n","datadir = \"../data/\"\n","trainfile =  '/content/drive/MyDrive/DSBA M2/2 NLP/exercise_2/data/traindata.csv'\n","devfile =  '/content/drive/MyDrive/DSBA M2/2 NLP/exercise_2/data/devdata.csv'\n","testfile = None\n","\n","# Runs\n","start_time = time.perf_counter()\n","devaccs = []\n","testaccs = []\n","for i in range(1, n_runs+1):\n","    classifier =  Classifier()\n","    devacc, testacc = train_and_eval(classifier, trainfile, devfile, testfile, i)\n","    devaccs.append(np.round(devacc, 2))\n","    testaccs.append(np.round(testacc, 2))\n","    print('the loss is: ', classifier.each_train_loss)\n","    \n","print('\\nCompleted %d runs.' % n_runs)\n","total_exec_time = (time.perf_counter() - start_time)\n","print(\"Dev accs:\", devaccs)\n","print(\"Test accs:\", testaccs)\n","print()\n","print(\"Mean Dev Acc.: %.2f (%.2f)\" % (np.mean(devaccs), np.std(devaccs)))\n","print(\"Mean Test Acc.: %.2f (%.2f)\" % (np.mean(testaccs), np.std(testaccs)))\n","print(\"\\nExec time: %.2f s. ( %d per run )\" % (total_exec_time, total_exec_time / n_runs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gt-pRBgnlN2n","executionInfo":{"status":"ok","timestamp":1646782905537,"user_tz":-60,"elapsed":468337,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"55aa4bcc-7c9c-4a85-a0c0-33b2f6c2fcd9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at roberta-large-mnli were not used when initializing TFRobertaModel: ['classifier']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-large-mnli.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["\n","RUN: 1\n","  1.1. Training the classifier...\n","loading the train file...\n","getting embeddings...\n","encoder fit transform...\n","optimizing and scheduling...\n","calculating loss...\n","epochs starts\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:36<00:00,  2.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["training & fit...\n","It takes  36.395  seconds to train\n","\n","  1.2. Eval on the dev set... Acc.: 87.50\n","\n","the loss is:  [1.0977405309677124, 1.0519875288009644, 0.9388105869293213, 0.7984851598739624, 0.8515506982803345, 0.7915442585945129, 0.7654146552085876, 0.7980470657348633, 0.7912758588790894, 0.774084210395813, 0.7609551548957825, 0.7490226030349731, 0.7518850564956665, 0.7504044771194458, 0.7485488057136536, 0.743499755859375, 0.7406907081604004, 0.7402287125587463, 0.7441688179969788, 0.7359353303909302, 0.7316527366638184, 0.743293821811676, 0.731830358505249, 0.7158421874046326, 0.7278779745101929, 0.7219838500022888, 0.719635009765625, 0.7212733030319214, 0.7039272785186768, 0.7131145596504211, 0.7037259936332703, 0.6996718049049377, 0.6917752027511597, 0.6843138933181763, 0.6835312247276306, 0.6727147102355957, 0.6700255274772644, 0.6674694418907166, 0.6549434065818787, 0.65208899974823, 0.6336787939071655, 0.6238657832145691, 0.6098898649215698, 0.5923947691917419, 0.5725078582763672, 0.5795440673828125, 0.6509215235710144, 0.5405580997467041, 0.5861564874649048, 0.5305987000465393, 0.5578611493110657, 0.483572393655777, 0.5281771421432495, 0.4629444181919098, 0.5109037160873413, 0.44674259424209595, 0.4772324562072754, 0.42515939474105835, 0.44237494468688965, 0.4295218586921692, 0.4019440710544586, 0.4313585162162781, 0.3806964159011841, 0.42653918266296387, 0.37556588649749756, 0.4261808395385742, 0.365092396736145, 0.3964925706386566, 0.36691680550575256, 0.39714616537094116, 0.36031055450439453, 0.38095375895500183, 0.36521488428115845, 0.3787531554698944, 0.37060341238975525, 0.35180985927581787, 0.3634278178215027, 0.359409362077713, 0.3566214442253113, 0.34472185373306274, 0.35270097851753235, 0.3337177038192749, 0.34580472111701965, 0.33395954966545105, 0.3369689881801605, 0.3321920931339264, 0.33829495310783386, 0.33540141582489014, 0.32873910665512085, 0.32778093218803406, 0.32453426718711853, 0.3284951448440552, 0.3210655152797699, 0.32689687609672546, 0.3125210106372833, 0.3143545091152191, 0.31496354937553406, 0.31222885847091675, 0.31748849153518677, 0.30934712290763855]\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at roberta-large-mnli were not used when initializing TFRobertaModel: ['classifier']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-large-mnli.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["\n","RUN: 2\n","  2.1. Training the classifier...\n","loading the train file...\n","getting embeddings...\n","encoder fit transform...\n","optimizing and scheduling...\n","calculating loss...\n","epochs starts\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:36<00:00,  2.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["training & fit...\n","It takes  36.404  seconds to train\n","\n","  2.2. Eval on the dev set... Acc.: 87.77\n","\n","the loss is:  [1.165979027748108, 1.1259963512420654, 0.9876649975776672, 0.7947739958763123, 0.9254895448684692, 0.8119766116142273, 0.7501807808876038, 0.7834502458572388, 0.7844245433807373, 0.7624731063842773, 0.7381739020347595, 0.7511001229286194, 0.7567413449287415, 0.7400014400482178, 0.73997563123703, 0.7473169565200806, 0.749054491519928, 0.7465540170669556, 0.7381739020347595, 0.7353038787841797, 0.7502076625823975, 0.7382429242134094, 0.7361026406288147, 0.7216767072677612, 0.7209374904632568, 0.7185457348823547, 0.7181941270828247, 0.7119982838630676, 0.7120673060417175, 0.7083032131195068, 0.7027655839920044, 0.6978038549423218, 0.689099907875061, 0.6965693235397339, 0.6869969964027405, 0.6850834488868713, 0.6727761626243591, 0.6739047765731812, 0.6732456088066101, 0.6688107848167419, 0.6519079804420471, 0.6555505394935608, 0.6521246433258057, 0.6345229744911194, 0.6216958165168762, 0.6162489056587219, 0.6041035652160645, 0.5896255373954773, 0.5690304636955261, 0.5463210940361023, 0.5996549129486084, 0.6140388250350952, 0.4969828426837921, 0.6043190956115723, 0.492102712392807, 0.5512429475784302, 0.4732693135738373, 0.49503859877586365, 0.4632561504840851, 0.46304935216903687, 0.45955508947372437, 0.4268006682395935, 0.4499069154262543, 0.399353563785553, 0.43798398971557617, 0.3888643980026245, 0.4084334969520569, 0.3887172043323517, 0.3866555988788605, 0.39714592695236206, 0.3831947445869446, 0.3857342302799225, 0.3634014427661896, 0.38746708631515503, 0.36241641640663147, 0.38663262128829956, 0.3609907329082489, 0.390421986579895, 0.3546079099178314, 0.36651453375816345, 0.3546733856201172, 0.3630727529525757, 0.3472346365451813, 0.3519783020019531, 0.35962963104248047, 0.348797082901001, 0.3503206670284271, 0.3463855981826782, 0.3391266465187073, 0.33978015184402466, 0.34085410833358765, 0.32471439242362976, 0.3357950747013092, 0.330536425113678, 0.33310258388519287, 0.32082802057266235, 0.32458236813545227, 0.32564979791641235, 0.32331785559654236, 0.3300609588623047]\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at roberta-large-mnli were not used when initializing TFRobertaModel: ['classifier']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-large-mnli.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["\n","RUN: 3\n","  3.1. Training the classifier...\n","loading the train file...\n","getting embeddings...\n","encoder fit transform...\n","optimizing and scheduling...\n","calculating loss...\n","epochs starts\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:36<00:00,  2.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["training & fit...\n","It takes  36.425  seconds to train\n","\n","  3.2. Eval on the dev set... Acc.: 87.77\n","\n","the loss is:  [1.0500218868255615, 1.0131685733795166, 0.8738751411437988, 0.8101620674133301, 0.8130550980567932, 0.7600733637809753, 0.7730361223220825, 0.7745593786239624, 0.7531313300132751, 0.7486962080001831, 0.7470096349716187, 0.744663655757904, 0.7283483147621155, 0.7265585064888, 0.7442070841789246, 0.735657811164856, 0.733401358127594, 0.7242487668991089, 0.7435766458511353, 0.7217280864715576, 0.7242707014083862, 0.7282087206840515, 0.7226179838180542, 0.7115640044212341, 0.7173522710800171, 0.7149165868759155, 0.7164503335952759, 0.7079725861549377, 0.7018657922744751, 0.6979240775108337, 0.6908993124961853, 0.6884250044822693, 0.6809750199317932, 0.6697709560394287, 0.6709455847740173, 0.6568313837051392, 0.6585338711738586, 0.6434084177017212, 0.6389695405960083, 0.6223509311676025, 0.6074385643005371, 0.5935637950897217, 0.5698172450065613, 0.5684476494789124, 0.5810556411743164, 0.5607764720916748, 0.5011728405952454, 0.5413646101951599, 0.49846741557121277, 0.4662590026855469, 0.4908297061920166, 0.42937803268432617, 0.4468553066253662, 0.43326884508132935, 0.4099213182926178, 0.41409382224082947, 0.3872843086719513, 0.40383830666542053, 0.37851405143737793, 0.3815193772315979, 0.3928937017917633, 0.36647266149520874, 0.3730764091014862, 0.3847174346446991, 0.37194597721099854, 0.35495737195014954, 0.3689481317996979, 0.3530145585536957, 0.3606583774089813, 0.35524502396583557, 0.34309718012809753, 0.3564291000366211, 0.3388786017894745, 0.351760596036911, 0.34491199254989624, 0.3348742425441742, 0.3409138321876526, 0.3302426040172577, 0.3383401930332184, 0.32858777046203613, 0.3221704065799713, 0.3225167393684387, 0.31696972250938416, 0.31782424449920654, 0.32265278697013855, 0.3192686438560486, 0.3152199685573578, 0.30777913331985474, 0.3091889023780823, 0.3084529936313629, 0.29882171750068665, 0.29974666237831116, 0.300778865814209, 0.3046852648258209, 0.3000180721282959, 0.30320456624031067, 0.30352017283439636, 0.3056222200393677, 0.2936123311519623, 0.2832731008529663]\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at roberta-large-mnli were not used when initializing TFRobertaModel: ['classifier']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-large-mnli.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["\n","RUN: 4\n","  4.1. Training the classifier...\n","loading the train file...\n","getting embeddings...\n","encoder fit transform...\n","optimizing and scheduling...\n","calculating loss...\n","epochs starts\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:36<00:00,  2.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["training & fit...\n","It takes  36.407  seconds to train\n","\n","  4.2. Eval on the dev set... Acc.: 85.11\n","\n","the loss is:  [0.9886118769645691, 0.9581215381622314, 0.8769978880882263, 0.7554482221603394, 0.8362687230110168, 0.7735928893089294, 0.7857083678245544, 0.7697206735610962, 0.7596205472946167, 0.7498047351837158, 0.7397313117980957, 0.7339783310890198, 0.7344040870666504, 0.7315775156021118, 0.7339692115783691, 0.7218517065048218, 0.7286475300788879, 0.7222419381141663, 0.7197319269180298, 0.7191426157951355, 0.7120681405067444, 0.7073688507080078, 0.7186498641967773, 0.7095806002616882, 0.7064765095710754, 0.6986330151557922, 0.6923379898071289, 0.6861690282821655, 0.6801499724388123, 0.6749484539031982, 0.6799889802932739, 0.6696586608886719, 0.6725613474845886, 0.6710067987442017, 0.6680944561958313, 0.6666259169578552, 0.6488478183746338, 0.64266037940979, 0.638488233089447, 0.6281055212020874, 0.6246422529220581, 0.6049442291259766, 0.5928634405136108, 0.5764477849006653, 0.5586705803871155, 0.5431677103042603, 0.5479717254638672, 0.5328753590583801, 0.4854602813720703, 0.4768754839897156, 0.5026983022689819, 0.4481002986431122, 0.42650043964385986, 0.46099281311035156, 0.39955705404281616, 0.4066812992095947, 0.4035419225692749, 0.37541505694389343, 0.3951302766799927, 0.3569353520870209, 0.3872942328453064, 0.3865185081958771, 0.35243457555770874, 0.3676457703113556, 0.3594726622104645, 0.3520323932170868, 0.3570031523704529, 0.34849369525909424, 0.3582870066165924, 0.34603965282440186, 0.345906138420105, 0.34680813550949097, 0.3344743251800537, 0.3348294198513031, 0.3330610990524292, 0.33266210556030273, 0.3235056698322296, 0.3211415708065033, 0.3217594027519226, 0.32117828726768494, 0.31827765703201294, 0.31663012504577637, 0.32275694608688354, 0.31098926067352295, 0.3100341856479645, 0.31136175990104675, 0.29953426122665405, 0.30956602096557617, 0.3062664568424225, 0.3019353151321411, 0.2969509959220886, 0.30168113112449646, 0.286794513463974, 0.29814639687538147, 0.30150848627090454, 0.291458398103714, 0.29248151183128357, 0.28806281089782715, 0.2929046154022217, 0.29514622688293457]\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at roberta-large-mnli were not used when initializing TFRobertaModel: ['classifier']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-large-mnli.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["\n","RUN: 5\n","  5.1. Training the classifier...\n","loading the train file...\n","getting embeddings...\n","encoder fit transform...\n","optimizing and scheduling...\n","calculating loss...\n","epochs starts\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:36<00:00,  2.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["training & fit...\n","It takes  36.403  seconds to train\n","\n","  5.2. Eval on the dev set... Acc.: 87.77\n","\n","the loss is:  [0.9853392839431763, 0.9614309072494507, 0.8883952498435974, 0.7760313749313354, 0.7787042856216431, 0.8353460431098938, 0.7632879614830017, 0.7464967370033264, 0.743514358997345, 0.751507580280304, 0.7617197036743164, 0.749642014503479, 0.742085337638855, 0.7378603219985962, 0.7254273891448975, 0.7347118854522705, 0.7271013855934143, 0.7285395860671997, 0.71587735414505, 0.7228161692619324, 0.7119287848472595, 0.7108550071716309, 0.7062638401985168, 0.6995516419410706, 0.7005006074905396, 0.6873953938484192, 0.6926607489585876, 0.6890223622322083, 0.7015345692634583, 0.6876016855239868, 0.6786012649536133, 0.6802892684936523, 0.6683964729309082, 0.6601879000663757, 0.6525802612304688, 0.6427298188209534, 0.6410078406333923, 0.6341890096664429, 0.6286835074424744, 0.6130013465881348, 0.6015526056289673, 0.5746905207633972, 0.5674437880516052, 0.5862661600112915, 0.561068058013916, 0.5117164254188538, 0.5497169494628906, 0.49326419830322266, 0.5096672177314758, 0.47877588868141174, 0.4557843804359436, 0.45470333099365234, 0.4252910912036896, 0.4391671121120453, 0.39744696021080017, 0.4131256639957428, 0.3931035101413727, 0.3945620357990265, 0.40546002984046936, 0.37869763374328613, 0.3761110305786133, 0.3935678005218506, 0.3638370931148529, 0.3855119049549103, 0.36555880308151245, 0.3595387637615204, 0.36501508951187134, 0.3433227837085724, 0.3562089800834656, 0.3457558751106262, 0.3514251708984375, 0.3469434082508087, 0.34575462341308594, 0.3433598279953003, 0.3338812589645386, 0.34193482995033264, 0.32433027029037476, 0.32685431838035583, 0.3224366307258606, 0.31829383969306946, 0.3232016861438751, 0.31499525904655457, 0.30981937050819397, 0.3113856911659241, 0.30826497077941895, 0.29932838678359985, 0.3032819926738739, 0.307635635137558, 0.3036688268184662, 0.30201807618141174, 0.2984654903411865, 0.29858720302581787, 0.2923596501350403, 0.28995752334594727, 0.2865549921989441, 0.2917344570159912, 0.3059200346469879, 0.3209981322288513, 0.31291860342025757, 0.28398552536964417]\n","\n","Completed 5 runs.\n","Dev accs: [87.5, 87.77, 87.77, 85.11, 87.77]\n","Test accs: [-1, -1, -1, -1, -1]\n","\n","Mean Dev Acc.: 87.18 (1.04)\n","Mean Test Acc.: -1.00 (0.00)\n","\n","Exec time: 468.11 s. ( 93 per run )\n"]}]},{"cell_type":"code","source":["set_reproducible()\n","n_runs = 5\n","datadir = \"../data/\"\n","trainfile =  '/content/drive/MyDrive/DSBA M2/2 NLP/exercise_2/data/traindata.csv'\n","devfile =  '/content/drive/MyDrive/DSBA M2/2 NLP/exercise_2/data/devdata.csv'\n","testfile = None\n","\n","slabels = classifier.predict(devfile)\n","glabels = load_label_output(devfile)"],"metadata":{"id":"lujzzNXxlXF0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(sklearn.metrics.classification_report(glabels, slabels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uDgBIJoslXRn","executionInfo":{"status":"ok","timestamp":1646782914113,"user_tz":-60,"elapsed":13,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"ee27d5cd-2b15-4015-f554-03f4d770d98b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","    negative       0.77      0.80      0.78        98\n","     neutral       0.00      0.00      0.00        14\n","    positive       0.91      0.94      0.92       264\n","\n","    accuracy                           0.87       376\n","   macro avg       0.56      0.58      0.57       376\n","weighted avg       0.84      0.87      0.85       376\n","\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"k4H6wNDQ5df0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"aeY1aHq-5djs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"D9InCh585dnZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"I_rASs6b5dqy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Following are scratch"],"metadata":{"id":"MWmNT7mL5O7t"}},{"cell_type":"code","source":["set_reproducible()\n","n_runs = 5\n","\n","datadir = \"../data/\"\n","trainfile =  '/content/drive/MyDrive/DSBA M2/2 NLP/exercise_2/data/traindata.csv'\n","devfile =  '/content/drive/MyDrive/DSBA M2/2 NLP/exercise_2/data/devdata.csv'\n","testfile = None"],"metadata":{"id":"bVkTPer_YBH6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 先运行一遍试一下\n","classifier2 =  Classifier()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5VAMKrgYOFH","executionInfo":{"status":"ok","timestamp":1646777251839,"user_tz":-60,"elapsed":5624,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"d058457f-7f2f-4565-d4dd-3d2be1790cc0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at roberta-large-mnli were not used when initializing TFRobertaModel: ['classifier']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-large-mnli.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]}]},{"cell_type":"code","source":["devacc, testacc = train_and_eval(classifier2, trainfile, devfile, testfile, i)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jb6TIRjPYOIh","executionInfo":{"status":"ok","timestamp":1646777350474,"user_tz":-60,"elapsed":98641,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"b0269316-6896-42f0-d2d3-23ee9b0482ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","RUN: 5\n","  5.1. Training the classifier...\n","loading the train file...\n","optimizing and scheduling...\n","calculating loss...\n","epochs starts\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2/2 [00:44<00:00, 22.02s/it]\n"]},{"output_type":"stream","name":"stdout","text":["training & fit...\n","It takes  44.053  seconds to train\n","\n","  5.2. Eval on the dev set... Acc.: 70.21\n","\n"]}]},{"cell_type":"code","source":["devaccs.append(np.round(devacc,2))\n","testaccs.append(np.round(testacc,2))"],"metadata":{"id":"E27G8UaDYOMC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"iGozx4Z7ktbV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"GblZcqubktjc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"9kPwVujaktpZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Wzfe24ibYURk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(trainfile, model, devfile=None):\n","    # WARNING: DO NOT USE THE DEV DATA AS TRAINING EXAMPLES, YOU CAN USE \n","    # THEM ONLY FOR THE OPTIMIZATION OF MODEL HYPERPARAMETERS Trains the classifier model on the training set stored in file trainfile\n","\n","    # loading the train file\n","    print('loading the train file...')\n","    df = loadfile(trainfile)\n","    X = get_embeddings(df)\n","    Y = df['sentiment'].values\n","    Y = encoder.fit_transform(np.array(Y).reshape(-1,1))\n","    X = torch.tensor(X)\n","    Y = torch.tensor(Y)\n","\n","    # optimizer and scheduler\n","    print('optimizing and scheduling...')\n","\n","    optimizer = torch.optim.Adam(model.parameters(), 0.001)\n","    # optim = tf.keras.optimizers.Adam(learning_rate=0.001)\n","    \n","    # loss\n","    print('calculating loss...')\n","    criterion = nn.CrossEntropyLoss()\n","\n","    each_train_loss = []\n","    epochs = 1 # 先用5试一下\n","\n","    print('epochs starts')\n","    temp_start = time.time()\n","    for i in tqdm(range(epochs)):\n","        # change to GPU\n","        # inputs = inputs.to(device)\n","        # labels = labels.to(device)\n","\n","        # make everything to 0\n","        optimizer.zero_grad()\n","\n","        # make prediction\n","        outputs = model(X)\n","\n","        # calculate loss\n","        loss = criterion(outputs, Y)\n","\n","        # backwards\n","        loss.backward()\n","        optimizer.step()\n","\n","        each_train_loss.append(loss.item())\n","    \n","    # self.model.compile(optimizer=optim, loss= 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n","    \n","    # training\n","    print('training & fit...')\n","    # self.model.fit(X, Y, epochs =50, verbose = 0)\n","    \n","    temp_end = time.time()\n","    print('It takes ', str(round((temp_end - temp_start), 3)), ' seconds to train')\n","\n","\n","def predict(datafile, model):\n","    \"\"\"Predicts class labels for the input instances in file 'datafile'\n","    Returns the list of predicted labels\n","    \"\"\"\n","    \n","    # loading the test file\n","    df2 = loadfile(datafile)\n","    X2 = get_embeddings(df2)\n","    X2 = torch.tensor(X2)\n","    \n","    # Predictions\n","    # pred = self.model.predict(X2)\n","    pred = model(X2)\n","    # print(pred)\n","    pred = pred.detach().numpy()\n","\n","    # Encoding to original class labels\n","    pred = np.argmax(pred, axis= 1)\n","    y2_pred = encoder.inverse_transform(pred)\n","    \n","    return list(y2_pred)\n","\n","\n","def loadfile(data_csv):\n","    '''\n","    Load the files as pandas dataframe object\n","    '''\n","    columns = ['sentiment','aspect_category','aspect_term','slice','review']\n","    df = pd.read_csv(data_csv,sep='\\t',names = columns, header = None)\n","    return df\n","\n","\n","def get_embeddings(df):\n","    '''\n","    Load embeddings  from the RoBERTa Model\n","    '''\n","    emb_list = []\n","    for col, MAX_LENGTH in max_token_dict.items():\n","        str_inp = df[src_column_dict[col]].values\n","        inputs = tokenizer([str(i) for i in str_inp],\n","                      max_length = MAX_LENGTH,\n","                      pad_to_max_length = True,return_tensors=\"pt\",truncation=True)\n","    \n","        inputs = [np.array(v) for _,v in inputs.items()]\n","        out = bert_model.predict(inputs)\n","        emb_list.append(out[0])\n","  \n","    X = np.concatenate(emb_list,axis =1)\n","    return X\n","\n","\n","def create_model():\n","    '''\n","    Final Classifier NN model. \n","    Takes embeddings as input and predicts the class encoded label.\n","    '''\n","    # model= tf.keras.Sequential()\n","    # model.add(tf.keras.layers.Dense(2048))\n","    # model.add(tf.keras.layers.Dense(512))\n","    # model.add(tf.keras.layers.Dense(256))\n","    # model.add(tf.keras.layers.Dense(128))\n","    # model.add(tf.keras.layers.Flatten())\n","    # model.add(tf.keras.layers.Dense(4000))\n","    # model.add(tf.keras.layers.Dropout(0.2))\n","\n","    # model.add(tf.keras.layers.Dense(1250))\n","    # model.add(tf.keras.layers.Dropout(0.2))\n","\n","    # model.add(tf.keras.layers.Dense(512))\n","    # model.add(tf.keras.layers.Dropout(0.2))\n","\n","    # model.add(tf.keras.layers.Dense(256,activation= tf.nn.leaky_relu))\n","    # model.add(tf.keras.layers.Dropout(0.2))\n","\n","    # model.add(tf.keras.layers.Dense(64)) #tf.nn.leaky_relu\n","    # model.add(tf.keras.layers.Dropout(0.2))\n","    # model.add(tf.keras.layers.Dense(16,activation= tf.nn.leaky_relu))\n","    # model.add(tf.keras.layers.Dropout(0.1))\n","    # model.add(tf.keras.layers.Dense(units=3, activation='softmax'))\n","\n","\n","    model = nn.Sequential(\n","                  nn.Linear(1024, 2048),\n","                  nn.Linear(2048, 512),\n","                  nn.Linear(512, 256),\n","                  nn.Linear(256, 128),\n","                  nn.Flatten(),\n","\n","                  # nn.Linear(128, 4000),\n","                  nn.Linear(11520, 4000),\n","                  nn.Dropout(p=0.2),\n","\n","                  nn.Linear(4000, 1250),\n","                  nn.Dropout(p=0.2),\n","                  \n","                  nn.Linear(1250, 512),\n","                  nn.Dropout(p=0.2),\n","\n","                  nn.Linear(512, 256),\n","                  nn.LeakyReLU(0.2),\n","                  nn.Dropout(p=0.2),\n","\n","                  nn.Linear(256, 64),\n","                  nn.Dropout(p=0.2),\n","\n","                  nn.Linear(64, 16),\n","                  nn.LeakyReLU(0.2),\n","                  nn.Dropout(p=0.1),\n","\n","                  nn.Linear(16, 3),\n","                  nn.Softmax()\n","                  )\n","                      \n","    return model"],"metadata":{"id":"rHssyJyrYUVB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialising the class and loading the BERT model from HuggingFace and giving max embeddings to get for each columns.\n","# load models\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-large-mnli')\n","bert_model = TFRobertaModel.from_pretrained('roberta-large-mnli')\n","\n","# parameters for getting embedding\n","max_token_dict = {'asp_cat_emb':16,'asp_term_emb':24,'review_emb':50}\n","src_column_dict = {'asp_cat_emb':'aspect_category','asp_term_emb':'aspect_term','review_emb':'review'}\n","\n","# loading variable encoder\n","encoder = LabelEncoder()\n","\n","# model\n","model = create_model()\n","\n","# for storing training losses\n","each_train_loss = []"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7-kjQoNRhtpo","executionInfo":{"status":"ok","timestamp":1646776590449,"user_tz":-60,"elapsed":5367,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"beccccf2-0b5a-4c22-f103-a59e69f5387e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at roberta-large-mnli were not used when initializing TFRobertaModel: ['classifier']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-large-mnli.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]}]},{"cell_type":"code","source":["# def train_and_eval(classifier, trainfile, devfile, testfile, run_id):\n","# classifier.train(trainfile, devfile)\n","train(trainfile, model=model, devfile=devfile)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gfr3d58Qia2E","executionInfo":{"status":"ok","timestamp":1646776973177,"user_tz":-60,"elapsed":47419,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"4be4a4d0-122d-4fcc-9772-11160e8ac444"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loading the train file...\n","optimizing and scheduling...\n","calculating loss...\n","epochs starts\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:22<00:00, 22.27s/it]"]},{"output_type":"stream","name":"stdout","text":["training & fit...\n","It takes  22.276  seconds to train\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["slabels = predict(devfile, model=model)"],"metadata":{"id":"D3hjfY6zjEsA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","Counter(slabels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fYu1KSsjkVfT","executionInfo":{"status":"ok","timestamp":1646777179393,"user_tz":-60,"elapsed":213,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"ccd37117-7013-4bbc-c6c5-2bbde03c9d01"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({'positive': 376})"]},"metadata":{},"execution_count":112}]},{"cell_type":"code","source":["glabels = load_label_output(devfile)\n","devacc = eval_list(glabels, slabels)\n","print(\" Acc.: %.2f\" % devacc)\n","testacc = -1\n","if testfile is not None:\n","    # Evaluation on the test data\n","    print(\"  %s.3. Eval on the test set...\" % str(run_id), end=\"\")\n","    slabels = classifier.predict(testfile)\n","    glabels = load_label_output(testfile)\n","    testacc = eval_list(glabels, slabels)\n","    print(\" Acc.: %.2f\" % testacc)\n","# print()\n","# return (devacc, testacc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"83Sw5q0TjqB7","executionInfo":{"status":"ok","timestamp":1646777189575,"user_tz":-60,"elapsed":200,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}},"outputId":"003cda47-bf4f-4473-fc4d-2838748c5e95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Acc.: 70.21\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"ROZknfhviPwN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"8TrGvyiKiP-F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"c0EPIXUnhty_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"6D3fqSIzht3A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"LZV-wm6Cht7l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Runs\n","start_time = time.perf_counter()\n","devaccs = []\n","testaccs = []\n","for i in range(1, n_runs+1):\n","    classifier =  Classifier()\n","    devacc, testacc = train_and_eval(classifier, trainfile, devfile, testfile, i)\n","    devaccs.append(np.round(devacc,2))\n","    testaccs.append(np.round(testacc,2))\n"],"metadata":{"id":"H4kR0b9gYBNM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('\\nCompleted %d runs.' % n_runs)\n","total_exec_time = (time.perf_counter() - start_time)\n","print(\"Dev accs:\", devaccs)\n","print(\"Test accs:\", testaccs)\n","print()\n","print(\"Mean Dev Acc.: %.2f (%.2f)\" % (np.mean(devaccs), np.std(devaccs)))\n","print(\"Mean Test Acc.: %.2f (%.2f)\" % (np.mean(testaccs), np.std(testaccs)))\n","print(\"\\nExec time: %.2f s. ( %d per run )\" % (total_exec_time, total_exec_time / n_runs))"],"metadata":{"id":"J5gcqyUyYBRQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"wYEPs7p7YBUq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"2JrjZfenYBX_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"niD9LMnqYBb4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ntNzPPpCM3dr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"oh0_t1Y0M3bI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"CyLk7pUHM3ZK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"OJozOz8wM3XG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"q0TKZVCmM3VC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"CJ0wcTJ7M3SS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"mLfHd3PfM3P0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"0HMyjDxNM3Nm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"YlwViiJ0M3Le"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"H4-8WfkRM3JJ"},"execution_count":null,"outputs":[]}]}