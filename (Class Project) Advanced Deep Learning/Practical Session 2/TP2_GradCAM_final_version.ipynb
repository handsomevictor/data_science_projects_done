{"cells":[{"cell_type":"markdown","metadata":{"id":"uRqNBEa9dYo9"},"source":["## Visualization of CNN: Grad-CAM\n","* **Objective**: Convolutional Neural Networks are widely used on computer vision. It is powerful for processing grid-like data. However we hardly know how and why it works, due to the lack of decomposability into individually intuitive components. In this assignment, we use Grad-CAM, which highlights the regions of the input image that were important for the neural network prediction.\n","\n","* **To be submitted by next session**: this notebook, **cleaned** (i.e. without results, for file size reasons: `menu > kernel > restart and clean`), in a state ready to be executed (if one just presses 'Enter' till the end, one should obtain all the results for all images) with a few comments at the end. No additional report, just the notebook!\n","\n","* NB: if `PIL` is not installed, try `conda install pillow`.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"-ltxcgdodYo_","executionInfo":{"status":"ok","timestamp":1643926208281,"user_tz":-60,"elapsed":7030,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import models, datasets, transforms\n","import matplotlib.pyplot as plt\n","import pickle\n","import urllib.request\n","\n","import numpy as np\n","from PIL import Image\n","\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"DHWhpp5cdYpB"},"source":["### Download the Model\n","We provide you a pretrained model `ResNet-34` for `ImageNet` classification dataset.\n","* **ImageNet**: A large dataset of photographs with 1 000 classes.\n","* **ResNet-34**: A deep architecture for image classification."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdyK9SoNdYpC","scrolled":true},"outputs":[],"source":["resnet34 = models.resnet34(pretrained=True)\n","resnet34.eval() # set the model to evaluation mode"]},{"cell_type":"markdown","metadata":{"id":"XxZ30HOhdYpD"},"source":["![ResNet34](https://miro.medium.com/max/1050/1*Y-u7dH4WC-dXyn9jOG4w0w.png)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"idWwaKX_dYpD","executionInfo":{"status":"ok","timestamp":1643926087844,"user_tz":-60,"elapsed":241,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}}},"outputs":[],"source":["classes = pickle.load(urllib.request.urlopen('https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl') )"]},{"cell_type":"markdown","metadata":{"id":"I1cXhrYhdYpE"},"source":["### Input Images\n","We provide you 20 images from ImageNet (download link on the webpage of the course or download directly using the following command line,).<br>\n","In order to use the pretrained model resnet34, the input image should be normalized using `mean = [0.485, 0.456, 0.406]`, and `std = [0.229, 0.224, 0.225]`, and be resized as `(224, 224)`."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"WpbxTdwXdYpH","executionInfo":{"status":"ok","timestamp":1643926088911,"user_tz":-60,"elapsed":2,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}}},"outputs":[],"source":["def preprocess_image(dir_path):\n","    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                     std=[0.229, 0.224, 0.225])\n","\n","    dataset = datasets.ImageFolder(dir_path, transforms.Compose([\n","            transforms.Resize(256), \n","            transforms.CenterCrop(224), # resize the image to 224x224\n","            transforms.ToTensor(), # convert numpy.array to tensor\n","            normalize])) #normalize the tensor\n","\n","    return (dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_i4sHAcMdYpJ","scrolled":true},"outputs":[],"source":["# The images should be in a *sub*-folder of \"data/\" (ex: data/TP2_images/images.jpg) and *not* directly in \"data/\"!\n","# otherwise the function won't find them\n","!rm -rf data\n","\n","import os\n","os.mkdir(\"data\")\n","os.mkdir(\"data/TP2_images\")\n","!cd data/TP2_images && wget \"https://www.lri.fr/~gcharpia/deeppractice/2022/TP2/TP2_images.zip\" && unzip TP2_images.zip\n","# dir_path = project_folder+\"/data/\" \n","dataset = preprocess_image(\"data/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9DeMqRFdYpM"},"outputs":[],"source":["# show the orignal image \n","index = 5\n","input_image = Image.open(dataset.imgs[index][0]).convert('RGB')\n","plt.imshow(input_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YAcvG-JwdYpO"},"outputs":[],"source":["output = resnet34(dataset[index][0].view(1, 3, 224, 224))\n","values, indices = torch.topk(output, 3)\n","print(\"Top 3-classes:\", indices[0].numpy(), [classes[x] for x in indices[0].numpy()])\n","print(\"Raw class scores:\", values[0].detach().numpy())"]},{"cell_type":"markdown","metadata":{"id":"7EuD9vrndYpP"},"source":["### Grad-CAM \n","* **Overview:** Given an image, and a category (â€˜tiger catâ€™) as input, we forward-propagate the image through the model to obtain the `raw class scores` before softmax. The gradients are set to zero for all classes except the desired class (tiger cat), which is set to 1. This signal is then backpropagated to the `rectified convolutional feature map` of interest, where we can compute the coarse Grad-CAM localization (blue heatmap).\n","\n","\n","* **To Do**: Define your own function Grad_CAM to achieve the visualization of the given images. For each image, choose the top-3 possible labels as the desired classes. Compare the heatmaps of the three classes, and conclude. \n","\n","\n","* **Hints**: \n"," + We need to record the output and grad_output of the feature maps to achieve Grad-CAM. In pytorch, the function `Hook` is defined for this purpose. Read the tutorial of [hook](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) carefully. \n"," + The pretrained model resnet34 doesn't have an activation function after its last layer, the output is indeed the `raw class scores`, you can use them directly. \n"," + The size of feature maps is 7x7, so your heatmap will have the same size. You need to project the heatmap to the resized image (224x224, not the original one, before the normalization) to have a better observation. The function [`torch.nn.functional.interpolate`](https://pytorch.org/docs/stable/nn.functional.html?highlight=interpolate#torch.nn.functional.interpolate) may help.  \n"," + Here is the link of the paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/pdf/1610.02391.pdf)"]},{"cell_type":"markdown","metadata":{"id":"itqfTdG1dYpR"},"source":["![Grad-CAM](https://da2so.github.io/assets/post_img/2020-08-10-GradCAM/2.png)"]},{"cell_type":"markdown","metadata":{"id":"JZ1u9b5E1JgX"},"source":["https://github.com/jacobgil/pytorch-grad-cam/tree/master/pytorch_grad_cam"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1640,"status":"ok","timestamp":1643926099219,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"},"user_tz":-60},"id":"rlj_iutAwUVm","outputId":"b3e60d51-4aca-4945-c58b-a3b0656225fc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":8}],"source":["\n","resnet34 = models.resnet34(pretrained=True)\n","resnet34.eval() # set the model to evaluation mode\n","0\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"5jSdj9vFgkYd","executionInfo":{"status":"ok","timestamp":1643926100192,"user_tz":-60,"elapsed":2,"user":{"displayName":"Victor Li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16877734298003550452"}}},"outputs":[],"source":["\n","# Compute the gradient weights\n","def get_weights(grads):\n","    return np.mean(grads, axis=(2, 3))\n","\n","# Multiply the gradient weights \n","def get_cam(grads, acts):\n","    weights = get_weights(grads)\n","    return np.sum((weights[:, :, None, None] * acts), axis=1)\n","\n","# Average the grad-cam image over all of the cam images from the filters of layer 4\n","def get_mean_cam(gradients, activations):\n","    cam_images = []\n","    for i in range(len(gradients)):\n","        cam = get_cam(gradients[i].data.numpy(), activations[i].data.numpy())\n","        cam_images.append(np.maximum(cam, 0)[:, None, :]) # ReLU\n","\n","    cam_images = np.concatenate(cam_images, axis=1)\n","    cam_images = np.maximum(cam_images, 0) # ReLU\n","    cam = np.mean(cam_images, axis=1)\n","    cam /= np.max(cam)\n","\n","    return cam\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Ha0YNhHm2tL"},"outputs":[],"source":["\n","\n","activations = []\n","gradients = []\n","\n","# Retrieve the activations of the filters\n","def save_activation(module, input, output):\n","    output = torch.nn.functional.interpolate(output, size=(224, 224), mode=\"bilinear\")\n","    activations.append(output)\n","    \n","def _store_grad(grad):\n","    global gradients\n","    grad = torch.nn.functional.interpolate(grad, size=(224, 224), mode=\"bilinear\")\n","    gradients = gradients + [grad.cpu().detach()] \n","\n","# Retrieve the gradient in the back propagation\n","def save_gradient(module, input, output):\n","    output.register_hook(_store_grad)\n","\n","# Compute the grad cam image for the top k classes detected in an input image\n","def grad_cam(index, k=3):\n","    activations.clear()\n","    gradients.clear()\n","\n","    heatmaps = []\n","    label_classes = None\n","    for i in range(k+1):\n","        # Initialize a new resnet model\n","        resnet34 = models.resnet34(pretrained=True)\n","        resnet34.eval() # set the model to evaluation mode\n","        \n","        target_layers = [resnet34.layer4[0].conv1, resnet34.layer4[0].conv2, \n","                        resnet34.layer4[1].conv1, resnet34.layer4[1].conv2, \n","                        resnet34.layer4[2].conv1, resnet34.layer4[2].conv2]\n","        \n","        # Set up the hooks to retrieve the activations and gradients\n","        for target_layer in target_layers:\n","            target_layer.register_forward_hook(save_activation)\n","            target_layer.register_forward_hook(save_gradient)\n","        \n","        output = resnet34(dataset[index][0].view(1, 3, 224, 224))\n","        # Get the top k predicted labels\n","        values, indices = torch.topk(output, k)\n","        if label_classes is None:\n","            label_classes = [classes[x] for x in indices[0].numpy()]\n","        if i < k:\n","            label = indices[0].numpy()[i]\n","        else:\n","            # Add a random label for comparison\n","            label = np.random.randint(len(classes))\n","            label_classes.append(classes[label] + \" (random)\")\n","        # Back propagate to get the activations and gradients\n","        output[:, label].backward()\n","        # Compute the grad-cam heatmap from the activations and gradients\n","        heatmap = get_mean_cam(gradients[-6:], activations[-6:])\n","        heatmaps.append(heatmap)\n","\n","    return heatmaps, label_classes\n","\n","index = np.random.randint(len(dataset))\n","heatmaps, label_classes = grad_cam(index)\n","\n","fig = plt.figure(figsize=(20, 40))\n","\n","# Plot the heatmaps\n","for i, heatmap in enumerate(heatmaps):\n","    fig.add_subplot(1, 5, i + 1)\n","    plt.title(label_classes[i])\n","    plt.imshow(heatmap[0])\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gRXirLR6mCPZ"},"outputs":[],"source":["\n","import cv2\n","import torchvision.transforms.functional as TF\n","\n","# Merge an image and a heatmap to superpose the two in one image\n","def merge_img_heatmap(img, heatmap):\n","\n","    heatmap = np.uint8(heatmap * 255)[0]\n","    heatmap = Image.fromarray(heatmap)\n","    heatmap = cv2.cvtColor(-np.array(heatmap), cv2.COLOR_BGR2RGB)\n","    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n","\n","    combination = cv2.addWeighted(img, 0.4, heatmap, 0.6, 0)\n","\n","    return combination\n","\n","img = dataset[index][0].view(3, 224, 224)\n","img = img.numpy().transpose(1, 2, 0)\n","img = (img - np.min(img)) / (np.max(img) - np.min(img))\n","img = np.uint8(img * 255)\n","\n","fig = plt.figure(figsize=(20, 40))\n","\n","# Plot the original image\n","fig.add_subplot(1, 5, 1)\n","plt.title(\"Original image\")\n","plt.imshow(img)\n","\n","# Plot the image and its heatmap for a few labels\n","for i, heatmap in enumerate(heatmaps):\n","    fig.add_subplot(1, 5, i + 2)\n","    plt.title(label_classes[i])\n","    plt.imshow(merge_img_heatmap(img, heatmap))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"J_cxlgaGgvd4"},"outputs":[],"source":["\n","for index in range(len(dataset)):\n","\n","    heatmaps, label_classes = grad_cam(index)\n","\n","    img = dataset[index][0].view(3, 224, 224)\n","    img = img.numpy().transpose(1, 2, 0)\n","    img = (img - np.min(img)) / (np.max(img) - np.min(img))\n","    img = np.uint8(img * 255)\n","\n","    fig = plt.figure(figsize=(20, 40))\n","\n","    # Plot the original image\n","    fig.add_subplot(1, 5, 1)\n","    plt.title(\"Original image\")\n","    plt.imshow(img)\n","\n","    # Plot the image and its heatmap for a few labels\n","    for i, heatmap in enumerate(heatmaps):\n","        fig.add_subplot(1, 5, i + 2)\n","        plt.title(label_classes[i])\n","        plt.imshow(merge_img_heatmap(img, heatmap))\n","    \n","    print(\"Done visualizing image {}/{}.\".format(index+1, len(dataset)))\n"]},{"cell_type":"markdown","metadata":{"id":"vmEC0RjUk0bv"},"source":["We have defined our own function Grad_CAM and achieved the visualization of the given images as above, and we have chosen the top 3 possible labels as the desired classes. The fourth image is the result of the GradCam for a random label, and as thought, they does not make any sense.\n","\n","After comparison of the heatmaps with the original image, we think our GradCam works pretty well, as you can see in the final visualization, the heatmap well distinguished the main part of the image. This can tell us where have the machine paid attention to so that in further training, we can make the training process more efficient by applying this attention system."]}],"metadata":{"colab":{"collapsed_sections":[],"name":"TP2_GradCAM_final_version.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}