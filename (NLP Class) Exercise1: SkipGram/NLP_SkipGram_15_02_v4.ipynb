{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TjaPeYyF6bc2"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import argparse\n",
    "import numpy as np\n",
    "import collections\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AUKAjDq_Ejw"
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "WHS8Qh2K6bf3",
    "outputId": "c9f478d9-2d03-4ff7-c044-a162a65216c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n__authors__ = ['Wen TIAN','Pierrick ALLEGRE','Joao Gabriel Lopes De Oliveira','Bhanu Prakash Yedla ']\\n__emails__  = ['wen.tian@student-cs.fr','pierrick.allegre@student-cs.fr','joao-gabriel.lopes@student-cs.fr', 'bhanu-prakash.yedla@student-cs.fr']\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "__authors__ = ['Wen TIAN','Pierrick ALLEGRE','Joao Gabriel Lopes De Oliveira','Bhanu Prakash Yedla ']\n",
    "__emails__  = ['wen.tian@student-cs.fr','pierrick.allegre@student-cs.fr','joao-gabriel.lopes@student-cs.fr', 'bhanu-prakash.yedla@student-cs.fr']\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BGW_x0Q_Ejy"
   },
   "source": [
    "# Training Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IdJ5gAP6bi6"
   },
   "outputs": [],
   "source": [
    "def text2sentences(path):\n",
    "    punctuations = {'!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',',\n",
    "                    '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', \n",
    "                    ']', '^', '_', '`', '{', '|', '}', '~',' '}\n",
    "    \n",
    "    sentences = []\n",
    "    with open(path, encoding = 'utf-8') as f:\n",
    "        for l in f:\n",
    "            words = [''.join(ch for ch in word if ch not in punctuations) for word in l.lower().split()]\n",
    "            if len(words) > 1:      # we have removed the sentences that have only one word.\n",
    "                sentences.append( words )\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IhQEZTO_Ej1"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    if x > 5:\n",
    "        return 1\n",
    "    elif x < -5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYaIsmLqE3a6"
   },
   "outputs": [],
   "source": [
    "trainset_path = './news.en-00002-of-00100'\n",
    "\n",
    "\n",
    "sentences = text2sentences(trainset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsfmz1lh-tCT"
   },
   "outputs": [],
   "source": [
    "class SkipGram:\n",
    "    def __init__(self, sentences, nEmbed=300, negativeRate=5, winSize = 5, minCount = 5):\n",
    "        # Storing parameters as class variables\n",
    "        self.negativeRate = negativeRate\n",
    "        self.winSize = winSize\n",
    "        self.minCount = minCount\n",
    "        self.lr = 0.001\n",
    "        self.nEmbed = nEmbed\n",
    "        \n",
    "        # Introducing a dictionary to track word counts\n",
    "        # key: word - value: number of times word shows in training data\n",
    "        self.word_counts = collections.defaultdict(int)\n",
    "        \n",
    "        # Adding an unknown word tag\n",
    "        # (Used for when calculating similarity on a new dataset)\n",
    "        self.word_counts['UNK_WORD'] = 0\n",
    "        \n",
    "        # Counting words\n",
    "        for row in sentences:\n",
    "            for word in row:\n",
    "                self.word_counts[word] += 1\n",
    "\n",
    "        # Storing count of unique words\n",
    "        self.w_count = len(self.word_counts.keys())\n",
    "        \n",
    "        # Storing a vocabulary of all words\n",
    "        self.vocab = list(self.word_counts.keys())\n",
    "\n",
    "        # Creating dictionaries mapping from index to word and vice versa\n",
    "        self.w2id = dict((word, i) for i, word in enumerate(self.vocab)) \n",
    "        self.id2w = dict((i, word) for i, word in enumerate(self.vocab)) \n",
    "\n",
    "        # Shorten the unigram table to fit in your computer's RAM\n",
    "        simplify_table_ratio = 2\n",
    "        \n",
    "        self.unigram_table = np.zeros(max(sum(self.word_counts.values()), 1000) , dtype=np.uint32)\n",
    "\n",
    "        index_fill = 0 \n",
    "        for word, counts in self.word_counts.items():\n",
    "            counts = np.uint32((np.ceil((counts ** (3/4))) + simplify_table_ratio) // simplify_table_ratio)\n",
    "            for _ in range(counts):\n",
    "                self.unigram_table[index_fill] = self.w2id[word]\n",
    "                index_fill += 1\n",
    "\n",
    "        self.unigram_size = index_fill\n",
    "        self.unigram_table = self.unigram_table[:index_fill]\n",
    "        \n",
    "        # Initialize the W (embedding vectors) and C (context vectors) matrices\n",
    "        self.W = np.random.randn(self.w_count, nEmbed)\n",
    "        self.C = np.random.randn(self.w_count, nEmbed)\n",
    "        \n",
    "        # Ensuring compatibility with the train function\n",
    "        self.trainset = sentences\n",
    "        self.trainWords = 0\n",
    "        self.accLoss= 0\n",
    "        self.loss = []\n",
    "\n",
    "    def sample(self, omit):\n",
    "        \"\"\"samples negative words, ommitting those in set omit\"\"\"\n",
    "        \n",
    "        # Converting from set to list\n",
    "        omit = list(omit)\n",
    "        # Extracting the id of the words to omit\n",
    "        widx=self.id2w.get(omit[0])\n",
    "        cidx=self.id2w.get(omit[1])\n",
    "        \n",
    "        # Extracting random samples from the dataset using the words indexes\n",
    "        sampled_indexes = self.unigram_table[np.random.randint(0, high=self.unigram_size, size = self.negativeRate)]\n",
    "        \n",
    "        # In case an omission was sampled, resample that value\n",
    "        for i, sampled_idx in enumerate(sampled_indexes):\n",
    "            while sampled_idx == cidx or sampled_idx == widx:\n",
    "                sampled_idx = self.unigram_table[np.random.randint(0, high=self.unigram_size, size = self.negativeRate)]\n",
    "                sampled_indexes[i] = sampled_idx\n",
    "        \n",
    "        return sampled_indexes\n",
    "        \n",
    "    def train(self):\n",
    "        self.loss_rec_ = []\n",
    "        for counter, sentence in enumerate(self.trainset):\n",
    "            # Small adjustment adding list() to make python 3 compatible\n",
    "            sentence = list(filter(lambda word: word in self.vocab, sentence))\n",
    "            \n",
    "            for wpos, word in enumerate(sentence):\n",
    "                wIdx = self.w2id[word]\n",
    "                winsize = np.random.randint(self.winSize) + 1\n",
    "                start = max(0, wpos - winsize)\n",
    "                end = min(wpos + winsize + 1, len(sentence))\n",
    "\n",
    "                for context_word in sentence[start:end]:\n",
    "                    ctxtId = self.w2id[context_word]\n",
    "                    if ctxtId == wIdx: continue \n",
    "                    negativeIds = self.sample({wIdx, ctxtId})\n",
    "                    self.trainWord(wIdx, ctxtId, negativeIds)\n",
    "                    # Calculating loss function\n",
    "                    self.accLoss += self.loss_function(wIdx, ctxtId, negativeIds)\n",
    "                    self.trainWords += 1\n",
    "\n",
    "            if counter % 300 == 0:\n",
    "                # Changing print structure to f-string to make python 3 compatible\n",
    "                print(f' > training {counter} of {len(self.trainset)}')\n",
    "                self.loss.append(self.accLoss / self.trainWords)\n",
    "                self.trainWords = 0\n",
    "                self.accLoss = 0.\n",
    "                print(self.loss[-1])\n",
    "            self.loss_rec_.append(self.loss[-1])\n",
    "        return self.loss_rec_\n",
    "\n",
    "    def trainWord(self, wordId, contextId, negativeIds):\n",
    "        \"\"\" Calculates and updates the W and C matrices \"\"\"\n",
    "        # See derivations from https://arxiv.org/pdf/1411.2738.pdf\n",
    "                        \n",
    "        W_update = 0 \n",
    "        W_update -= (sigmoid(np.dot(self.W[wordId,:], self.C[contextId, :])) - 1) * self.C[contextId, :]\n",
    "        \n",
    "        for negativeId in negativeIds:\n",
    "            self.C[contextId, :] -= self.lr * sigmoid(np.dot(self.W[negativeId,:], self.C[contextId, :])) * self.W[wordId, :]\n",
    "            W_update -= sigmoid(np.dot(self.W[negativeId,:], self.C[contextId, :])) * self.C[contextId, :]\n",
    "        \n",
    "        self.W[wordId, :] -= self.lr * W_update\n",
    "        \n",
    "        \n",
    "        \n",
    "        # get the derivative of the positive part of the likelihood wrt w\n",
    "        \n",
    "        #der_log_sigmoid_wrt_w = self.C[contextId, :]/( 1 + np.exp(np.dot(self.W[wordId,:],self.C[contextId,:])))\n",
    "        #der_neg_sample_wrt_w = sum(-self.C[contextId, :]/(1+ np.exp(np.dot(-self.W[negId,:],self.C[contextId,:]))) for negId in negativeIds)\n",
    "        \n",
    "        \n",
    "        # get the derivative of the positive part of the likelihood wrt y\n",
    "        #der_log_sigmoid_wrt_c = self.W[wordId, :]/( 1 + np.exp(np.dot(self.W[wordId,:],self.C[contextId,:])))\n",
    "        #der_neg_sample_wrt_c = sum(-self.W[wordId,:]/(1+ np.exp(np.dot(-self.W[negId,:],self.C[contextId,:]))) for negId in negativeIds)\n",
    "        #der_neg_sample_wrt_c = 0\n",
    "        \n",
    "        #print(f\"{der_log_sigmoid_wrt_w} {der_neg_sample_wrt_w} {der_log_sigmoid_wrt_c} {der_neg_sample_wrt_c}\")\n",
    "        # Update the x column of the W matrix\n",
    "        #self.W[wordId,:] -= self.lr * (der_log_sigmoid_wrt_w + der_neg_sample_wrt_w )\n",
    "        # Update the y column of the C matrix\n",
    "        #self.C[contextId,:] -= self.lr * (der_log_sigmoid_wrt_c + der_neg_sample_wrt_c )\n",
    "\n",
    "    def save(self,path='mymodel.model'):\n",
    "        \"\"\" Saves the model using np.save function. The matrices are saved sequentially as such:\n",
    "                1. The word_counts is transformed into a nDrive, will send you the links once all is done!\n",
    "umpy array and saved.\n",
    "                2. The W matrix is saved\n",
    "                3. The C matrix is saved\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(path, 'wb') as f:\n",
    "            np.save(f, np.array(list(self.word_counts.items())), allow_pickle=False, fix_imports=True)\n",
    "            np.save(f, self.W, allow_pickle=False, fix_imports=True)\n",
    "            np.save(f, self.C, allow_pickle=False, fix_imports=True)\n",
    "\n",
    "    def similarity(self,word1,word2):\n",
    "        \"\"\"\n",
    "        computes similiarity between the two words. unknown words are mapped to one common vector\n",
    "        :param word1:\n",
    "        :param word2:\n",
    "        :return: a float \\in [0,1] indicating the similarity (the higher the more similar)\n",
    "        \"\"\"\n",
    "        \n",
    "        # If the two words are the same, they are absolutely similar\n",
    "        if word1 == word2:\n",
    "            print(\"same word\")\n",
    "            return 1\n",
    "        \n",
    "        # Checks if word is in dictionary, if not, uses UNK_WORD tag\n",
    "        if word1 in self.w2id:\n",
    "            id1 = self.w2id[word1]\n",
    "        else:\n",
    "            id1 = self.w2id['UNK_WORD']\n",
    "        if word2 in self.w2id:\n",
    "            id2 = self.w2id[word2]\n",
    "        else:\n",
    "            id2 = self.w2id['UNK_WORD']\n",
    "        \n",
    "        # If both words are unknown, default to them being completely different (heuristic)\n",
    "        if id1 == id2:\n",
    "            print(\"Unknown word\")\n",
    "            return 0\n",
    "        \n",
    "        #embedding = self.W[id1, :]\n",
    "        \n",
    "        cos_sim = np.dot(self.W[id1,:], self.W[id2,:])/(np.linalg.norm(self.W[id1,:])*np.linalg.norm(self.W[id2,:]))\n",
    "        \n",
    "        if cos_sim > 1:\n",
    "            cos_sim = 1\n",
    "        elif cos_sim < -1:\n",
    "            cos_sim = -1\n",
    "            \n",
    "        return (cos_sim + 1)/2\n",
    "        #return 1 / (1 + np.exp(np.dot(self.W[id1,:], self.C[id2,:])))\n",
    "   \n",
    "    def loss_function(self, wordId, contextId, negativeIds):\n",
    "        \"\"\" Returns the loss for the given word, its context and the negative samples\"\"\"\n",
    "        \n",
    "        l_sum = sigmoid(np.dot( -self.W[wordId, :], self.C[contextId, :] ))\n",
    "        \n",
    "        for negativeId in negativeIds:\n",
    "            l_sum *= 1 - sigmoid(np.dot( -self.W[negativeId, :], self.C[contextId, :] ))\n",
    "        \n",
    "    \n",
    "        #rint(expit(np.dot( self.W[wordId, :], self.C[contextId, :] )), end='\\r')\n",
    "        return l_sum\n",
    "                           \n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        \"\"\" Loads a new instance of the class from the file created during save()\n",
    "            It generates a blank SkipGram class and fills it with the data in the saved file\n",
    "        \"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            word_counts_array = np.load(f)\n",
    "            W = np.load(f)\n",
    "            C = np.load(f)\n",
    "         \n",
    "        loadedModel = SkipGram([])\n",
    "        \n",
    "        for word, counts in word_counts_array:\n",
    "            loadedModel.word_counts[word] = counts\n",
    "            \n",
    "        # Storing count of unique words\n",
    "        loadedModel.w_count = len(loadedModel.word_counts.keys())\n",
    "        \n",
    "        # Storing a vocabulary of all words\n",
    "        loadedModel.vocab = list(loadedModel.word_counts.keys())\n",
    "\n",
    "        # Creating dictionaries mapping from index to word and vice versa\n",
    "        loadedModel.w2id = dict((word, i) for i, word in enumerate(loadedModel.vocab)) \n",
    "        loadedModel.id2w = dict((i, word) for i, word in enumerate(loadedModel.vocab)) \n",
    "\n",
    "        \n",
    "        loadedModel.W = W\n",
    "        loadedModel.C = C\n",
    "        \n",
    "        return loadedModel\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qx8QnKoZ_Ej8"
   },
   "source": [
    "# Toy dataset -> Use for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpPXHRcD_Ej8",
    "outputId": "e1ab2ca9-4895-4ce0-9db0-908aa612e689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > training 0 of 2\n",
      "0.012428968728271135\n",
      " > training 0 of 2\n",
      "0.02368082072083945\n",
      " > training 0 of 2\n",
      "0.014054614967399482\n",
      " > training 0 of 2\n",
      "0.015722078815172753\n",
      " > training 0 of 2\n",
      "0.018181132783171277\n",
      " > training 0 of 2\n",
      "0.015331858000717818\n",
      " > training 0 of 2\n",
      "0.012617103746402208\n",
      " > training 0 of 2\n",
      "0.019325728758234694\n",
      " > training 0 of 2\n",
      "0.01666157319669799\n",
      " > training 0 of 2\n",
      "0.011539451401135508\n",
      " > training 0 of 2\n",
      "0.01424650138347291\n",
      " > training 0 of 2\n",
      "0.014672389726479742\n",
      " > training 0 of 2\n",
      "0.01478163343042228\n",
      " > training 0 of 2\n",
      "0.008026374881812374\n",
      " > training 0 of 2\n",
      "0.012457815634297755\n",
      " > training 0 of 2\n",
      "0.0264788988029332\n",
      " > training 0 of 2\n",
      "0.012558230623721984\n",
      " > training 0 of 2\n",
      "0.016132821355710994\n",
      " > training 0 of 2\n",
      "0.008234686402199428\n",
      " > training 0 of 2\n",
      "0.013574901107493827\n",
      " > training 0 of 2\n",
      "0.01692873925990161\n",
      " > training 0 of 2\n",
      "0.011611272510014078\n",
      " > training 0 of 2\n",
      "0.008932528502076414\n",
      " > training 0 of 2\n",
      "0.018118878414144717\n",
      " > training 0 of 2\n",
      "0.012435786625187257\n",
      " > training 0 of 2\n",
      "0.021690913727219856\n",
      " > training 0 of 2\n",
      "0.008403534301243742\n",
      " > training 0 of 2\n",
      "0.014270014494578145\n",
      " > training 0 of 2\n",
      "0.01338835505562066\n",
      " > training 0 of 2\n",
      "0.01770764216894981\n",
      " > training 0 of 2\n",
      "0.020452880261949042\n",
      " > training 0 of 2\n",
      "0.01591785889218457\n",
      " > training 0 of 2\n",
      "0.01612193697595048\n",
      " > training 0 of 2\n",
      "0.012599481998519384\n",
      " > training 0 of 2\n",
      "0.028423358077646777\n",
      " > training 0 of 2\n",
      "0.013247734790310594\n",
      " > training 0 of 2\n",
      "0.026004935312536042\n",
      " > training 0 of 2\n",
      "0.02198750566164719\n",
      " > training 0 of 2\n",
      "0.013969282573191969\n",
      " > training 0 of 2\n",
      "0.029676226957359858\n",
      " > training 0 of 2\n",
      "0.012314879370722636\n",
      " > training 0 of 2\n",
      "0.010657535429749463\n",
      " > training 0 of 2\n",
      "0.015794729387392653\n",
      " > training 0 of 2\n",
      "0.01299552539945072\n",
      " > training 0 of 2\n",
      "0.018124807375794305\n",
      " > training 0 of 2\n",
      "0.013747834119398623\n",
      " > training 0 of 2\n",
      "0.008939759152736673\n",
      " > training 0 of 2\n",
      "0.012007872756168178\n",
      " > training 0 of 2\n",
      "0.019617082426982076\n",
      " > training 0 of 2\n",
      "0.012379875681748878\n",
      " > training 0 of 2\n",
      "0.01876549229770778\n",
      " > training 0 of 2\n",
      "0.013624313782945189\n",
      " > training 0 of 2\n",
      "0.014910724051342115\n",
      " > training 0 of 2\n",
      "0.015906414451301847\n",
      " > training 0 of 2\n",
      "0.015116633803561277\n",
      " > training 0 of 2\n",
      "0.017612149494160352\n",
      " > training 0 of 2\n",
      "0.0224549064575257\n",
      " > training 0 of 2\n",
      "0.012934714245275362\n",
      " > training 0 of 2\n",
      "0.01249593670920137\n",
      " > training 0 of 2\n",
      "0.02373637589025728\n",
      " > training 0 of 2\n",
      "0.019389048080252307\n",
      " > training 0 of 2\n",
      "0.015757686468021517\n",
      " > training 0 of 2\n",
      "0.013531735661361942\n",
      " > training 0 of 2\n",
      "0.017765255887571017\n",
      " > training 0 of 2\n",
      "0.01318622293341103\n",
      " > training 0 of 2\n",
      "0.014544248171947875\n",
      " > training 0 of 2\n",
      "0.016131926244725973\n",
      " > training 0 of 2\n",
      "0.016054216959920275\n",
      " > training 0 of 2\n",
      "0.011586269390256365\n",
      " > training 0 of 2\n",
      "0.020578200447150034\n",
      " > training 0 of 2\n",
      "0.013672629163064862\n",
      " > training 0 of 2\n",
      "0.0254274487352498\n",
      " > training 0 of 2\n",
      "0.021458396373139427\n",
      " > training 0 of 2\n",
      "0.01245833965512757\n",
      " > training 0 of 2\n",
      "0.02107653047429935\n",
      " > training 0 of 2\n",
      "0.01742814033684594\n",
      " > training 0 of 2\n",
      "0.020458461958682356\n",
      " > training 0 of 2\n",
      "0.022377392409373913\n",
      " > training 0 of 2\n",
      "0.022128723718648966\n",
      " > training 0 of 2\n",
      "0.019420989618251835\n",
      " > training 0 of 2\n",
      "0.029126228426550372\n",
      " > training 0 of 2\n",
      "0.008343106668984305\n",
      " > training 0 of 2\n",
      "0.030407414394809693\n",
      " > training 0 of 2\n",
      "0.014544590804148797\n",
      " > training 0 of 2\n",
      "0.014676301282277945\n",
      " > training 0 of 2\n",
      "0.01659911932486411\n",
      " > training 0 of 2\n",
      "0.015435887170377804\n",
      " > training 0 of 2\n",
      "0.024233110873026643\n",
      " > training 0 of 2\n",
      "0.017033968306156836\n",
      " > training 0 of 2\n",
      "0.013867968985471022\n",
      " > training 0 of 2\n",
      "0.016244621648767294\n",
      " > training 0 of 2\n",
      "0.01493525010308076\n",
      " > training 0 of 2\n",
      "0.02347692921020058\n",
      " > training 0 of 2\n",
      "0.016647253903725297\n",
      " > training 0 of 2\n",
      "0.027049090837098985\n",
      " > training 0 of 2\n",
      "0.01890643975783716\n",
      " > training 0 of 2\n",
      "0.025798704802224223\n",
      " > training 0 of 2\n",
      "0.016134447292401197\n",
      " > training 0 of 2\n",
      "0.009795391358510423\n",
      " > training 0 of 2\n",
      "0.021571627958022182\n",
      " > training 0 of 2\n",
      "0.014021953633840486\n",
      " > training 0 of 2\n",
      "0.012590776530432501\n",
      " > training 0 of 2\n",
      "0.038170929263280216\n",
      " > training 0 of 2\n",
      "0.02923256811657077\n",
      " > training 0 of 2\n",
      "0.020565497637880258\n",
      " > training 0 of 2\n",
      "0.018505637703063856\n",
      " > training 0 of 2\n",
      "0.013287305726353917\n",
      " > training 0 of 2\n",
      "0.02558359873867035\n",
      " > training 0 of 2\n",
      "0.0149973032034999\n",
      " > training 0 of 2\n",
      "0.01884268026356973\n",
      " > training 0 of 2\n",
      "0.015824864698587512\n",
      " > training 0 of 2\n",
      "0.02631340231102588\n",
      " > training 0 of 2\n",
      "0.02675506541660372\n",
      " > training 0 of 2\n",
      "0.018008671159019374\n",
      " > training 0 of 2\n",
      "0.013782909950180345\n",
      " > training 0 of 2\n",
      "0.01805102104084396\n",
      " > training 0 of 2\n",
      "0.0220309074754523\n",
      " > training 0 of 2\n",
      "0.013412599180289218\n",
      " > training 0 of 2\n",
      "0.019651800662079333\n",
      " > training 0 of 2\n",
      "0.01495308320878035\n",
      " > training 0 of 2\n",
      "0.019314877861151995\n",
      " > training 0 of 2\n",
      "0.013477363644046406\n",
      " > training 0 of 2\n",
      "0.022513877956995564\n",
      " > training 0 of 2\n",
      "0.018802452645498003\n",
      " > training 0 of 2\n",
      "0.01928981518352282\n",
      " > training 0 of 2\n",
      "0.015212041751146215\n",
      " > training 0 of 2\n",
      "0.02288717552691483\n",
      " > training 0 of 2\n",
      "0.00908113057341808\n",
      " > training 0 of 2\n",
      "0.023607746606720253\n",
      " > training 0 of 2\n",
      "0.014776602877784959\n",
      " > training 0 of 2\n",
      "0.022427447730765975\n",
      " > training 0 of 2\n",
      "0.019224449842011673\n",
      " > training 0 of 2\n",
      "0.012557498330006507\n",
      " > training 0 of 2\n",
      "0.02671707910571795\n",
      " > training 0 of 2\n",
      "0.013471893155980103\n",
      " > training 0 of 2\n",
      "0.02296074395791007\n",
      " > training 0 of 2\n",
      "0.02007391459762379\n",
      " > training 0 of 2\n",
      "0.010356656051926823\n",
      " > training 0 of 2\n",
      "0.03820558612341161\n",
      " > training 0 of 2\n",
      "0.015169130685817293\n",
      " > training 0 of 2\n",
      "0.013745022613500897\n",
      " > training 0 of 2\n",
      "0.017822827649991286\n",
      " > training 0 of 2\n",
      "0.017234781713763248\n",
      " > training 0 of 2\n",
      "0.015634500076462537\n",
      " > training 0 of 2\n",
      "0.014176057915682279\n",
      " > training 0 of 2\n",
      "0.011696542613574609\n",
      " > training 0 of 2\n",
      "0.016828294218951604\n",
      " > training 0 of 2\n",
      "0.02748876406796467\n",
      " > training 0 of 2\n",
      "0.014746711958943759\n",
      " > training 0 of 2\n",
      "0.015612994944701782\n",
      " > training 0 of 2\n",
      "0.02878584299743928\n",
      " > training 0 of 2\n",
      "0.019290920830306025\n",
      " > training 0 of 2\n",
      "0.024375948081028276\n",
      " > training 0 of 2\n",
      "0.020663852522308478\n",
      " > training 0 of 2\n",
      "0.023019715547107612\n",
      " > training 0 of 2\n",
      "0.029026002326391742\n",
      " > training 0 of 2\n",
      "0.030712550499404016\n",
      " > training 0 of 2\n",
      "0.01224001340366087\n",
      " > training 0 of 2\n",
      "0.016262934408339244\n",
      " > training 0 of 2\n",
      "0.0353973475575794\n",
      " > training 0 of 2\n",
      "0.026021189695897715\n",
      " > training 0 of 2\n",
      "0.021833821162461003\n",
      " > training 0 of 2\n",
      "0.029521110640098044\n",
      " > training 0 of 2\n",
      "0.013651959047747431\n",
      " > training 0 of 2\n",
      "0.03600184529528821\n",
      " > training 0 of 2\n",
      "0.029640558758864906\n",
      " > training 0 of 2\n",
      "0.015596839905321156\n",
      " > training 0 of 2\n",
      "0.014500661924068521\n",
      " > training 0 of 2\n",
      "0.009816225612132254\n",
      " > training 0 of 2\n",
      "0.017013715521604647\n",
      " > training 0 of 2\n",
      "0.015457474367634097\n",
      " > training 0 of 2\n",
      "0.01021371108629371\n",
      " > training 0 of 2\n",
      "0.01462172482551523\n",
      " > training 0 of 2\n",
      "0.013812235749134729\n",
      " > training 0 of 2\n",
      "0.01594957018949551\n",
      " > training 0 of 2\n",
      "0.015865186737643658\n",
      " > training 0 of 2\n",
      "0.02180875626423048\n",
      " > training 0 of 2\n",
      "0.029068340294391922\n",
      " > training 0 of 2\n",
      "0.019225960767089045\n",
      " > training 0 of 2\n",
      "0.03183813388640017\n",
      " > training 0 of 2\n",
      "0.015323162067958177\n",
      " > training 0 of 2\n",
      "0.01381531328089264\n",
      " > training 0 of 2\n",
      "0.025185610183695948\n",
      " > training 0 of 2\n",
      "0.029128059715588024\n",
      " > training 0 of 2\n",
      "0.019555344955268495\n",
      " > training 0 of 2\n",
      "0.019718351512687214\n",
      " > training 0 of 2\n",
      "0.01925744686379213\n",
      " > training 0 of 2\n",
      "0.02375188542568335\n",
      " > training 0 of 2\n",
      "0.035410064490419724\n",
      " > training 0 of 2\n",
      "0.014915010561080067\n",
      " > training 0 of 2\n",
      "0.020024739802670992\n",
      " > training 0 of 2\n",
      "0.020737184895065278\n",
      " > training 0 of 2\n",
      "0.02050748630890222\n",
      " > training 0 of 2\n",
      "0.015345757909593907\n",
      " > training 0 of 2\n",
      "0.014132338735426424\n",
      " > training 0 of 2\n",
      "0.03505994157843096\n",
      " > training 0 of 2\n",
      "0.012385532655528242\n",
      " > training 0 of 2\n",
      "0.03150181861506204\n",
      " > training 0 of 2\n",
      "0.0190772754420777\n",
      " > training 0 of 2\n",
      "0.026682921259525295\n",
      " > training 0 of 2\n",
      "0.033525179523875515\n",
      " > training 0 of 2\n",
      "0.014946179022964181\n",
      " > training 0 of 2\n",
      "0.01668355840604425\n",
      " > training 0 of 2\n",
      "0.014634755451213283\n",
      " > training 0 of 2\n",
      "0.01196180171802025\n",
      " > training 0 of 2\n",
      "0.031180423995062424\n",
      " > training 0 of 2\n",
      "0.02461019711443432\n",
      " > training 0 of 2\n",
      "0.012544723391710604\n",
      " > training 0 of 2\n",
      "0.01574626768243616\n",
      " > training 0 of 2\n",
      "0.010965792874713897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > training 0 of 2\n",
      "0.01666644741068274\n",
      " > training 0 of 2\n",
      "0.01388969947306677\n",
      " > training 0 of 2\n",
      "0.029988459908281744\n",
      " > training 0 of 2\n",
      "0.012391691873155303\n",
      " > training 0 of 2\n",
      "0.024028122487726067\n",
      " > training 0 of 2\n",
      "0.013279394816960754\n",
      " > training 0 of 2\n",
      "0.014095904359344703\n",
      " > training 0 of 2\n",
      "0.02027580876443785\n",
      " > training 0 of 2\n",
      "0.010800503452327062\n",
      " > training 0 of 2\n",
      "0.02221037739311246\n",
      " > training 0 of 2\n",
      "0.025699677987017968\n",
      " > training 0 of 2\n",
      "0.03813975499395326\n",
      " > training 0 of 2\n",
      "0.011454287021898832\n",
      " > training 0 of 2\n",
      "0.011360936039093595\n",
      " > training 0 of 2\n",
      "0.009421625924124573\n",
      " > training 0 of 2\n",
      "0.01720957977134552\n",
      " > training 0 of 2\n",
      "0.014942548696695155\n",
      " > training 0 of 2\n",
      "0.023105171115555018\n",
      " > training 0 of 2\n",
      "0.016949258649312023\n",
      " > training 0 of 2\n",
      "0.03049188332612725\n",
      " > training 0 of 2\n",
      "0.006254731719124184\n",
      " > training 0 of 2\n",
      "0.021805373408184173\n",
      " > training 0 of 2\n",
      "0.017066465841643303\n",
      " > training 0 of 2\n",
      "0.01819460017347548\n",
      " > training 0 of 2\n",
      "0.013418454253675214\n",
      " > training 0 of 2\n",
      "0.01651642103722642\n",
      " > training 0 of 2\n",
      "0.006934449424587502\n",
      " > training 0 of 2\n",
      "0.005818324289205302\n",
      " > training 0 of 2\n",
      "0.016518346557631132\n",
      " > training 0 of 2\n",
      "0.014774119477163693\n",
      " > training 0 of 2\n",
      "0.020545837363566653\n",
      " > training 0 of 2\n",
      "0.01441530502724549\n",
      " > training 0 of 2\n",
      "0.011914209329267837\n",
      " > training 0 of 2\n",
      "0.019048195438957267\n",
      " > training 0 of 2\n",
      "0.026183222029866223\n",
      " > training 0 of 2\n",
      "0.017579540351377133\n",
      " > training 0 of 2\n",
      "0.008752869754274632\n",
      " > training 0 of 2\n",
      "0.011967911762943812\n",
      " > training 0 of 2\n",
      "0.0071106034074739895\n",
      " > training 0 of 2\n",
      "0.010719818199365844\n",
      " > training 0 of 2\n",
      "0.015117177408649362\n",
      " > training 0 of 2\n",
      "0.016351162864560734\n",
      " > training 0 of 2\n",
      "0.012152323824005282\n",
      " > training 0 of 2\n",
      "0.03392593502129168\n",
      " > training 0 of 2\n",
      "0.01476111235801753\n",
      " > training 0 of 2\n",
      "0.022470410294027014\n",
      " > training 0 of 2\n",
      "0.006370138427255788\n",
      " > training 0 of 2\n",
      "0.010127509850500776\n",
      " > training 0 of 2\n",
      "0.005019588028323827\n",
      " > training 0 of 2\n",
      "0.02150411988675386\n",
      " > training 0 of 2\n",
      "0.012858296724429742\n",
      " > training 0 of 2\n",
      "0.011701004205151055\n",
      " > training 0 of 2\n",
      "0.012088742637909034\n",
      " > training 0 of 2\n",
      "0.010843319383997864\n",
      " > training 0 of 2\n",
      "0.01507762608668486\n",
      " > training 0 of 2\n",
      "0.013485852552599898\n",
      " > training 0 of 2\n",
      "0.014291165881506494\n",
      " > training 0 of 2\n",
      "0.0201229413671712\n",
      " > training 0 of 2\n",
      "0.010308523069993731\n",
      " > training 0 of 2\n",
      "0.02743604125818155\n",
      " > training 0 of 2\n",
      "0.008287864475235055\n",
      " > training 0 of 2\n",
      "0.013854912055169677\n",
      " > training 0 of 2\n",
      "0.009773226318084154\n",
      " > training 0 of 2\n",
      "0.00996269752271613\n",
      " > training 0 of 2\n",
      "0.007636184798550555\n",
      " > training 0 of 2\n",
      "0.024817192474813572\n",
      " > training 0 of 2\n",
      "0.010735217679170635\n",
      " > training 0 of 2\n",
      "0.012881463304760555\n",
      " > training 0 of 2\n",
      "0.020807391316609344\n",
      " > training 0 of 2\n",
      "0.02198180726798916\n",
      " > training 0 of 2\n",
      "0.019207203306571333\n",
      " > training 0 of 2\n",
      "0.011378587749984383\n",
      " > training 0 of 2\n",
      "0.005773752900163737\n",
      " > training 0 of 2\n",
      "0.024042494408006196\n",
      " > training 0 of 2\n",
      "0.01497527372820272\n",
      " > training 0 of 2\n",
      "0.024054143764080587\n",
      " > training 0 of 2\n",
      "0.013472760336828628\n",
      " > training 0 of 2\n",
      "0.012733292861261448\n",
      " > training 0 of 2\n",
      "0.012093620868846085\n",
      " > training 0 of 2\n",
      "0.01977694950976028\n",
      " > training 0 of 2\n",
      "0.017966302340878013\n",
      " > training 0 of 2\n",
      "0.012589775839058105\n",
      " > training 0 of 2\n",
      "0.01786441522869728\n",
      " > training 0 of 2\n",
      "0.01889656478334626\n",
      " > training 0 of 2\n",
      "0.006891259857304662\n",
      " > training 0 of 2\n",
      "0.014423149753057822\n",
      " > training 0 of 2\n",
      "0.02304964462701727\n",
      " > training 0 of 2\n",
      "0.01561809681839833\n",
      " > training 0 of 2\n",
      "0.007736154441566545\n",
      " > training 0 of 2\n",
      "0.012137555459794842\n",
      " > training 0 of 2\n",
      "0.011309377978507437\n",
      " > training 0 of 2\n",
      "0.016825101641566396\n",
      " > training 0 of 2\n",
      "0.02103908647452208\n",
      " > training 0 of 2\n",
      "0.020883372785722273\n",
      " > training 0 of 2\n",
      "0.004474901463596637\n",
      " > training 0 of 2\n",
      "0.012278474742763725\n",
      " > training 0 of 2\n",
      "0.009792614267383945\n",
      " > training 0 of 2\n",
      "0.021439528987762763\n",
      " > training 0 of 2\n",
      "0.018311936843223353\n",
      " > training 0 of 2\n",
      "0.01007656823184746\n",
      " > training 0 of 2\n",
      "0.010191605385476971\n",
      " > training 0 of 2\n",
      "0.018947061706001543\n",
      " > training 0 of 2\n",
      "0.008229178181680582\n",
      " > training 0 of 2\n",
      "0.011564132012907357\n",
      " > training 0 of 2\n",
      "0.00979124893528765\n",
      " > training 0 of 2\n",
      "0.007092767881476095\n",
      " > training 0 of 2\n",
      "0.010084999898697126\n",
      " > training 0 of 2\n",
      "0.010889984899268143\n",
      " > training 0 of 2\n",
      "0.009595392406547804\n",
      " > training 0 of 2\n",
      "0.007996551898549024\n",
      " > training 0 of 2\n",
      "0.008267086485622781\n",
      " > training 0 of 2\n",
      "0.011613059491648306\n",
      " > training 0 of 2\n",
      "0.021423410759485013\n",
      " > training 0 of 2\n",
      "0.007398267196096077\n",
      " > training 0 of 2\n",
      "0.013897118100943509\n",
      " > training 0 of 2\n",
      "0.012412661367092294\n",
      " > training 0 of 2\n",
      "0.01079074346902619\n",
      " > training 0 of 2\n",
      "0.01177038921657344\n",
      " > training 0 of 2\n",
      "0.019751653564352864\n",
      " > training 0 of 2\n",
      "0.008420806859242387\n",
      " > training 0 of 2\n",
      "0.020503585765798422\n",
      " > training 0 of 2\n",
      "0.010533446815957923\n",
      " > training 0 of 2\n",
      "0.010250585166067621\n",
      " > training 0 of 2\n",
      "0.007740631334961895\n",
      " > training 0 of 2\n",
      "0.018965446856853228\n",
      " > training 0 of 2\n",
      "0.006305827968342287\n",
      " > training 0 of 2\n",
      "0.012499411757920633\n",
      " > training 0 of 2\n",
      "0.013717272819503634\n",
      " > training 0 of 2\n",
      "0.010883893317147099\n",
      " > training 0 of 2\n",
      "0.023940468514679347\n",
      " > training 0 of 2\n",
      "0.015766598712184707\n",
      " > training 0 of 2\n",
      "0.019181837085463616\n",
      " > training 0 of 2\n",
      "0.025754108078810792\n",
      " > training 0 of 2\n",
      "0.007417763274665303\n",
      " > training 0 of 2\n",
      "0.02307956927921973\n",
      " > training 0 of 2\n",
      "0.010490827758862677\n",
      " > training 0 of 2\n",
      "0.021049772424264604\n",
      " > training 0 of 2\n",
      "0.0184044775405953\n",
      " > training 0 of 2\n",
      "0.01188379973323768\n",
      " > training 0 of 2\n",
      "0.007497574804293647\n",
      " > training 0 of 2\n",
      "0.006903077400836707\n",
      " > training 0 of 2\n",
      "0.014282167188061353\n",
      " > training 0 of 2\n",
      "0.00336517906137159\n",
      " > training 0 of 2\n",
      "0.006431286541367949\n",
      " > training 0 of 2\n",
      "0.00501220458891182\n",
      " > training 0 of 2\n",
      "0.007723593543207845\n",
      " > training 0 of 2\n",
      "0.012302901300924407\n",
      " > training 0 of 2\n",
      "0.016109201990532663\n",
      " > training 0 of 2\n",
      "0.022195074238544076\n",
      " > training 0 of 2\n",
      "0.01159518602785132\n",
      " > training 0 of 2\n",
      "0.012333820854180201\n",
      " > training 0 of 2\n",
      "0.008331612211236272\n",
      " > training 0 of 2\n",
      "0.014539634664635953\n",
      " > training 0 of 2\n",
      "0.006242354035670774\n",
      " > training 0 of 2\n",
      "0.006789476314098145\n",
      " > training 0 of 2\n",
      "0.00527896176273521\n",
      " > training 0 of 2\n",
      "0.006484547575400906\n",
      " > training 0 of 2\n",
      "0.015370220491628157\n",
      " > training 0 of 2\n",
      "0.005598812998606879\n",
      " > training 0 of 2\n",
      "0.01069565841782812\n",
      " > training 0 of 2\n",
      "0.004116629520230566\n",
      " > training 0 of 2\n",
      "0.009174781550143337\n",
      " > training 0 of 2\n",
      "0.01770412863151543\n",
      " > training 0 of 2\n",
      "0.012447230054906666\n",
      " > training 0 of 2\n",
      "0.008900501640949804\n",
      " > training 0 of 2\n",
      "0.01640077246567556\n",
      " > training 0 of 2\n",
      "0.02295661601752766\n",
      " > training 0 of 2\n",
      "0.011433745993572515\n",
      " > training 0 of 2\n",
      "0.01591841385211406\n",
      " > training 0 of 2\n",
      "0.006459223429141009\n",
      " > training 0 of 2\n",
      "0.01570575068803931\n",
      " > training 0 of 2\n",
      "0.007038774665782249\n",
      " > training 0 of 2\n",
      "0.022859153680762256\n",
      " > training 0 of 2\n",
      "0.010283606267441686\n",
      " > training 0 of 2\n",
      "0.015226519006542314\n",
      " > training 0 of 2\n",
      "0.009857207842210747\n",
      " > training 0 of 2\n",
      "0.02874718037405694\n",
      " > training 0 of 2\n",
      "0.02537245652569786\n",
      " > training 0 of 2\n",
      "0.006386866942005551\n",
      " > training 0 of 2\n",
      "0.010790205636675647\n",
      " > training 0 of 2\n",
      "0.007696229021591606\n",
      " > training 0 of 2\n",
      "0.009676042959635034\n",
      " > training 0 of 2\n",
      "0.008970915093625263\n",
      " > training 0 of 2\n",
      "0.011869557974768734\n",
      " > training 0 of 2\n",
      "0.008674356962202026\n",
      " > training 0 of 2\n",
      "0.006515300090443118\n",
      " > training 0 of 2\n",
      "0.006147499125324474\n",
      " > training 0 of 2\n",
      "0.008362919904671152\n",
      " > training 0 of 2\n",
      "0.012594990838597995\n",
      " > training 0 of 2\n",
      "0.016892970814724385\n",
      " > training 0 of 2\n",
      "0.006754594394077507\n",
      " > training 0 of 2\n",
      "0.014105096045711205\n",
      " > training 0 of 2\n",
      "0.014580216080420684\n",
      " > training 0 of 2\n",
      "0.014249568369852972\n",
      " > training 0 of 2\n",
      "0.016212430785058785\n",
      " > training 0 of 2\n",
      "0.01957580552631259\n",
      " > training 0 of 2\n",
      "0.008266601794233512\n",
      " > training 0 of 2\n",
      "0.004140613165099838\n",
      " > training 0 of 2\n",
      "0.01162806990403752\n",
      " > training 0 of 2\n",
      "0.019745860657178347\n",
      " > training 0 of 2\n",
      "0.005559709303063619\n",
      " > training 0 of 2\n",
      "0.015214622317322228\n",
      " > training 0 of 2\n",
      "0.01278348253243124\n",
      " > training 0 of 2\n",
      "0.014674981286456475\n",
      " > training 0 of 2\n",
      "0.006481048791566842\n",
      " > training 0 of 2\n",
      "0.01038811507317695\n",
      " > training 0 of 2\n",
      "0.012752879419564322\n",
      " > training 0 of 2\n",
      "0.015814750499646656\n",
      " > training 0 of 2\n",
      "0.007473927887356116\n",
      " > training 0 of 2\n",
      "0.01083507345146675\n",
      " > training 0 of 2\n",
      "0.013546500611224541\n",
      " > training 0 of 2\n",
      "0.009998369398166387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > training 0 of 2\n",
      "0.013523480436188503\n",
      " > training 0 of 2\n",
      "0.022694244194433\n",
      " > training 0 of 2\n",
      "0.011038548880994295\n",
      " > training 0 of 2\n",
      "0.025693885301841756\n",
      " > training 0 of 2\n",
      "0.03840860665309028\n",
      " > training 0 of 2\n",
      "0.020638868198865585\n",
      " > training 0 of 2\n",
      "0.015681085883447406\n",
      " > training 0 of 2\n",
      "0.012314972725418977\n",
      " > training 0 of 2\n",
      "0.02607415557248528\n",
      " > training 0 of 2\n",
      "0.01587429342008579\n",
      " > training 0 of 2\n",
      "0.016523004101048625\n",
      " > training 0 of 2\n",
      "0.010474947408165167\n",
      " > training 0 of 2\n",
      "0.02071470352082525\n",
      " > training 0 of 2\n",
      "0.022802853349301138\n",
      " > training 0 of 2\n",
      "0.012438733865822641\n",
      " > training 0 of 2\n",
      "0.020614475346739117\n",
      " > training 0 of 2\n",
      "0.015523155424632945\n",
      " > training 0 of 2\n",
      "0.010278944438816625\n",
      " > training 0 of 2\n",
      "0.016107514060451585\n",
      " > training 0 of 2\n",
      "0.00953114476934423\n",
      " > training 0 of 2\n",
      "0.02704836181326447\n",
      " > training 0 of 2\n",
      "0.015335031824538617\n",
      " > training 0 of 2\n",
      "0.02989240739002579\n",
      " > training 0 of 2\n",
      "0.01938557591432345\n",
      " > training 0 of 2\n",
      "0.01303252332353675\n",
      " > training 0 of 2\n",
      "0.030484334012080158\n",
      " > training 0 of 2\n",
      "0.012037456167338296\n",
      " > training 0 of 2\n",
      "0.016937845582844214\n",
      " > training 0 of 2\n",
      "0.010230745724298723\n",
      " > training 0 of 2\n",
      "0.01421682928671962\n",
      " > training 0 of 2\n",
      "0.01076282795307269\n",
      " > training 0 of 2\n",
      "0.013972664453106706\n",
      " > training 0 of 2\n",
      "0.018939469742043986\n",
      " > training 0 of 2\n",
      "0.03181147123477254\n",
      " > training 0 of 2\n",
      "0.022387763407891323\n",
      " > training 0 of 2\n",
      "0.026327456386763582\n",
      " > training 0 of 2\n",
      "0.025538324227785836\n",
      " > training 0 of 2\n",
      "0.019311515827752132\n",
      " > training 0 of 2\n",
      "0.014031982050760698\n",
      " > training 0 of 2\n",
      "0.014077483398733652\n",
      " > training 0 of 2\n",
      "0.008809267223661477\n",
      " > training 0 of 2\n",
      "0.013199806337703936\n",
      " > training 0 of 2\n",
      "0.02646864368436367\n",
      " > training 0 of 2\n",
      "0.01652265775868697\n",
      " > training 0 of 2\n",
      "0.02179292610999848\n",
      " > training 0 of 2\n",
      "0.01571011710036195\n",
      " > training 0 of 2\n",
      "0.008719860652854737\n",
      " > training 0 of 2\n",
      "0.03122649331317147\n",
      " > training 0 of 2\n",
      "0.04107412395299498\n",
      " > training 0 of 2\n",
      "0.025245474591132532\n",
      " > training 0 of 2\n",
      "0.0133949084317701\n",
      " > training 0 of 2\n",
      "0.025527885478512785\n",
      " > training 0 of 2\n",
      "0.04951975232905846\n",
      " > training 0 of 2\n",
      "0.03074176024477743\n",
      " > training 0 of 2\n",
      "0.02142879209569433\n",
      " > training 0 of 2\n",
      "0.01287068861547\n",
      " > training 0 of 2\n",
      "0.032962904229028506\n",
      " > training 0 of 2\n",
      "0.02263312720233208\n",
      " > training 0 of 2\n",
      "0.017649462651547872\n",
      " > training 0 of 2\n",
      "0.0182382301887476\n",
      " > training 0 of 2\n",
      "0.019891714471616112\n",
      " > training 0 of 2\n",
      "0.01679877919750735\n",
      " > training 0 of 2\n",
      "0.015422240113282212\n",
      " > training 0 of 2\n",
      "0.020204866209243\n",
      " > training 0 of 2\n",
      "0.014644713896762755\n",
      " > training 0 of 2\n",
      "0.016949247048869954\n",
      " > training 0 of 2\n",
      "0.02377189073907007\n",
      " > training 0 of 2\n",
      "0.022438201794503117\n",
      " > training 0 of 2\n",
      "0.017226584679280944\n",
      " > training 0 of 2\n",
      "0.018441122471059057\n",
      " > training 0 of 2\n",
      "0.020462104193348516\n",
      " > training 0 of 2\n",
      "0.016045868113225906\n",
      " > training 0 of 2\n",
      "0.02213633883889695\n",
      " > training 0 of 2\n",
      "0.014697474636859338\n",
      " > training 0 of 2\n",
      "0.031191883370731824\n",
      " > training 0 of 2\n",
      "0.030704946183149055\n",
      " > training 0 of 2\n",
      "0.025903057490436213\n",
      " > training 0 of 2\n",
      "0.010880280911375622\n",
      " > training 0 of 2\n",
      "0.023652158799742157\n",
      " > training 0 of 2\n",
      "0.03006639669617272\n",
      " > training 0 of 2\n",
      "0.018741321810512895\n",
      " > training 0 of 2\n",
      "0.015368535777104907\n",
      " > training 0 of 2\n",
      "0.025458370149656632\n",
      " > training 0 of 2\n",
      "0.01881005604918137\n",
      " > training 0 of 2\n",
      "0.03669594473926249\n",
      " > training 0 of 2\n",
      "0.026911881263564284\n",
      " > training 0 of 2\n",
      "0.03349901595088906\n",
      " > training 0 of 2\n",
      "0.021058311747097675\n",
      " > training 0 of 2\n",
      "0.01819176600556492\n",
      " > training 0 of 2\n",
      "0.015539495935173314\n",
      " > training 0 of 2\n",
      "0.017577388869187784\n",
      " > training 0 of 2\n",
      "0.031336926875282244\n",
      " > training 0 of 2\n",
      "0.02082224768264131\n",
      " > training 0 of 2\n",
      "0.022201113048580065\n",
      " > training 0 of 2\n",
      "0.031821764158953755\n",
      " > training 0 of 2\n",
      "0.023326699835695915\n",
      " > training 0 of 2\n",
      "0.0165853638721629\n",
      " > training 0 of 2\n",
      "0.01959323480286188\n",
      " > training 0 of 2\n",
      "0.020778894965507503\n",
      " > training 0 of 2\n",
      "0.016861899256262525\n",
      " > training 0 of 2\n",
      "0.029375366326607907\n",
      " > training 0 of 2\n",
      "0.019880484318807123\n",
      " > training 0 of 2\n",
      "0.026921138179168663\n",
      " > training 0 of 2\n",
      "0.02877191072312365\n",
      " > training 0 of 2\n",
      "0.04298469005139254\n",
      " > training 0 of 2\n",
      "0.019042058779366468\n",
      " > training 0 of 2\n",
      "0.02964465437741361\n",
      " > training 0 of 2\n",
      "0.03051734912847362\n",
      " > training 0 of 2\n",
      "0.033263939754452854\n",
      " > training 0 of 2\n",
      "0.019483860469951098\n",
      " > training 0 of 2\n",
      "0.03666941208580717\n",
      " > training 0 of 2\n",
      "0.027255932867905976\n",
      " > training 0 of 2\n",
      "0.03478667532108391\n",
      " > training 0 of 2\n",
      "0.02340204470745738\n",
      " > training 0 of 2\n",
      "0.02173083125504299\n",
      " > training 0 of 2\n",
      "0.028325913537589623\n",
      " > training 0 of 2\n",
      "0.013185621679184826\n",
      " > training 0 of 2\n",
      "0.01417092978254212\n",
      " > training 0 of 2\n",
      "0.019891232627212426\n",
      " > training 0 of 2\n",
      "0.03266708284328408\n",
      " > training 0 of 2\n",
      "0.021735036305746983\n",
      " > training 0 of 2\n",
      "0.01419003670017619\n",
      " > training 0 of 2\n",
      "0.02041048230797804\n",
      " > training 0 of 2\n",
      "0.025100663355179523\n",
      " > training 0 of 2\n",
      "0.033304819865434244\n",
      " > training 0 of 2\n",
      "0.01778099569120434\n",
      " > training 0 of 2\n",
      "0.03482519261759281\n",
      " > training 0 of 2\n",
      "0.02639249876606078\n",
      " > training 0 of 2\n",
      "0.021019770423017465\n",
      " > training 0 of 2\n",
      "0.030125020870409663\n",
      " > training 0 of 2\n",
      "0.02196961736109371\n",
      " > training 0 of 2\n",
      "0.015474009665985592\n",
      " > training 0 of 2\n",
      "0.013603943984616175\n",
      " > training 0 of 2\n",
      "0.027425109873284028\n",
      " > training 0 of 2\n",
      "0.02433572171779421\n",
      " > training 0 of 2\n",
      "0.009292990758772154\n",
      " > training 0 of 2\n",
      "0.019890234401822044\n",
      " > training 0 of 2\n",
      "0.01621882749835261\n",
      " > training 0 of 2\n",
      "0.024800359917130452\n",
      " > training 0 of 2\n",
      "0.017773455528916213\n",
      " > training 0 of 2\n",
      "0.0167488931542547\n",
      " > training 0 of 2\n",
      "0.027781576429708455\n",
      " > training 0 of 2\n",
      "0.018797133587938806\n",
      " > training 0 of 2\n",
      "0.023523569705480134\n",
      " > training 0 of 2\n",
      "0.021238518233586884\n",
      " > training 0 of 2\n",
      "0.013088725895344521\n",
      " > training 0 of 2\n",
      "0.017717468246099963\n",
      " > training 0 of 2\n",
      "0.013079723488302084\n",
      " > training 0 of 2\n",
      "0.013057176974033514\n",
      " > training 0 of 2\n",
      "0.012417732381144915\n",
      " > training 0 of 2\n",
      "0.013338320272918846\n",
      " > training 0 of 2\n",
      "0.011330380203065883\n",
      " > training 0 of 2\n",
      "0.01176328771248331\n",
      " > training 0 of 2\n",
      "0.01455547469002\n",
      " > training 0 of 2\n",
      "0.031617074957266456\n",
      " > training 0 of 2\n",
      "0.010545238775228918\n",
      " > training 0 of 2\n",
      "0.012853315885232752\n",
      " > training 0 of 2\n",
      "0.016912370787143107\n",
      " > training 0 of 2\n",
      "0.010840549626763687\n",
      " > training 0 of 2\n",
      "0.02968424131394651\n",
      " > training 0 of 2\n",
      "0.014275814477034882\n",
      " > training 0 of 2\n",
      "0.01656481723046874\n",
      " > training 0 of 2\n",
      "0.01662490445504849\n",
      " > training 0 of 2\n",
      "0.008303345305780321\n",
      " > training 0 of 2\n",
      "0.01048322707574253\n",
      " > training 0 of 2\n",
      "0.025549953403740677\n",
      " > training 0 of 2\n",
      "0.03268723825456042\n",
      " > training 0 of 2\n",
      "0.008636325262115246\n",
      " > training 0 of 2\n",
      "0.024828483573348433\n",
      " > training 0 of 2\n",
      "0.007647737714171576\n",
      " > training 0 of 2\n",
      "0.026151245777429225\n",
      " > training 0 of 2\n",
      "0.01753767177015734\n",
      " > training 0 of 2\n",
      "0.021134197950773998\n",
      " > training 0 of 2\n",
      "0.021745092437576337\n",
      " > training 0 of 2\n",
      "0.02220813193929022\n",
      " > training 0 of 2\n",
      "0.019204810559360123\n",
      " > training 0 of 2\n",
      "0.01987202400543184\n",
      " > training 0 of 2\n",
      "0.01672771170017391\n",
      " > training 0 of 2\n",
      "0.022988154896401845\n",
      " > training 0 of 2\n",
      "0.01135550825358942\n",
      " > training 0 of 2\n",
      "0.010179112451783436\n",
      " > training 0 of 2\n",
      "0.011150098421433689\n",
      " > training 0 of 2\n",
      "0.009795339686768814\n",
      " > training 0 of 2\n",
      "0.023802601243548254\n",
      " > training 0 of 2\n",
      "0.005688367794499428\n",
      " > training 0 of 2\n",
      "0.014618922026067236\n",
      " > training 0 of 2\n",
      "0.009891957864094983\n",
      " > training 0 of 2\n",
      "0.017958095150540282\n",
      " > training 0 of 2\n",
      "0.011241477607134214\n",
      " > training 0 of 2\n",
      "0.006017176438110312\n",
      " > training 0 of 2\n",
      "0.01749812769593456\n",
      " > training 0 of 2\n",
      "0.011151313191648912\n",
      " > training 0 of 2\n",
      "0.020177507951348237\n",
      " > training 0 of 2\n",
      "0.010586756682351255\n",
      " > training 0 of 2\n",
      "0.015160001015529213\n",
      " > training 0 of 2\n",
      "0.00496608628642182\n",
      " > training 0 of 2\n",
      "0.01083186022048545\n",
      " > training 0 of 2\n",
      "0.01895748517684155\n",
      " > training 0 of 2\n",
      "0.012851380416341863\n",
      " > training 0 of 2\n",
      "0.019257164882571204\n",
      " > training 0 of 2\n",
      "0.013910620308396831\n",
      " > training 0 of 2\n",
      "0.029438941601144922\n",
      " > training 0 of 2\n",
      "0.01859425174235311\n",
      " > training 0 of 2\n",
      "0.015110323854758936\n",
      " > training 0 of 2\n",
      "0.031488853885868695\n",
      " > training 0 of 2\n",
      "0.026628594571657636\n",
      " > training 0 of 2\n",
      "0.005709920377550689\n",
      " > training 0 of 2\n",
      "0.020353786522918863\n",
      " > training 0 of 2\n",
      "0.021091473702988393\n",
      " > training 0 of 2\n",
      "0.01574668317065913\n",
      " > training 0 of 2\n",
      "0.007931438569645904\n",
      " > training 0 of 2\n",
      "0.01274060832633696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > training 0 of 2\n",
      "0.013573265497406926\n",
      " > training 0 of 2\n",
      "0.013245583403371392\n",
      " > training 0 of 2\n",
      "0.02316254057172192\n",
      " > training 0 of 2\n",
      "0.022976072565323816\n",
      " > training 0 of 2\n",
      "0.020963928978502695\n",
      " > training 0 of 2\n",
      "0.020671244934100014\n",
      " > training 0 of 2\n",
      "0.019307204154993794\n",
      " > training 0 of 2\n",
      "0.016477353927460612\n",
      " > training 0 of 2\n",
      "0.004639726995868524\n",
      " > training 0 of 2\n",
      "0.027674595722402627\n",
      " > training 0 of 2\n",
      "0.009633706305229577\n",
      " > training 0 of 2\n",
      "0.023269589681656086\n",
      " > training 0 of 2\n",
      "0.013565908116697221\n",
      " > training 0 of 2\n",
      "0.008110886974246119\n",
      " > training 0 of 2\n",
      "0.015619633924000537\n",
      " > training 0 of 2\n",
      "0.00999393899872953\n",
      " > training 0 of 2\n",
      "0.010376734086040593\n",
      " > training 0 of 2\n",
      "0.024179495302889212\n",
      " > training 0 of 2\n",
      "0.00910428147392882\n",
      " > training 0 of 2\n",
      "0.03410352227884202\n",
      " > training 0 of 2\n",
      "0.015628451672953337\n",
      " > training 0 of 2\n",
      "0.01925965648359057\n",
      " > training 0 of 2\n",
      "0.015679137378664443\n",
      " > training 0 of 2\n",
      "0.009064192950244757\n",
      " > training 0 of 2\n",
      "0.010280502057001959\n",
      " > training 0 of 2\n",
      "0.025886774137610848\n",
      " > training 0 of 2\n",
      "0.0197249210841777\n",
      " > training 0 of 2\n",
      "0.008099370862282488\n",
      " > training 0 of 2\n",
      "0.012621192854190761\n",
      " > training 0 of 2\n",
      "0.012441269500055462\n",
      " > training 0 of 2\n",
      "0.024667044559756265\n",
      " > training 0 of 2\n",
      "0.00398329852161484\n",
      " > training 0 of 2\n",
      "0.017174702402740227\n",
      " > training 0 of 2\n",
      "0.024342464998172855\n",
      " > training 0 of 2\n",
      "0.012225433633691383\n",
      " > training 0 of 2\n",
      "0.012971923171112992\n",
      " > training 0 of 2\n",
      "0.008266353577367375\n",
      " > training 0 of 2\n",
      "0.011926698110447895\n",
      " > training 0 of 2\n",
      "0.006036783074594848\n",
      " > training 0 of 2\n",
      "0.006836034856690114\n",
      " > training 0 of 2\n",
      "0.010770593250812352\n",
      " > training 0 of 2\n",
      "0.008524377413393968\n",
      " > training 0 of 2\n",
      "0.013483092261813837\n",
      " > training 0 of 2\n",
      "0.012437216066306272\n",
      " > training 0 of 2\n",
      "0.009640408775038144\n",
      " > training 0 of 2\n",
      "0.0061872619861211365\n",
      " > training 0 of 2\n",
      "0.011324119264737873\n",
      " > training 0 of 2\n",
      "0.021503455415069293\n",
      " > training 0 of 2\n",
      "0.012209593873831477\n",
      " > training 0 of 2\n",
      "0.0066274470625172\n",
      " > training 0 of 2\n",
      "0.008935819928869418\n",
      " > training 0 of 2\n",
      "0.010962578533608465\n",
      " > training 0 of 2\n",
      "0.016283635631067554\n",
      " > training 0 of 2\n",
      "0.009599619415120577\n",
      " > training 0 of 2\n",
      "0.012487458612602724\n",
      " > training 0 of 2\n",
      "0.00994352315010682\n",
      " > training 0 of 2\n",
      "0.015122992842103824\n",
      " > training 0 of 2\n",
      "0.01875790030470597\n",
      " > training 0 of 2\n",
      "0.010911906108885969\n",
      " > training 0 of 2\n",
      "0.007838273343194624\n",
      " > training 0 of 2\n",
      "0.02095179382772136\n",
      " > training 0 of 2\n",
      "0.011953499901534641\n",
      " > training 0 of 2\n",
      "0.008905973902305298\n",
      " > training 0 of 2\n",
      "0.010991245150075378\n",
      " > training 0 of 2\n",
      "0.009021151341836864\n",
      " > training 0 of 2\n",
      "0.0026166756361661477\n",
      " > training 0 of 2\n",
      "0.006525505785166904\n",
      " > training 0 of 2\n",
      "0.008998302266094878\n",
      " > training 0 of 2\n",
      "0.010917381812327624\n",
      " > training 0 of 2\n",
      "0.008513308008608295\n",
      " > training 0 of 2\n",
      "0.014311806583266822\n",
      " > training 0 of 2\n",
      "0.015020481387982383\n",
      " > training 0 of 2\n",
      "0.01351366657013218\n",
      " > training 0 of 2\n",
      "0.0053339387710034936\n",
      " > training 0 of 2\n",
      "0.017848377158760375\n",
      " > training 0 of 2\n",
      "0.014837390008238456\n",
      " > training 0 of 2\n",
      "0.010254450403232251\n",
      " > training 0 of 2\n",
      "0.01083176580116095\n",
      " > training 0 of 2\n",
      "0.006380367123531721\n",
      " > training 0 of 2\n",
      "0.016163412913301593\n",
      " > training 0 of 2\n",
      "0.008876435238034398\n",
      " > training 0 of 2\n",
      "0.009263094136230474\n",
      " > training 0 of 2\n",
      "0.00969146656902832\n",
      " > training 0 of 2\n",
      "0.01279655662642166\n",
      " > training 0 of 2\n",
      "0.02020861070464379\n",
      " > training 0 of 2\n",
      "0.009859562085343753\n",
      " > training 0 of 2\n",
      "0.007488817335741575\n",
      " > training 0 of 2\n",
      "0.019636828788813468\n",
      " > training 0 of 2\n",
      "0.011438017407510848\n",
      " > training 0 of 2\n",
      "0.015214622512475297\n",
      " > training 0 of 2\n",
      "0.009485996878996565\n",
      " > training 0 of 2\n",
      "0.0055858925522905434\n",
      " > training 0 of 2\n",
      "0.013360604733184714\n",
      " > training 0 of 2\n",
      "0.004121177658687898\n",
      " > training 0 of 2\n",
      "0.01047761041100102\n",
      " > training 0 of 2\n",
      "0.007433353761515515\n",
      " > training 0 of 2\n",
      "0.009658711323287852\n",
      " > training 0 of 2\n",
      "0.014480425783297378\n",
      " > training 0 of 2\n",
      "0.007123857546817408\n",
      " > training 0 of 2\n",
      "0.008287853563706997\n",
      " > training 0 of 2\n",
      "0.009391920403862046\n",
      " > training 0 of 2\n",
      "0.00657594471938491\n",
      " > training 0 of 2\n",
      "0.010213921865699794\n",
      " > training 0 of 2\n",
      "0.007941332104336505\n",
      " > training 0 of 2\n",
      "0.02390989139193005\n",
      " > training 0 of 2\n",
      "0.004236298463974108\n",
      " > training 0 of 2\n",
      "0.011746242109120284\n",
      " > training 0 of 2\n",
      "0.01577192595521237\n",
      " > training 0 of 2\n",
      "0.00989408326539518\n",
      " > training 0 of 2\n",
      "0.019221257831291555\n",
      " > training 0 of 2\n",
      "0.020104690828533518\n",
      " > training 0 of 2\n",
      "0.0032497946805002884\n",
      " > training 0 of 2\n",
      "0.00498024573203461\n",
      " > training 0 of 2\n",
      "0.013568792113076909\n",
      " > training 0 of 2\n",
      "0.013328271050363117\n",
      " > training 0 of 2\n",
      "0.01834022978794721\n",
      " > training 0 of 2\n",
      "0.010509676955565796\n",
      " > training 0 of 2\n",
      "0.003231888319507679\n",
      " > training 0 of 2\n",
      "0.004045855910869957\n",
      " > training 0 of 2\n",
      "0.0106693358503956\n",
      " > training 0 of 2\n",
      "0.008633854795625099\n",
      " > training 0 of 2\n",
      "0.013007541200359405\n",
      " > training 0 of 2\n",
      "0.007076061450398372\n",
      " > training 0 of 2\n",
      "0.004335531463647838\n",
      " > training 0 of 2\n",
      "0.009689229876852055\n",
      " > training 0 of 2\n",
      "0.009624589895965615\n",
      " > training 0 of 2\n",
      "0.013772936629697917\n",
      " > training 0 of 2\n",
      "0.009282522118284656\n",
      " > training 0 of 2\n",
      "0.025519215163982905\n",
      " > training 0 of 2\n",
      "0.015020318014326501\n",
      " > training 0 of 2\n",
      "0.013118208628331584\n",
      " > training 0 of 2\n",
      "0.02033474848696813\n",
      " > training 0 of 2\n",
      "0.007477660404092653\n",
      " > training 0 of 2\n",
      "0.01864356098483509\n",
      " > training 0 of 2\n",
      "0.013450612411141464\n",
      " > training 0 of 2\n",
      "0.012929871703328114\n",
      " > training 0 of 2\n",
      "0.007004945279805897\n",
      " > training 0 of 2\n",
      "0.002611763202041045\n",
      " > training 0 of 2\n",
      "0.006004009640844862\n",
      " > training 0 of 2\n",
      "0.0028197306595315564\n",
      " > training 0 of 2\n",
      "0.004755702679258138\n",
      " > training 0 of 2\n",
      "0.010435774033439305\n",
      " > training 0 of 2\n",
      "0.0070197002379474216\n",
      " > training 0 of 2\n",
      "0.00767662576541446\n",
      " > training 0 of 2\n",
      "0.010044719539754611\n",
      " > training 0 of 2\n",
      "0.004100384121320932\n",
      " > training 0 of 2\n",
      "0.009016497709741073\n",
      " > training 0 of 2\n",
      "0.006186960459509921\n",
      " > training 0 of 2\n",
      "0.01758969683709682\n",
      " > training 0 of 2\n",
      "0.00896541251558632\n",
      " > training 0 of 2\n",
      "0.017701946168194645\n",
      " > training 0 of 2\n",
      "0.006497802727546092\n",
      " > training 0 of 2\n",
      "0.010075586836764527\n",
      " > training 0 of 2\n",
      "0.008574066733342243\n",
      " > training 0 of 2\n",
      "0.005867664829212237\n",
      " > training 0 of 2\n",
      "0.020098772641727485\n",
      " > training 0 of 2\n",
      "0.0037332092267438003\n",
      " > training 0 of 2\n",
      "0.014045201174887383\n",
      " > training 0 of 2\n",
      "0.005216546115657743\n",
      " > training 0 of 2\n",
      "0.01383720913361435\n",
      " > training 0 of 2\n",
      "0.009498168912329883\n",
      " > training 0 of 2\n",
      "0.009945797808610561\n",
      " > training 0 of 2\n",
      "0.013646036301012571\n",
      " > training 0 of 2\n",
      "0.0061845293209099135\n",
      " > training 0 of 2\n",
      "0.004024845897633488\n",
      " > training 0 of 2\n",
      "0.016778120620591305\n",
      " > training 0 of 2\n",
      "0.01187981215440702\n",
      " > training 0 of 2\n",
      "0.01139328642030699\n",
      " > training 0 of 2\n",
      "0.005962660779423691\n",
      " > training 0 of 2\n",
      "0.0139812288642262\n",
      " > training 0 of 2\n",
      "0.020542710494838513\n",
      " > training 0 of 2\n",
      "0.03199699322596346\n",
      " > training 0 of 2\n",
      "0.019441197709201972\n",
      " > training 0 of 2\n",
      "0.009275883806093663\n",
      " > training 0 of 2\n",
      "0.015750846848453882\n",
      " > training 0 of 2\n",
      "0.011180141219083279\n",
      " > training 0 of 2\n",
      "0.009053076955029318\n",
      " > training 0 of 2\n",
      "0.009908649105429077\n",
      " > training 0 of 2\n",
      "0.006976807515889385\n",
      " > training 0 of 2\n",
      "0.006583731397761616\n",
      " > training 0 of 2\n",
      "0.026086591885782474\n",
      " > training 0 of 2\n",
      "0.002981550218661492\n",
      " > training 0 of 2\n",
      "0.00545057117643989\n",
      " > training 0 of 2\n",
      "0.008916047428374853\n",
      " > training 0 of 2\n",
      "0.007132428799843008\n",
      " > training 0 of 2\n",
      "0.0157836045246375\n",
      " > training 0 of 2\n",
      "0.007946534949213215\n",
      " > training 0 of 2\n",
      "0.010013367271152577\n",
      " > training 0 of 2\n",
      "0.005823765445695471\n",
      " > training 0 of 2\n",
      "0.023622867833373928\n",
      " > training 0 of 2\n",
      "0.007680699266902149\n",
      " > training 0 of 2\n",
      "0.008955415072590664\n",
      " > training 0 of 2\n",
      "0.01039747948625661\n",
      " > training 0 of 2\n",
      "0.011681827960186292\n",
      " > training 0 of 2\n",
      "0.018829163231230166\n",
      " > training 0 of 2\n",
      "0.010940076279711405\n",
      " > training 0 of 2\n",
      "0.012831349630191636\n",
      " > training 0 of 2\n",
      "0.01107372786583728\n",
      " > training 0 of 2\n",
      "0.01264023614501036\n",
      " > training 0 of 2\n",
      "0.012945443831547885\n",
      " > training 0 of 2\n",
      "0.040636249436220954\n",
      " > training 0 of 2\n",
      "0.026683875259167662\n",
      " > training 0 of 2\n",
      "0.007887307988268602\n",
      " > training 0 of 2\n",
      "0.026657078493755506\n",
      " > training 0 of 2\n",
      "0.012259404008398133\n",
      " > training 0 of 2\n",
      "0.02042318238655237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > training 0 of 2\n",
      "0.02914258070295552\n",
      " > training 0 of 2\n",
      "0.015955410619296607\n",
      " > training 0 of 2\n",
      "0.020758070183801592\n",
      " > training 0 of 2\n",
      "0.006850386498497252\n",
      " > training 0 of 2\n",
      "0.023682481429861044\n",
      " > training 0 of 2\n",
      "0.02046924487747813\n",
      " > training 0 of 2\n",
      "0.009640906255995438\n",
      " > training 0 of 2\n",
      "0.026690219952778024\n",
      " > training 0 of 2\n",
      "0.013316782229917723\n",
      " > training 0 of 2\n",
      "0.0181885333992873\n",
      " > training 0 of 2\n",
      "0.018100420715715937\n",
      " > training 0 of 2\n",
      "0.014791329385932781\n",
      " > training 0 of 2\n",
      "0.034391530206997326\n",
      " > training 0 of 2\n",
      "0.012991542652157262\n",
      " > training 0 of 2\n",
      "0.018112480277227685\n",
      " > training 0 of 2\n",
      "0.013376171447802112\n",
      " > training 0 of 2\n",
      "0.027417259408508962\n",
      " > training 0 of 2\n",
      "0.013526999500137032\n",
      " > training 0 of 2\n",
      "0.020541434580878305\n",
      " > training 0 of 2\n",
      "0.02377920846110653\n",
      " > training 0 of 2\n",
      "0.024671252603952913\n",
      " > training 0 of 2\n",
      "0.019151190782962957\n",
      " > training 0 of 2\n",
      "0.022004332428120738\n",
      " > training 0 of 2\n",
      "0.014964727544780204\n",
      " > training 0 of 2\n",
      "0.02709281261459799\n",
      " > training 0 of 2\n",
      "0.025678328241726542\n",
      " > training 0 of 2\n",
      "0.030066208751862906\n",
      " > training 0 of 2\n",
      "0.014984659945516116\n",
      " > training 0 of 2\n",
      "0.012261155299327036\n",
      " > training 0 of 2\n",
      "0.009497868736376765\n",
      " > training 0 of 2\n",
      "0.035762816330338065\n",
      " > training 0 of 2\n",
      "0.0398115291652393\n",
      " > training 0 of 2\n",
      "0.020995867354697054\n",
      " > training 0 of 2\n",
      "0.01817124322531366\n",
      " > training 0 of 2\n",
      "0.02828977731455759\n",
      " > training 0 of 2\n",
      "0.013142214682485748\n",
      " > training 0 of 2\n",
      "0.02777887602365261\n",
      " > training 0 of 2\n",
      "0.03727094710620879\n",
      " > training 0 of 2\n",
      "0.032790955828305685\n",
      " > training 0 of 2\n",
      "0.023606790318043004\n",
      " > training 0 of 2\n",
      "0.026996906282481834\n",
      " > training 0 of 2\n",
      "0.04278830843769058\n",
      " > training 0 of 2\n",
      "0.046914610784981096\n",
      " > training 0 of 2\n",
      "0.021574781567391236\n",
      " > training 0 of 2\n",
      "0.07245982482314271\n",
      " > training 0 of 2\n",
      "0.009397278431207735\n",
      " > training 0 of 2\n",
      "0.0412431136419285\n",
      " > training 0 of 2\n",
      "0.04271403000777763\n",
      " > training 0 of 2\n",
      "0.04201665920677308\n",
      " > training 0 of 2\n",
      "0.055680830442257793\n",
      " > training 0 of 2\n",
      "0.04922965254620459\n",
      " > training 0 of 2\n",
      "0.05770056611737695\n",
      " > training 0 of 2\n",
      "0.06801714323565089\n",
      " > training 0 of 2\n",
      "0.03237926248883184\n",
      " > training 0 of 2\n",
      "0.02688551013227269\n",
      " > training 0 of 2\n",
      "0.021938691417299712\n",
      " > training 0 of 2\n",
      "0.022338778790281514\n",
      " > training 0 of 2\n",
      "0.021989717744069184\n",
      " > training 0 of 2\n",
      "0.03029459234169869\n",
      " > training 0 of 2\n",
      "0.0224960253238682\n",
      " > training 0 of 2\n",
      "0.0463910613451654\n",
      " > training 0 of 2\n",
      "0.0309311091698601\n",
      " > training 0 of 2\n",
      "0.03444805111866509\n",
      " > training 0 of 2\n",
      "0.034653149940845264\n",
      " > training 0 of 2\n",
      "0.01867875853389392\n",
      " > training 0 of 2\n",
      "0.029877650962372788\n",
      " > training 0 of 2\n",
      "0.0137099829041781\n",
      " > training 0 of 2\n",
      "0.05914989304787518\n",
      " > training 0 of 2\n",
      "0.02971203657585784\n",
      " > training 0 of 2\n",
      "0.030193165979321344\n",
      " > training 0 of 2\n",
      "0.03916942962089352\n",
      " > training 0 of 2\n",
      "0.0581305571231663\n",
      " > training 0 of 2\n",
      "0.06021993747737407\n",
      " > training 0 of 2\n",
      "0.03618994233812932\n",
      " > training 0 of 2\n",
      "0.03054362641653753\n",
      " > training 0 of 2\n",
      "0.018559132235364494\n",
      " > training 0 of 2\n",
      "0.041963704134474096\n",
      " > training 0 of 2\n",
      "0.05371972772042006\n",
      " > training 0 of 2\n",
      "0.042321791037796014\n",
      " > training 0 of 2\n",
      "0.011870438965022097\n",
      " > training 0 of 2\n",
      "0.059787794290326136\n",
      " > training 0 of 2\n",
      "0.008281896394617376\n",
      " > training 0 of 2\n",
      "0.044553782917571\n",
      " > training 0 of 2\n",
      "0.024716938653992125\n",
      " > training 0 of 2\n",
      "0.03423450676321667\n",
      " > training 0 of 2\n",
      "0.03240893633991056\n",
      " > training 0 of 2\n",
      "0.019124434873442633\n",
      " > training 0 of 2\n",
      "0.006683564381364994\n",
      " > training 0 of 2\n",
      "0.029053541689503325\n",
      " > training 0 of 2\n",
      "0.029754284502826467\n",
      " > training 0 of 2\n",
      "0.033081637378226655\n",
      " > training 0 of 2\n",
      "0.01853990656393251\n",
      " > training 0 of 2\n",
      "0.03681154161089115\n",
      " > training 0 of 2\n",
      "0.022637772280815923\n",
      " > training 0 of 2\n",
      "0.031310194542187385\n",
      " > training 0 of 2\n",
      "0.03225547314419543\n",
      " > training 0 of 2\n",
      "0.022169918808910404\n",
      " > training 0 of 2\n",
      "0.02096950883466639\n",
      " > training 0 of 2\n",
      "0.02538235405188823\n",
      " > training 0 of 2\n",
      "0.016751263555849064\n",
      " > training 0 of 2\n",
      "0.017318789911167382\n",
      " > training 0 of 2\n",
      "0.018561566698875504\n",
      " > training 0 of 2\n",
      "0.025295127499872325\n",
      " > training 0 of 2\n",
      "0.035757656533195126\n",
      " > training 0 of 2\n",
      "0.018723196303331524\n",
      " > training 0 of 2\n",
      "0.026992205063886315\n",
      " > training 0 of 2\n",
      "0.02291240766926873\n",
      " > training 0 of 2\n",
      "0.020915142510057647\n",
      " > training 0 of 2\n",
      "0.021660844854090658\n",
      " > training 0 of 2\n",
      "0.01987151730217453\n",
      " > training 0 of 2\n",
      "0.03152860625559057\n",
      " > training 0 of 2\n",
      "0.018741758054019692\n",
      " > training 0 of 2\n",
      "0.02422247053453225\n",
      " > training 0 of 2\n",
      "0.015953926020054705\n",
      " > training 0 of 2\n",
      "0.02299703190308566\n",
      " > training 0 of 2\n",
      "0.034665627275353786\n",
      " > training 0 of 2\n",
      "0.025663411195047257\n",
      " > training 0 of 2\n",
      "0.016678599939630258\n",
      " > training 0 of 2\n",
      "0.009450440949517984\n",
      " > training 0 of 2\n",
      "0.022053817770169147\n",
      " > training 0 of 2\n",
      "0.023380446168832735\n",
      " > training 0 of 2\n",
      "0.04979346022064911\n",
      " > training 0 of 2\n",
      "0.0281143287781665\n",
      " > training 0 of 2\n",
      "0.02701184246878174\n",
      " > training 0 of 2\n",
      "0.023726964176584046\n",
      " > training 0 of 2\n",
      "0.04769982931835008\n",
      " > training 0 of 2\n",
      "0.020489638965924394\n",
      " > training 0 of 2\n",
      "0.025128907730318673\n",
      " > training 0 of 2\n",
      "0.015575964001472159\n",
      " > training 0 of 2\n",
      "0.015306138795780674\n",
      " > training 0 of 2\n",
      "0.014214965207686624\n",
      " > training 0 of 2\n",
      "0.020949213797567954\n",
      " > training 0 of 2\n",
      "0.026353218180904102\n",
      " > training 0 of 2\n",
      "0.01963854921553084\n",
      " > training 0 of 2\n",
      "0.01524165498283979\n",
      " > training 0 of 2\n",
      "0.025527113464643866\n",
      " > training 0 of 2\n",
      "0.031328255996890327\n",
      " > training 0 of 2\n",
      "0.01382851559445353\n",
      " > training 0 of 2\n",
      "0.015228702145028028\n",
      " > training 0 of 2\n",
      "0.018320162749782334\n",
      " > training 0 of 2\n",
      "0.011577253627571494\n",
      " > training 0 of 2\n",
      "0.014051073867124434\n",
      " > training 0 of 2\n",
      "0.0291464230797683\n",
      " > training 0 of 2\n",
      "0.056146268698755684\n",
      " > training 0 of 2\n",
      "0.010857322165164617\n",
      " > training 0 of 2\n",
      "0.011899046249581989\n",
      " > training 0 of 2\n",
      "0.006406169225812661\n",
      " > training 0 of 2\n",
      "0.015057851189660075\n",
      " > training 0 of 2\n",
      "0.008903555551585129\n",
      " > training 0 of 2\n",
      "0.011925730293873543\n",
      " > training 0 of 2\n",
      "0.013200258790160808\n",
      " > training 0 of 2\n",
      "0.006757827495592127\n",
      " > training 0 of 2\n",
      "0.018367619675874653\n",
      " > training 0 of 2\n",
      "0.012253623963813038\n",
      " > training 0 of 2\n",
      "0.014490769541727464\n",
      " > training 0 of 2\n",
      "0.007936841403753008\n",
      " > training 0 of 2\n",
      "0.027038457331300655\n",
      " > training 0 of 2\n",
      "0.006528926559696573\n",
      " > training 0 of 2\n",
      "0.006655084100388646\n",
      " > training 0 of 2\n",
      "0.0070081068627625654\n",
      "[[-0.8675186   0.78908504  1.55199722]\n",
      " [ 1.25436852 -0.81910304  0.39977113]\n",
      " [ 2.05915447  0.60834375  1.83141533]\n",
      " [ 1.72067958  0.83833991  0.53728593]\n",
      " [-2.91888941 -1.1364294   1.54615946]\n",
      " [ 1.7763673   0.168012    2.47750388]\n",
      " [ 1.24304385  1.94702476  1.70288605]\n",
      " [ 0.58456268 -2.85755189  3.23487754]\n",
      " [-3.57082287 -2.82807907  3.30340959]\n",
      " [-0.43967802 -3.23233996  3.24851084]\n",
      " [-0.74058802 -3.39333353  5.13603396]\n",
      " [ 1.49410238 -0.89531538  0.50893434]\n",
      " [-0.7348892  -2.88534176  4.35127417]]\n",
      "[[ 1.05043701  1.27260185 -1.22986998]\n",
      " [ 0.58873296  0.86228444 -0.05261479]\n",
      " [ 0.74123715 -1.06707832 -0.40282107]\n",
      " [-2.19401943 -0.46269848  0.77951953]\n",
      " [-0.79158414 -0.18023637  0.12629402]\n",
      " [-0.61002644 -0.92088423 -1.17174308]\n",
      " [ 1.94678699  0.72870738 -0.61867706]\n",
      " [-3.29638775  1.10770338 -1.63990959]\n",
      " [ 0.99214006  0.25423485 -1.11790606]\n",
      " [ 1.41953837 -0.0141291  -1.39083871]\n",
      " [-0.9172013  -0.95006255  0.14262795]\n",
      " [-0.6478106   1.18521507 -1.28527282]\n",
      " [-0.07992052 -0.66193953 -0.20497716]]\n",
      "0.8218599472382859\n",
      "0.9138781749958422\n",
      "0.7377864453590874\n",
      "0.7377864453590874\n",
      "Unknown words test\n",
      "0.6399022835557813\n",
      "Unknown word\n",
      "0\n",
      "Unknown word\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "toy_sentences = [[\"an\", \"amazing\", \"apple\", \"article\", \"articulated\", \"android\"], [\"the\", \"tired\", \"tiger\", \"tisturbs\", \"the\", \"timid\", \"tablet\"]]\n",
    "\n",
    "model = SkipGram(toy_sentences, nEmbed=3)\n",
    "for i in range(1000):\n",
    "    model.train()\n",
    "print(model.W)\n",
    "\n",
    "print(model.C)\n",
    "\n",
    "print(model.similarity(\"an\", \"amazing\"))\n",
    "print(model.similarity(\"tired\", \"tiger\"))\n",
    "\n",
    "print(model.similarity(\"tiger\", \"an\"))\n",
    "print(model.similarity(\"an\", \"tiger\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"Unknown words test\")\n",
    "print(model.similarity(\"parser\", \"amazing\"))\n",
    "print(model.similarity(\"parser\", \"yellow\"))\n",
    "print(model.similarity(\"amzing\", \"parser\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IL1H7cNP_Ej9"
   },
   "source": [
    "# True dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPGHkBzF_Ej9"
   },
   "outputs": [],
   "source": [
    "model = SkipGram(sentences[:5000], winSize = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gW-QPdSJ_Ej-",
    "outputId": "43cc1e62-3562-48c7-8590-bba07231c411"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > training 0 of 5000\n",
      "0.01369405094325184\n",
      " > training 300 of 5000\n",
      "0.014901512004146661\n",
      " > training 600 of 5000\n",
      "0.014894713826914147\n",
      " > training 900 of 5000\n",
      "0.01541942161624279\n",
      " > training 1200 of 5000\n",
      "0.015646874446167355\n",
      " > training 1500 of 5000\n",
      "0.015595199979043746\n",
      " > training 1800 of 5000\n",
      "0.01640101690936631\n",
      " > training 2100 of 5000\n",
      "0.014693374679705119\n",
      " > training 2400 of 5000\n",
      "0.016000673894749788\n",
      " > training 2700 of 5000\n",
      "0.015341494031501063\n",
      " > training 3000 of 5000\n",
      "0.017194094233014882\n",
      " > training 3300 of 5000\n",
      "0.017537598674117517\n",
      " > training 3600 of 5000\n",
      "0.016197745876362423\n",
      " > training 3900 of 5000\n",
      "0.016393879973187326\n",
      " > training 4200 of 5000\n",
      "0.016336065982718258\n",
      " > training 4500 of 5000\n",
      "0.014540762480162292\n",
      " > training 4800 of 5000\n",
      "0.016210261183109077\n"
     ]
    }
   ],
   "source": [
    "train_loss_ = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d8aDvcN_Ej-",
    "outputId": "d205849d-0f36-4db0-9b37-175b87d85012"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train_loss_ = model.train()\n",
    "import matplotlib.pyplot as plt\n",
    "# Save performance of the loss\n",
    "log = pd.DataFrame()\n",
    "log['train_loss'] = np.array(train_loss_)\n",
    "log.to_csv('log.csv')\n",
    "\n",
    "'''Visualize the performance'''\n",
    "log = pd.read_csv('log.csv') # Load the performance log\n",
    "plt.figure(figsize=(12,6),facecolor='c')\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Train Loss')\n",
    "plt.xlabel('sentences')\n",
    "plt.plot(log.train_loss, color='coral', linestyle='-.', label='Train Loss')\n",
    "plt.legend()\n",
    "plt.grid(color='steelblue', linestyle='--', linewidth=1.0,alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSewCTWL_Ej_"
   },
   "outputs": [],
   "source": [
    "model.save('50k_sentences_XinRong_grad_winSize_3.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dO6FHBj_Ej_"
   },
   "source": [
    "# Testing on SimLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-me51ii6bl2"
   },
   "outputs": [],
   "source": [
    "def loadPairs(path):\n",
    "        data = pd.read_csv(path, delimiter='\\t')\n",
    "        # Inserted SimLex999 column\n",
    "        data = pd.DataFrame (data, columns = ['word1','word2','SimLex999'])\n",
    "        pairs = zip(data['word1'],data['word2'],data['SimLex999'])\n",
    "        return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6XK6dD56bvs"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--text', help='path containing training data', required=True)\n",
    "    parser.add_argument('--model', help='path to store/read model (when training/testing)', required=True)\n",
    "    parser.add_argument('--test', help='enters test mode', action='store_true')\n",
    "\n",
    "    opts = parser.parse_args()\n",
    "\n",
    "    if not opts.test:\n",
    "        sentences = text2sentences(opts.text)\n",
    "        sg = SkipGram(sentences)\n",
    "        sg.train()\n",
    "        sg.save(opts.model)\n",
    "\n",
    "    else:\n",
    "        pairs = loadPairs(opts.text)\n",
    "\n",
    "        sg = SkipGram.load(opts.model)\n",
    "        for a,b,_ in pairs:\n",
    "            # make sure this does not raise any exception, even if a or b are not in sg.vocab\n",
    "            print(sg.similarity(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shobhqnG_EkA"
   },
   "outputs": [],
   "source": [
    "model = SkipGram.load('50k_sentences_XinRong_grad_winSize_3.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8nZjVW__EkB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pairs = loadPairs(\"SimLex-999/SimLex-999.txt\")\n",
    "l2_loss = []\n",
    "l1_loss = []\n",
    "for w1,w2, simi in pairs:\n",
    "    # make sure this does not raise any exception, even if a or b are not in sg.vocab\n",
    "    \n",
    "    # Adjusting cos similarity to [0, 1]\n",
    "    model_simi = (model.similarity(w1,w2) + 1)/2\n",
    "    truth_simi = simi/10\n",
    "    l2_loss.append( (truth_simi - model_simi)**2 )\n",
    "    l1_loss.append( np.abs(truth_simi - model_simi) )\n",
    "    #l.append(np.abs(np.round((simi/10)) - model.similarity(w1,w2)))\n",
    "    print(f\"{truth_simi} + {model_simi} + {(truth_simi - model_simi)**2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHUFon8v_EkB"
   },
   "outputs": [],
   "source": [
    "sum(l2_loss)/len(l2_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5-_D4x9_EkC"
   },
   "outputs": [],
   "source": [
    "sum(l1_loss) / len(l1_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBqWEjYL_EkC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_SkipGram_15_02_v4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
