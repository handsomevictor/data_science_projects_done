{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLNS - Kaggle Competition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "9Erw2eT0nkgu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uvfozhe4ZB9G"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn import preprocessing\n",
        "import nltk\n",
        "import csv\n",
        "import re\n",
        "from pprint import pprint\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaMulticore\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('punkt') # for tokenization\n",
        "nltk.download('stopwords')\n",
        "stpwds = list(nltk.corpus.stopwords.words(\"english\"))\n",
        "stpwds.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "stpwds = set(stpwds)\n",
        "stemmer = nltk.stem.PorterStemmer()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVstov4aa5Nn",
        "outputId": "f3c8dd2e-52cb-4376-bc4c-6063e5a1100e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install igraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI7f8yelaacT",
        "outputId": "24b885dd-8e4e-493a-d25a-dc088ebe8a5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: igraph in /usr/local/lib/python3.7/dist-packages (0.9.9)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.7/dist-packages (from igraph) (1.6.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import igraph"
      ],
      "metadata": {
        "id": "mNLNDqYqacRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Drive connection"
      ],
      "metadata": {
        "id": "r0pIauXPZsHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "from google.colab import drive\n",
        "\n",
        "## connect your drive with the notebook\n",
        "drive = drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrAVb6swZtrf",
        "outputId": "95c60ebc-7b1e-4044-ee30-7e72db70c903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project_folder = \"drive/My Drive/MLNS\""
      ],
      "metadata": {
        "id": "Yk-fAalTZyit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading"
      ],
      "metadata": {
        "id": "nRyOpEUKZ1Dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open(project_folder + \"/training_set.txt\", \"r\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    training_set  = list(reader)\n",
        "\n",
        "training_set = [element[0].split(\" \") for element in training_set]\n"
      ],
      "metadata": {
        "id": "lD97c1cfbLTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open(project_folder + \"/testing_set.txt\", \"r\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    testing_set  = list(reader)\n",
        "\n",
        "testing_set = [element[0].split(\" \") for element in testing_set]\n"
      ],
      "metadata": {
        "id": "TPShaSgEZ0am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_set[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPUfo7JPbB-D",
        "outputId": "0cb10841-ad91-4425-d108-5960acf8fe06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['9807076', '9807139'],\n",
              " ['109162', '1182'],\n",
              " ['9702187', '9510135'],\n",
              " ['111048', '110115'],\n",
              " ['9910176', '9410073']]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open(project_folder + \"/node_information.csv\", \"r\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    node_info  = list(reader)\n",
        "\n",
        "IDs = [element[0] for element in node_info]\n"
      ],
      "metadata": {
        "id": "znpDK2UPbFWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_info[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t56FHg-cbcoI",
        "outputId": "519e458c-9580-496a-d848-f1dbbb7de32b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1001',\n",
              " '2000',\n",
              " 'compactification geometry and duality',\n",
              " 'Paul S. Aspinwall',\n",
              " '',\n",
              " 'these are notes based on lectures given at tasi99 we review the geometry of the moduli space of n 2 theories in four dimensions from the point of view of superstring compactification the cases of a type iia or type iib string compactified on a calabi-yau threefold and the heterotic string compactified on k3xt2 are each considered in detail we pay specific attention to the differences between n 2 theories and n 2 theories the moduli spaces of vector multiplets and the moduli spaces of hypermultiplets are reviewed in the case of hypermultiplets this review is limited by the poor state of our current understanding some peculiarities such as mixed instantons and the non-existence of a universal hypermultiplet are discussed']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_set_percentage = 0.2\n",
        "# randomly select a percentage of training set\n",
        "to_keep = random.sample(range(len(training_set)), k=int(round(len(training_set)*training_set_percentage)))\n",
        "training_set_reduced = [training_set[i] for i in to_keep]\n"
      ],
      "metadata": {
        "id": "NtfvqgFUSgWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data visualization"
      ],
      "metadata": {
        "id": "65oK3RYFUerb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "node_df = pd.DataFrame(\n",
        "    node_info, \n",
        "    columns=[\"id\", \"publication_year\", \"title\", \n",
        "             \"authors\", \"journal_name\", \"abstract\"]\n",
        ")\n",
        "\n",
        "node_df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "y1_XfADXUgIY",
        "outputId": "784698a6-8cb7-4378-e802-4a15010bfbf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id publication_year                                              title  \\\n",
              "0  1001             2000              compactification geometry and duality   \n",
              "1  1002             2000  domain walls and massive gauged supergravity p...   \n",
              "2  1003             2000     comment on metric fluctuations in brane worlds   \n",
              "3  1004             2000         moving mirrors and thermodynamic paradoxes   \n",
              "4  1005             2000  bundles of chiral blocks and boundary conditio...   \n",
              "\n",
              "                       authors       journal_name  \\\n",
              "0            Paul S. Aspinwall                      \n",
              "1  M. Cvetic, H. Lu, C.N. Pope  Class.Quant.Grav.   \n",
              "2     Y.S. Myung, Gungwon Kang                      \n",
              "3               Adam D. Helfer          Phys.Rev.   \n",
              "4      J. Fuchs, C. Schweigert                      \n",
              "\n",
              "                                            abstract  \n",
              "0  these are notes based on lectures given at tas...  \n",
              "1  we point out that massive gauged supergravity ...  \n",
              "2  recently ivanov and volovich hep-th 9912242 cl...  \n",
              "3  quantum fields responding to moving mirrors ha...  \n",
              "4  proceedings of lie iii clausthal july 1999 var...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-958fe543-5f9a-4de9-b6c8-13755be481b0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>publication_year</th>\n",
              "      <th>title</th>\n",
              "      <th>authors</th>\n",
              "      <th>journal_name</th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1001</td>\n",
              "      <td>2000</td>\n",
              "      <td>compactification geometry and duality</td>\n",
              "      <td>Paul S. Aspinwall</td>\n",
              "      <td></td>\n",
              "      <td>these are notes based on lectures given at tas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1002</td>\n",
              "      <td>2000</td>\n",
              "      <td>domain walls and massive gauged supergravity p...</td>\n",
              "      <td>M. Cvetic, H. Lu, C.N. Pope</td>\n",
              "      <td>Class.Quant.Grav.</td>\n",
              "      <td>we point out that massive gauged supergravity ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1003</td>\n",
              "      <td>2000</td>\n",
              "      <td>comment on metric fluctuations in brane worlds</td>\n",
              "      <td>Y.S. Myung, Gungwon Kang</td>\n",
              "      <td></td>\n",
              "      <td>recently ivanov and volovich hep-th 9912242 cl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1004</td>\n",
              "      <td>2000</td>\n",
              "      <td>moving mirrors and thermodynamic paradoxes</td>\n",
              "      <td>Adam D. Helfer</td>\n",
              "      <td>Phys.Rev.</td>\n",
              "      <td>quantum fields responding to moving mirrors ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1005</td>\n",
              "      <td>2000</td>\n",
              "      <td>bundles of chiral blocks and boundary conditio...</td>\n",
              "      <td>J. Fuchs, C. Schweigert</td>\n",
              "      <td></td>\n",
              "      <td>proceedings of lie iii clausthal july 1999 var...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-958fe543-5f9a-4de9-b6c8-13755be481b0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-958fe543-5f9a-4de9-b6c8-13755be481b0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-958fe543-5f9a-4de9-b6c8-13755be481b0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "ONbNeVyQbrUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph generation"
      ],
      "metadata": {
        "id": "Kux34CGifael"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "edges = [(element[0],element[1]) for element in training_set if element[2]==\"1\"]\n",
        "\n",
        "G = nx.Graph()\n",
        "\n",
        "nodes = IDs\n",
        "\n",
        "G.add_nodes_from(nodes)\n",
        "\n",
        "G.add_edges_from(edges)\n"
      ],
      "metadata": {
        "id": "pYd0Bbicfcre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Degree Centrality measure\n",
        "deg_centrality = nx.degree_centrality(G)\n",
        "\n",
        "# Betweeness centrality measure\n",
        "# betweeness_centrality = nx.betweenness_centrality(G)\n",
        "betweeness_centrality = None\n"
      ],
      "metadata": {
        "id": "oLY-YHz0s-u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Topic modeling"
      ],
      "metadata": {
        "id": "3XWw-3Nsjug0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) \n",
        "             if word not in stpwds] for doc in texts]\n"
      ],
      "metadata": {
        "id": "Tr7OU2c3lNIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def topic_modeling(text_df, num_topics=5):\n",
        "    # Remove punctuation\n",
        "    preprocessed_texts = text_df.map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "    # Convert the titles to lowercase\n",
        "    preprocessed_texts = preprocessed_texts.map(lambda x: x.lower())\n",
        "    \n",
        "    # Convert to words\n",
        "    texts_data = preprocessed_texts.to_list()\n",
        "    texts_words = list(sent_to_words(texts_data))\n",
        "    # Remove stop words\n",
        "    texts_words = remove_stopwords(texts_words)\n",
        "    \n",
        "    # Create Dictionary\n",
        "    id2word = Dictionary(texts_words)\n",
        "    # Create Corpus\n",
        "    texts = texts_words\n",
        "    # Term Document Frequency\n",
        "    corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "    # Build LDA model\n",
        "    lda_model = LdaMulticore(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
        "\n",
        "    pprint(lda_model.print_topics())\n",
        "\n",
        "    lda_topics = lda_model[corpus]\n",
        "\n",
        "    return lda_topics\n"
      ],
      "metadata": {
        "id": "BDvs_wv-pd_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "title_lda = topic_modeling(node_df[\"title\"], num_topics=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaqkRSh6nEOc",
        "outputId": "169284c2-2b42-4bab-aaef-7417c31d1b28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.060*\"theory\" + 0.021*\"yang\" + 0.021*\"mills\" + 0.017*\"gauge\" + 0.017*\"ads\" '\n",
            "  '+ 0.017*\"string\" + 0.017*\"branes\" + 0.012*\"theories\" + 0.011*\"duality\" + '\n",
            "  '0.011*\"supersymmetric\"'),\n",
            " (1,\n",
            "  '0.032*\"dimensional\" + 0.023*\"black\" + 0.017*\"model\" + 0.017*\"two\" + '\n",
            "  '0.015*\"quantum\" + 0.015*\"string\" + 0.013*\"holes\" + 0.010*\"theory\" + '\n",
            "  '0.010*\"non\" + 0.009*\"hole\"'),\n",
            " (2,\n",
            "  '0.021*\"field\" + 0.019*\"brane\" + 0.016*\"models\" + 0.015*\"non\" + '\n",
            "  '0.014*\"quantum\" + 0.014*\"theory\" + 0.012*\"supersymmetric\" + 0.011*\"gravity\" '\n",
            "  '+ 0.009*\"symmetry\" + 0.007*\"gauge\"'),\n",
            " (3,\n",
            "  '0.030*\"theory\" + 0.023*\"gauge\" + 0.023*\"field\" + 0.022*\"theories\" + '\n",
            "  '0.020*\"quantum\" + 0.016*\"string\" + 0.011*\"action\" + 0.011*\"space\" + '\n",
            "  '0.011*\"effective\" + 0.010*\"model\"'),\n",
            " (4,\n",
            "  '0.021*\"string\" + 0.021*\"field\" + 0.017*\"theory\" + 0.014*\"branes\" + '\n",
            "  '0.013*\"theories\" + 0.012*\"conformal\" + 0.012*\"model\" + 0.011*\"quantum\" + '\n",
            "  '0.010*\"gravity\" + 0.010*\"gauge\"')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "abstract_lda = topic_modeling(node_df[\"abstract\"], num_topics=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQpgelo5q9hK",
        "outputId": "7355420c-ef1a-4bad-858b-13ce68ddbd6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.023*\"theory\" + 0.014*\"dimensional\" + 0.013*\"string\" + 0.011*\"gauge\" + '\n",
            "  '0.009*\"type\" + 0.009*\"theories\" + 0.009*\"field\" + 0.008*\"two\" + '\n",
            "  '0.008*\"model\" + 0.007*\"branes\"'),\n",
            " (1,\n",
            "  '0.022*\"theory\" + 0.011*\"field\" + 0.009*\"gauge\" + 0.009*\"one\" + 0.007*\"yang\" '\n",
            "  '+ 0.006*\"mills\" + 0.006*\"loop\" + 0.006*\"non\" + 0.006*\"order\" + 0.006*\"two\"'),\n",
            " (2,\n",
            "  '0.025*\"theory\" + 0.013*\"field\" + 0.013*\"string\" + 0.010*\"quantum\" + '\n",
            "  '0.007*\"space\" + 0.007*\"dimensional\" + 0.006*\"non\" + 0.006*\"one\" + '\n",
            "  '0.005*\"energy\" + 0.005*\"algebra\"'),\n",
            " (3,\n",
            "  '0.021*\"theory\" + 0.007*\"non\" + 0.007*\"theories\" + 0.007*\"action\" + '\n",
            "  '0.006*\"string\" + 0.006*\"dimensional\" + 0.006*\"field\" + 0.006*\"models\" + '\n",
            "  '0.006*\"gauge\" + 0.006*\"solutions\"'),\n",
            " (4,\n",
            "  '0.012*\"model\" + 0.010*\"black\" + 0.009*\"brane\" + 0.009*\"quantum\" + '\n",
            "  '0.008*\"two\" + 0.008*\"dimensional\" + 0.008*\"ads\" + 0.008*\"field\" + '\n",
            "  '0.007*\"theory\" + 0.007*\"hole\"'),\n",
            " (5,\n",
            "  '0.016*\"theory\" + 0.012*\"string\" + 0.012*\"field\" + 0.011*\"gauge\" + '\n",
            "  '0.010*\"space\" + 0.010*\"brane\" + 0.008*\"branes\" + 0.007*\"model\" + '\n",
            "  '0.007*\"non\" + 0.006*\"also\"'),\n",
            " (6,\n",
            "  '0.013*\"space\" + 0.009*\"energy\" + 0.009*\"gauge\" + 0.008*\"model\" + '\n",
            "  '0.008*\"two\" + 0.006*\"field\" + 0.006*\"one\" + 0.006*\"non\" + 0.006*\"theory\" + '\n",
            "  '0.005*\"time\"'),\n",
            " (7,\n",
            "  '0.010*\"functions\" + 0.007*\"equations\" + 0.007*\"point\" + 0.006*\"two\" + '\n",
            "  '0.006*\"theory\" + 0.006*\"dimensional\" + 0.006*\"field\" + 0.006*\"model\" + '\n",
            "  '0.006*\"models\" + 0.005*\"function\"'),\n",
            " (8,\n",
            "  '0.017*\"gauge\" + 0.014*\"theory\" + 0.013*\"quantum\" + 0.011*\"field\" + '\n",
            "  '0.009*\"theories\" + 0.008*\"non\" + 0.005*\"algebra\" + 0.005*\"space\" + '\n",
            "  '0.005*\"also\" + 0.005*\"terms\"'),\n",
            " (9,\n",
            "  '0.011*\"gauge\" + 0.008*\"fields\" + 0.008*\"theories\" + 0.007*\"space\" + '\n",
            "  '0.007*\"field\" + 0.007*\"quantum\" + 0.007*\"states\" + 0.006*\"black\" + '\n",
            "  '0.006*\"two\" + 0.006*\"su\"')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF vectorization"
      ],
      "metadata": {
        "id": "xcQR25PrrIBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# compute TFIDF vector of each paper\n",
        "corpus = [element[5] for element in node_info]\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "# each row is a node in the order of node_info\n",
        "features_TFIDF = vectorizer.fit_transform(corpus)\n",
        "features_TFIDF_names = vectorizer.get_feature_names_out()\n"
      ],
      "metadata": {
        "id": "jI6XbIoVSLMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_TFIDF[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h_Q4kfpSF7-",
        "outputId": "498d3e47-6eec-4205-fa58-473f9316e85d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<5x25043 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 228 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tfidf_top_words(features_TFIDF, vectorizer, id, k=30):\n",
        "    tfidf_vector = features_TFIDF[id]\n",
        "    tfidf_vector = np.array(tfidf_vector.todense())[0]\n",
        "    top_k_indices = np.argsort(-tfidf_vector)[:k]\n",
        "    return list(features_TFIDF_names[top_k_indices])\n"
      ],
      "metadata": {
        "id": "C-r4SyKsg173"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature computation"
      ],
      "metadata": {
        "id": "9aHtD6CbSMF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocessing_graph(graph, ds, node_df, \n",
        "                        deg_centrality, betweeness_centrality=None, \n",
        "                        train=True):\n",
        "\n",
        "    source_degree_centrality, target_degree_centrality, diff_bt = [], [], []\n",
        "    pref_attach, aai, jacard_coeff = [], [], []\n",
        "    common_neigh = []\n",
        "    overlap_title, temp_diff, comm_auth = [], [], []\n",
        "    comm_top_words, abstract_sim, same_journal_name = [], [], []\n",
        "    comm_title_topics, comm_abstract_topics = [], []\n",
        "\n",
        "    counter = 0\n",
        "    for i, edge in enumerate(ds):\n",
        "        if train:\n",
        "            source, target, label = edge\n",
        "        else:\n",
        "            source, target = edge\n",
        "        \n",
        "        index_source = IDs.index(source)\n",
        "        index_target = IDs.index(target)\n",
        "\n",
        "        # Degree Centrality\n",
        "        source_degree_centrality.append(deg_centrality[source])\n",
        "        target_degree_centrality.append(deg_centrality[target])\n",
        "        \n",
        "        if betweeness_centrality is not None:\n",
        "            # Betweeness centrality measure \n",
        "            diff_bt.append(betweeness_centrality[target] - betweeness_centrality[source])\n",
        "\n",
        "        # Preferential Attachement \n",
        "        pref_attach.append(list(nx.preferential_attachment(graph, [(source, target)]))[0][2])\n",
        "\n",
        "        # AdamicAdar\n",
        "        aai.append(list(nx.adamic_adar_index(graph, [(source, target)]))[0][2])\n",
        "\n",
        "        # Jaccard\n",
        "        jacard_coeff.append(list(nx.jaccard_coefficient(graph, [(source, target)]))[0][2])\n",
        "\n",
        "        # Number of common neighobrs\n",
        "        common_neigh.append(len(sorted(nx.common_neighbors(graph, source, target))))\n",
        "\n",
        "        # Node information features\n",
        "        source_info = node_df.loc[node_df.id == source].iloc[0]\n",
        "        target_info = node_df.loc[node_df.id == target].iloc[0]\n",
        "        \n",
        "        # convert to lowercase and tokenize\n",
        "        source_title = source_info[\"title\"].lower().split(\" \")\n",
        "        # remove stopwords\n",
        "        source_title = [token for token in source_title if token not in stpwds]\n",
        "        source_title = [stemmer.stem(token) for token in source_title]\n",
        "        \n",
        "        target_title = target_info[\"title\"].lower().split(\" \")\n",
        "        target_title = [token for token in target_title if token not in stpwds]\n",
        "        target_title = [stemmer.stem(token) for token in target_title]\n",
        "        \n",
        "        # Author lists\n",
        "        source_auth = source_info[\"authors\"].split(\",\")\n",
        "        target_auth = target_info[\"authors\"].split(\",\")\n",
        "\n",
        "        # TFIDF\n",
        "        source_top_10_words = tfidf_top_words(features_TFIDF, vectorizer, index_source)\n",
        "        target_top_10_words = tfidf_top_words(features_TFIDF, vectorizer, index_target)\n",
        "        abstract_sim.append(cosine_similarity(features_TFIDF[index_source], features_TFIDF[index_target])[0][0])\n",
        "\n",
        "        # Journal name\n",
        "        source_journal = source_info[\"journal_name\"].lower().strip()\n",
        "        target_journal = target_info[\"journal_name\"].lower().strip()\n",
        "\n",
        "        # Title and abstract topic\n",
        "        source_title_topic = title_lda[index_source][0][0]\n",
        "        target_title_topic = title_lda[index_target][0][0]\n",
        "        source_abstract_topics = [topic for topic, score in abstract_lda[index_source]]\n",
        "        target_abstract_topics = [topic for topic, score in abstract_lda[index_target]]\n",
        "        \n",
        "        overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
        "        temp_diff.append(int(source_info[\"publication_year\"]) - int(target_info[\"publication_year\"]))\n",
        "        comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
        "        comm_top_words.append(len(set(source_top_10_words).intersection(set(target_top_10_words))))\n",
        "        same_journal_name.append(1 if len(source_journal) > 0 and source_journal == target_journal else 0)\n",
        "        comm_title_topics.append(int(source_title_topic==target_title_topic))\n",
        "        comm_abstract_topics.append(len(set(source_abstract_topics).intersection(set(target_abstract_topics))))\n",
        "      \n",
        "        counter += 1\n",
        "        if counter % 1000 == 0:\n",
        "            print(f\"{counter}/{len(ds)} examples processsed\")\n",
        "        # break\n",
        "\n",
        "    # convert list of lists into array\n",
        "    # documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
        "    if betweeness_centrality is not None:\n",
        "        features = np.array([source_degree_centrality, target_degree_centrality, \n",
        "                             diff_bt, pref_attach, aai, jacard_coeff, common_neigh,\n",
        "                             overlap_title, temp_diff, comm_auth, \n",
        "                             comm_top_words, abstract_sim, same_journal_name, \n",
        "                             comm_title_topics, comm_abstract_topics]).T\n",
        "    else:\n",
        "        features = np.array([source_degree_centrality, target_degree_centrality, \n",
        "                             pref_attach, aai, jacard_coeff, common_neigh,\n",
        "                             overlap_title, temp_diff, comm_auth, \n",
        "                             comm_top_words, abstract_sim, same_journal_name, \n",
        "                             comm_title_topics, comm_abstract_topics]).T\n",
        "    # print(features)\n",
        "    print(f\"{len(features[0])} features computed.\")\n",
        "\n",
        "    # scale\n",
        "    features = preprocessing.scale(features)\n",
        "\n",
        "    if train:\n",
        "        # convert labels into integers then into column array\n",
        "        labels = [int(element[2]) for element in ds]\n",
        "        labels = list(labels)\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        return features, labels\n",
        "    else:\n",
        "        return features\n"
      ],
      "metadata": {
        "id": "rldAzENHSblL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_features, training_labels = preprocessing_graph(\n",
        "    G, training_set_reduced, node_df, \n",
        "    deg_centrality, betweeness_centrality, \n",
        "    train=True\n",
        ")\n",
        "\n",
        "np.save(project_folder + \"/training_features.npy\", training_features)\n",
        "np.save(project_folder + \"/training_labels.npy\", training_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4csXGSnTTx1l",
        "outputId": "6782318b-13e4-486d-fa91-bd96b17a094b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000/307756 examples processsed\n",
            "2000/307756 examples processsed\n",
            "3000/307756 examples processsed\n",
            "4000/307756 examples processsed\n",
            "5000/307756 examples processsed\n",
            "6000/307756 examples processsed\n",
            "7000/307756 examples processsed\n",
            "8000/307756 examples processsed\n",
            "9000/307756 examples processsed\n",
            "10000/307756 examples processsed\n",
            "11000/307756 examples processsed\n",
            "12000/307756 examples processsed\n",
            "13000/307756 examples processsed\n",
            "14000/307756 examples processsed\n",
            "15000/307756 examples processsed\n",
            "16000/307756 examples processsed\n",
            "17000/307756 examples processsed\n",
            "18000/307756 examples processsed\n",
            "19000/307756 examples processsed\n",
            "20000/307756 examples processsed\n",
            "21000/307756 examples processsed\n",
            "22000/307756 examples processsed\n",
            "23000/307756 examples processsed\n",
            "24000/307756 examples processsed\n",
            "25000/307756 examples processsed\n",
            "26000/307756 examples processsed\n",
            "27000/307756 examples processsed\n",
            "28000/307756 examples processsed\n",
            "29000/307756 examples processsed\n",
            "30000/307756 examples processsed\n",
            "31000/307756 examples processsed\n",
            "32000/307756 examples processsed\n",
            "33000/307756 examples processsed\n",
            "34000/307756 examples processsed\n",
            "35000/307756 examples processsed\n",
            "36000/307756 examples processsed\n",
            "37000/307756 examples processsed\n",
            "38000/307756 examples processsed\n",
            "39000/307756 examples processsed\n",
            "40000/307756 examples processsed\n",
            "41000/307756 examples processsed\n",
            "42000/307756 examples processsed\n",
            "43000/307756 examples processsed\n",
            "44000/307756 examples processsed\n",
            "45000/307756 examples processsed\n",
            "46000/307756 examples processsed\n",
            "47000/307756 examples processsed\n",
            "48000/307756 examples processsed\n",
            "49000/307756 examples processsed\n",
            "50000/307756 examples processsed\n",
            "51000/307756 examples processsed\n",
            "52000/307756 examples processsed\n",
            "53000/307756 examples processsed\n",
            "54000/307756 examples processsed\n",
            "55000/307756 examples processsed\n",
            "56000/307756 examples processsed\n",
            "57000/307756 examples processsed\n",
            "58000/307756 examples processsed\n",
            "59000/307756 examples processsed\n",
            "60000/307756 examples processsed\n",
            "61000/307756 examples processsed\n",
            "62000/307756 examples processsed\n",
            "63000/307756 examples processsed\n",
            "64000/307756 examples processsed\n",
            "65000/307756 examples processsed\n",
            "66000/307756 examples processsed\n",
            "67000/307756 examples processsed\n",
            "68000/307756 examples processsed\n",
            "69000/307756 examples processsed\n",
            "70000/307756 examples processsed\n",
            "71000/307756 examples processsed\n",
            "72000/307756 examples processsed\n",
            "73000/307756 examples processsed\n",
            "74000/307756 examples processsed\n",
            "75000/307756 examples processsed\n",
            "76000/307756 examples processsed\n",
            "77000/307756 examples processsed\n",
            "78000/307756 examples processsed\n",
            "79000/307756 examples processsed\n",
            "80000/307756 examples processsed\n",
            "81000/307756 examples processsed\n",
            "82000/307756 examples processsed\n",
            "83000/307756 examples processsed\n",
            "84000/307756 examples processsed\n",
            "85000/307756 examples processsed\n",
            "86000/307756 examples processsed\n",
            "87000/307756 examples processsed\n",
            "88000/307756 examples processsed\n",
            "89000/307756 examples processsed\n",
            "90000/307756 examples processsed\n",
            "91000/307756 examples processsed\n",
            "92000/307756 examples processsed\n",
            "93000/307756 examples processsed\n",
            "94000/307756 examples processsed\n",
            "95000/307756 examples processsed\n",
            "96000/307756 examples processsed\n",
            "97000/307756 examples processsed\n",
            "98000/307756 examples processsed\n",
            "99000/307756 examples processsed\n",
            "100000/307756 examples processsed\n",
            "101000/307756 examples processsed\n",
            "102000/307756 examples processsed\n",
            "103000/307756 examples processsed\n",
            "104000/307756 examples processsed\n",
            "105000/307756 examples processsed\n",
            "106000/307756 examples processsed\n",
            "107000/307756 examples processsed\n",
            "108000/307756 examples processsed\n",
            "109000/307756 examples processsed\n",
            "110000/307756 examples processsed\n",
            "111000/307756 examples processsed\n",
            "112000/307756 examples processsed\n",
            "113000/307756 examples processsed\n",
            "114000/307756 examples processsed\n",
            "115000/307756 examples processsed\n",
            "116000/307756 examples processsed\n",
            "117000/307756 examples processsed\n",
            "118000/307756 examples processsed\n",
            "119000/307756 examples processsed\n",
            "120000/307756 examples processsed\n",
            "121000/307756 examples processsed\n",
            "122000/307756 examples processsed\n",
            "123000/307756 examples processsed\n",
            "124000/307756 examples processsed\n",
            "125000/307756 examples processsed\n",
            "126000/307756 examples processsed\n",
            "127000/307756 examples processsed\n",
            "128000/307756 examples processsed\n",
            "129000/307756 examples processsed\n",
            "130000/307756 examples processsed\n",
            "131000/307756 examples processsed\n",
            "132000/307756 examples processsed\n",
            "133000/307756 examples processsed\n",
            "134000/307756 examples processsed\n",
            "135000/307756 examples processsed\n",
            "136000/307756 examples processsed\n",
            "137000/307756 examples processsed\n",
            "138000/307756 examples processsed\n",
            "139000/307756 examples processsed\n",
            "140000/307756 examples processsed\n",
            "141000/307756 examples processsed\n",
            "142000/307756 examples processsed\n",
            "143000/307756 examples processsed\n",
            "144000/307756 examples processsed\n",
            "145000/307756 examples processsed\n",
            "146000/307756 examples processsed\n",
            "147000/307756 examples processsed\n",
            "148000/307756 examples processsed\n",
            "149000/307756 examples processsed\n",
            "150000/307756 examples processsed\n",
            "151000/307756 examples processsed\n",
            "152000/307756 examples processsed\n",
            "153000/307756 examples processsed\n",
            "154000/307756 examples processsed\n",
            "155000/307756 examples processsed\n",
            "156000/307756 examples processsed\n",
            "157000/307756 examples processsed\n",
            "158000/307756 examples processsed\n",
            "159000/307756 examples processsed\n",
            "160000/307756 examples processsed\n",
            "161000/307756 examples processsed\n",
            "162000/307756 examples processsed\n",
            "163000/307756 examples processsed\n",
            "164000/307756 examples processsed\n",
            "165000/307756 examples processsed\n",
            "166000/307756 examples processsed\n",
            "167000/307756 examples processsed\n",
            "168000/307756 examples processsed\n",
            "169000/307756 examples processsed\n",
            "170000/307756 examples processsed\n",
            "171000/307756 examples processsed\n",
            "172000/307756 examples processsed\n",
            "173000/307756 examples processsed\n",
            "174000/307756 examples processsed\n",
            "175000/307756 examples processsed\n",
            "176000/307756 examples processsed\n",
            "177000/307756 examples processsed\n",
            "178000/307756 examples processsed\n",
            "179000/307756 examples processsed\n",
            "180000/307756 examples processsed\n",
            "181000/307756 examples processsed\n",
            "182000/307756 examples processsed\n",
            "183000/307756 examples processsed\n",
            "184000/307756 examples processsed\n",
            "185000/307756 examples processsed\n",
            "186000/307756 examples processsed\n",
            "187000/307756 examples processsed\n",
            "188000/307756 examples processsed\n",
            "189000/307756 examples processsed\n",
            "190000/307756 examples processsed\n",
            "191000/307756 examples processsed\n",
            "192000/307756 examples processsed\n",
            "193000/307756 examples processsed\n",
            "194000/307756 examples processsed\n",
            "195000/307756 examples processsed\n",
            "196000/307756 examples processsed\n",
            "197000/307756 examples processsed\n",
            "198000/307756 examples processsed\n",
            "199000/307756 examples processsed\n",
            "200000/307756 examples processsed\n",
            "201000/307756 examples processsed\n",
            "202000/307756 examples processsed\n",
            "203000/307756 examples processsed\n",
            "204000/307756 examples processsed\n",
            "205000/307756 examples processsed\n",
            "206000/307756 examples processsed\n",
            "207000/307756 examples processsed\n",
            "208000/307756 examples processsed\n",
            "209000/307756 examples processsed\n",
            "210000/307756 examples processsed\n",
            "211000/307756 examples processsed\n",
            "212000/307756 examples processsed\n",
            "213000/307756 examples processsed\n",
            "214000/307756 examples processsed\n",
            "215000/307756 examples processsed\n",
            "216000/307756 examples processsed\n",
            "217000/307756 examples processsed\n",
            "218000/307756 examples processsed\n",
            "219000/307756 examples processsed\n",
            "220000/307756 examples processsed\n",
            "221000/307756 examples processsed\n",
            "222000/307756 examples processsed\n",
            "223000/307756 examples processsed\n",
            "224000/307756 examples processsed\n",
            "225000/307756 examples processsed\n",
            "226000/307756 examples processsed\n",
            "227000/307756 examples processsed\n",
            "228000/307756 examples processsed\n",
            "229000/307756 examples processsed\n",
            "230000/307756 examples processsed\n",
            "231000/307756 examples processsed\n",
            "232000/307756 examples processsed\n",
            "233000/307756 examples processsed\n",
            "234000/307756 examples processsed\n",
            "235000/307756 examples processsed\n",
            "236000/307756 examples processsed\n",
            "237000/307756 examples processsed\n",
            "238000/307756 examples processsed\n",
            "239000/307756 examples processsed\n",
            "240000/307756 examples processsed\n",
            "241000/307756 examples processsed\n",
            "242000/307756 examples processsed\n",
            "243000/307756 examples processsed\n",
            "244000/307756 examples processsed\n",
            "245000/307756 examples processsed\n",
            "246000/307756 examples processsed\n",
            "247000/307756 examples processsed\n",
            "248000/307756 examples processsed\n",
            "249000/307756 examples processsed\n",
            "250000/307756 examples processsed\n",
            "251000/307756 examples processsed\n",
            "252000/307756 examples processsed\n",
            "253000/307756 examples processsed\n",
            "254000/307756 examples processsed\n",
            "255000/307756 examples processsed\n",
            "256000/307756 examples processsed\n",
            "257000/307756 examples processsed\n",
            "258000/307756 examples processsed\n",
            "259000/307756 examples processsed\n",
            "260000/307756 examples processsed\n",
            "261000/307756 examples processsed\n",
            "262000/307756 examples processsed\n",
            "263000/307756 examples processsed\n",
            "264000/307756 examples processsed\n",
            "265000/307756 examples processsed\n",
            "266000/307756 examples processsed\n",
            "267000/307756 examples processsed\n",
            "268000/307756 examples processsed\n",
            "269000/307756 examples processsed\n",
            "270000/307756 examples processsed\n",
            "271000/307756 examples processsed\n",
            "272000/307756 examples processsed\n",
            "273000/307756 examples processsed\n",
            "274000/307756 examples processsed\n",
            "275000/307756 examples processsed\n",
            "276000/307756 examples processsed\n",
            "277000/307756 examples processsed\n",
            "278000/307756 examples processsed\n",
            "279000/307756 examples processsed\n",
            "280000/307756 examples processsed\n",
            "281000/307756 examples processsed\n",
            "282000/307756 examples processsed\n",
            "283000/307756 examples processsed\n",
            "284000/307756 examples processsed\n",
            "285000/307756 examples processsed\n",
            "286000/307756 examples processsed\n",
            "287000/307756 examples processsed\n",
            "288000/307756 examples processsed\n",
            "289000/307756 examples processsed\n",
            "290000/307756 examples processsed\n",
            "291000/307756 examples processsed\n",
            "292000/307756 examples processsed\n",
            "293000/307756 examples processsed\n",
            "294000/307756 examples processsed\n",
            "295000/307756 examples processsed\n",
            "296000/307756 examples processsed\n",
            "297000/307756 examples processsed\n",
            "298000/307756 examples processsed\n",
            "299000/307756 examples processsed\n",
            "300000/307756 examples processsed\n",
            "301000/307756 examples processsed\n",
            "302000/307756 examples processsed\n",
            "303000/307756 examples processsed\n",
            "304000/307756 examples processsed\n",
            "305000/307756 examples processsed\n",
            "306000/307756 examples processsed\n",
            "307000/307756 examples processsed\n",
            "14 features computed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "testing_features = preprocessing_graph(\n",
        "    G, testing_set, node_df, \n",
        "    deg_centrality, betweeness_centrality, \n",
        "    train=False\n",
        ")\n",
        "\n",
        "np.save(project_folder + \"/testing_features.npy\", testing_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COz7XM6fZPRz",
        "outputId": "d4760759-bfd6-45ed-d7ac-661875fbe40e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000/32648 examples processsed\n",
            "2000/32648 examples processsed\n",
            "3000/32648 examples processsed\n",
            "4000/32648 examples processsed\n",
            "5000/32648 examples processsed\n",
            "6000/32648 examples processsed\n",
            "7000/32648 examples processsed\n",
            "8000/32648 examples processsed\n",
            "9000/32648 examples processsed\n",
            "10000/32648 examples processsed\n",
            "11000/32648 examples processsed\n",
            "12000/32648 examples processsed\n",
            "13000/32648 examples processsed\n",
            "14000/32648 examples processsed\n",
            "15000/32648 examples processsed\n",
            "16000/32648 examples processsed\n",
            "17000/32648 examples processsed\n",
            "18000/32648 examples processsed\n",
            "19000/32648 examples processsed\n",
            "20000/32648 examples processsed\n",
            "21000/32648 examples processsed\n",
            "22000/32648 examples processsed\n",
            "23000/32648 examples processsed\n",
            "24000/32648 examples processsed\n",
            "25000/32648 examples processsed\n",
            "26000/32648 examples processsed\n",
            "27000/32648 examples processsed\n",
            "28000/32648 examples processsed\n",
            "29000/32648 examples processsed\n",
            "30000/32648 examples processsed\n",
            "31000/32648 examples processsed\n",
            "32000/32648 examples processsed\n",
            "14 features computed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model fitting & predictions"
      ],
      "metadata": {
        "id": "eMd68cdEY73_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "names = [\n",
        "    \"Nearest Neighbors\",\n",
        "    # \"Linear SVM\",\n",
        "    # \"RBF SVM\",\n",
        "    # \"Gaussian Process\",\n",
        "    # \"Decision Tree\",\n",
        "    \"Random Forest\",\n",
        "    \"Neural Net 2\",\n",
        "    \"Neural Net 3\",\n",
        "    \"AdaBoost\",\n",
        "    # \"Naive Bayes\",\n",
        "    # \"QDA\",\n",
        "]\n",
        "\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(3),\n",
        "    # SVC(kernel=\"linear\", C=0.025),\n",
        "    # SVC(gamma=2, C=1),\n",
        "    # GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
        "    # DecisionTreeClassifier(max_depth=10),\n",
        "    RandomForestClassifier(max_depth=10, n_estimators=100),\n",
        "    MLPClassifier(hidden_layer_sizes=(100, 100), alpha=1e-2, max_iter=1000),\n",
        "    MLPClassifier(hidden_layer_sizes=(50, 50, 20), alpha=1e-2, max_iter=1000),\n",
        "    AdaBoostClassifier(),\n",
        "    # GaussianNB(),\n",
        "    # QuadraticDiscriminantAnalysis(),\n",
        "]\n"
      ],
      "metadata": {
        "id": "XNVD0ZmT7ipe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "# Neural Network layer size tuning\n",
        "\n",
        "names = []\n",
        "classifiers = []\n",
        "\n",
        "for l1_size in [20, 50, 100]:\n",
        "    for l2_size in [20, 50, 100]:\n",
        "        names.append(f\"Neural Net {l1_size} {l2_size}\")\n",
        "        classifiers.append(MLPClassifier(\n",
        "            hidden_layer_sizes=(l1_size,l2_size, 20), alpha=1e-2, max_iter=1000))\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "1OGjIc4Q-wYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_features = np.load(project_folder + \"/training_features.npy\")\n",
        "training_labels = np.load(project_folder + \"/training_labels.npy\")\n",
        "testing_features = np.load(project_folder + \"/testing_features.npy\")\n",
        "\n",
        "for name, classifier in zip(names, classifiers):\n",
        "    print(f\"Fitting '{name}' classifier...\")\n",
        "    \n",
        "    # train\n",
        "    classifier.fit(training_features, training_labels)\n",
        "\n",
        "    print(\"Predicting labels on the training set...\")\n",
        "    training_predictions = list(classifier.predict(training_features))\n",
        "    training_score = f1_score(training_labels, training_predictions)\n",
        "    print(\"Training F1-Score:\", training_score)\n",
        "\n",
        "    print(\"Predicting labels on the testing set...\")\n",
        "    # issue predictions\n",
        "    predictions= list(classifier.predict(testing_features))\n",
        "\n",
        "    # write predictions to .csv file suitable for Kaggle (just make sure to add the column names)\n",
        "    predictions = zip(range(len(testing_features)), predictions)\n",
        "\n",
        "    with open(project_folder + \"/predictions_{}.csv\".format(name), \"w\") as pred:\n",
        "        csv_out = csv.writer(pred)\n",
        "        csv_out.writerow([\"id\", \"category\"])\n",
        "        for row in predictions:\n",
        "            csv_out.writerow(row)\n"
      ],
      "metadata": {
        "id": "T0UlntbeZkAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model performances on the test set"
      ],
      "metadata": {
        "id": "7kA0fj8E9UFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### First batch of submission\n",
        "\n",
        "Test results for 10 features on 5% of the training data:\n",
        "\n",
        "|Model name| F1-Score|\n",
        "|------|------|\n",
        "|Nearest Neighbors|**0.95497**|\n",
        "|Linear SVM|0.95123|\n",
        "|RBF SVM|**0.95950**|\n",
        "|Gaussian Process|Not enough RAM|\n",
        "|Decision Tree|0.93841|\n",
        "|Random Forest|0.93517|\n",
        "|Neural Net|**0.95499**|\n",
        "|AdaBoost|**0.93997**|\n",
        "|Naive Bayes|0.92907|\n",
        "|QDA|0.80821|\n",
        "\n",
        "Features:\n",
        "\n",
        "*   **source_degree_centrality:** Degree centrality of the source node;\n",
        "*   **target_degree_centrality:** Degree centrality of the target node;\n",
        "*   **pref_attach:** Preferential attachement score of the two nodes;\n",
        "*   **aai:** Adamic Adar index of the two nodes;\n",
        "*   **jacard_coeff:** Jaccard coefficient of the two nodes;\n",
        "*   **overlap_title:** Number of common words in the paper titles;\n",
        "*   **temp_diff:** Difference in publication years of the two papers;\n",
        "*   **comm_auth:** Number of common authors in the papers;\n",
        "*   **comm_top_words:** Number of common words within the words with the 10 highest TF-IDF score (in the abstract);\n",
        "*   **same_journal_name:** Boolean indicating if the papers where published in the same journal.\n"
      ],
      "metadata": {
        "id": "qlBMw_ky5ijy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Second batch of submission\n",
        "\n",
        "Test results for 11 features on 20% of the training data:\n",
        "\n",
        "|Model name| F1-Score|\n",
        "|------|------|\n",
        "|Nearest Neighbors|0.96126|\n",
        "|RBF SVM|**0.96853**|\n",
        "|Neural Net 1 hidden layer|0.96132|\n",
        "|Neural Net 2 hidden layers|**0.97121**|\n",
        "|AdaBoost|**0.96768**|\n",
        "\n",
        "Additional feature (the other 10 features are the same as the first batch):\n",
        "\n",
        "*   **diff_bt:** Difference between the betweeness centralities of the two nodes (high computation needs).\n"
      ],
      "metadata": {
        "id": "MIypLO3s9YM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Third batch of submission\n",
        "\n",
        "Test results for 14 features on 20% of the training data:\n",
        "\n",
        "|Model name|Train F1-Score| Test F1-Score|\n",
        "|------|------|------|\n",
        "|Nearest Neighbors|0.97571|0.96243|\n",
        "|RBF SVM|Prediction time out|0.96824|\n",
        "|Random Forest|0.97609|0.97441|\n",
        "|Neural Net 2 hidden layers|0.97513|0.97430|\n",
        "|Neural Net 3 hidden layers|0.97500|**0.97535**|\n",
        "|AdaBoost|0.97092|0.97127|\n",
        "\n",
        "Additional features (the other 10 features are the same as the first batch):\n",
        "\n",
        "*   **common_neigh:** Number of common neighbors between the two nodes;\n",
        "*   **abstract_sim:** Cosine similarity between the TF-IDF vetors of the papers' abstracts;\n",
        "*   **comm_title_topics:** Boolean indicating if the main topic detected in the paper titles is the same;\n",
        "*   **comm_abstract_topics:** Number of common topics detected in the abstracts.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OZ9i3i5T5n6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network layer size tuning\n",
        "\n",
        "Note that the third layer was fixed at 20 nodes and the maximum size tested was 100 because of computation time constraints.\n",
        "\n",
        "|First layer size|Second layer size|Train F1-Score|Test F1-Score|\n",
        "|------|------|------|------|\n",
        "|20|20|0.97419|0.97477|\n",
        "|20|50|0.97445|0.97444|\n",
        "|20|100|0.97422|0.97449|\n",
        "|50|20|0.97461|0.97407|\n",
        "|50|50|0.97509|0.97457|\n",
        "|50|100|0.97516|0.97421|\n",
        "|100|20|0.97541|**0.9479**|\n",
        "|100|50|0.97499|0.97478|\n",
        "|100|100|**0.97592**|0.97477|"
      ],
      "metadata": {
        "id": "cVcrsXULHml2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kVn5uBshTzwQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}